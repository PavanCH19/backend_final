{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1478532c-329c-4894-afc0-802da75f8dac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Project structure created: ['data', 'models', 'notebooks', 'src']\n",
      "âœ… Seeds fixed to: 42\n",
      "ðŸ“¦ Suggested packages: ['numpy', 'pandas', 'scikit-learn', 'tensorflow', 'nltk', 'spacy', 'sentence-transformers', 'shap', 'lime', 'matplotlib']\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# Step 0 â€” Project Setup\n",
    "# ================================\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define project structure\n",
    "project_dirs = [\"data\", \"models\", \"notebooks\", \"src\"]\n",
    "\n",
    "for d in project_dirs:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "print(\"âœ… Project structure created:\", project_dirs)\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "print(\"âœ… Seeds fixed to:\", seed)\n",
    "\n",
    "# Verify packages (if running fresh environment)\n",
    "required_packages = [\n",
    "    \"numpy\", \"pandas\", \"scikit-learn\", \"tensorflow\", \n",
    "    \"nltk\", \"spacy\", \"sentence-transformers\", \"shap\", \"lime\", \"matplotlib\"\n",
    "]\n",
    "print(\"ðŸ“¦ Suggested packages:\", required_packages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9420ad4-b7b8-45f9-8fe0-d6046d233dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Domain requirement files saved.\n",
      "âœ… Generated 2000 synthetic resumes and saved to data/synthetic_resumes.json\n",
      "{\n",
      "  \"id\": \"candidate_0000\",\n",
      "  \"skills\": [\n",
      "    \"Express\",\n",
      "    \"HTML\",\n",
      "    \"Kubernetes\",\n",
      "    \"SQL\",\n",
      "    \"CSS\",\n",
      "    \"SQL\"\n",
      "  ],\n",
      "  \"projects\": [\n",
      "    \"Image Classification using CNN\",\n",
      "    \"ETL Pipeline with Spark\"\n",
      "  ],\n",
      "  \"work_experience\": [\n",
      "    {\n",
      "      \"title\": \"Cloud Engineer\",\n",
      "      \"years\": 2\n",
      "    },\n",
      "    {\n",
      "      \"title\": \"Backend Developer\",\n",
      "      \"years\": 3\n",
      "    }\n",
      "  ],\n",
      "  \"test_score\": 54,\n",
      "  \"preferred_domain\": \"Cloud Engineering\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# Step 1 â€” Generate / Collect Dataset\n",
    "# ================================\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ---------- 1.2 Domain Requirements ----------\n",
    "domain_requirements = {\n",
    "    \"Data Science\": {\n",
    "        \"domain\": \"Data Science\",\n",
    "        \"required_skills\": [\n",
    "            \"Python\", \"Pandas\", \"NumPy\", \"Scikit-learn\", \"PyTorch\", \"Docker\", \"Deep Learning\"\n",
    "        ]\n",
    "    },\n",
    "    \"Web Development\": {\n",
    "        \"domain\": \"Web Development\",\n",
    "        \"required_skills\": [\n",
    "            \"HTML\", \"CSS\", \"JavaScript\", \"React\", \"Node.js\", \"Express\", \"SQL\"\n",
    "        ]\n",
    "    },\n",
    "    \"Cloud Engineering\": {\n",
    "        \"domain\": \"Cloud Engineering\",\n",
    "        \"required_skills\": [\n",
    "            \"AWS\", \"Azure\", \"Docker\", \"Kubernetes\", \"Linux\", \"Terraform\", \"CI/CD\"\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save domain requirement files\n",
    "os.makedirs(\"data/domain_requirements\", exist_ok=True)\n",
    "for domain, req in domain_requirements.items():\n",
    "    with open(f\"data/domain_requirements/{domain.lower().replace(' ','_')}.json\", \"w\") as f:\n",
    "        json.dump(req, f, indent=4)\n",
    "\n",
    "print(\"âœ… Domain requirement files saved.\")\n",
    "\n",
    "\n",
    "# ---------- 1.3 Synthetic Resume Generator ----------\n",
    "skills_pool = list(set(sum([req[\"required_skills\"] for req in domain_requirements.values()], []))) + [\n",
    "    \"C++\", \"Java\", \"SQL\", \"Tableau\", \"Hadoop\", \"Spark\", \"Flask\"\n",
    "]\n",
    "\n",
    "job_titles = [\"Data Scientist\", \"Data Analyst\", \"ML Engineer\", \"Backend Developer\", \"Frontend Developer\", \"Cloud Engineer\"]\n",
    "\n",
    "projects_pool = [\n",
    "    \"Image Classification using CNN\", \"Web Scraping with Python\", \"Portfolio Website\",\n",
    "    \"Cloud Infrastructure Setup\", \"ETL Pipeline with Spark\", \"Dashboard with React\"\n",
    "]\n",
    "\n",
    "def generate_resume(idx, domains):\n",
    "    domain = np.random.choice(list(domains.keys()))\n",
    "    required = domains[domain][\"required_skills\"]\n",
    "\n",
    "    # Randomly sample skills\n",
    "    n_skills = np.random.randint(3, 10)\n",
    "    skills = list(np.random.choice(skills_pool, n_skills, replace=False))\n",
    "\n",
    "    # Projects\n",
    "    n_projects = np.random.randint(0, 4)\n",
    "    projects = list(np.random.choice(projects_pool, n_projects, replace=False))\n",
    "\n",
    "    # Work experience\n",
    "    n_exp = np.random.randint(1, 3)\n",
    "    work_experience = [\n",
    "        {\"title\": np.random.choice(job_titles), \"years\": np.random.randint(0, 6)}\n",
    "        for _ in range(n_exp)\n",
    "    ]\n",
    "\n",
    "    # Test score from clipped normal distribution (mean=65, std=20)\n",
    "    test_score = int(np.clip(np.random.normal(65, 20), 0, 100))\n",
    "\n",
    "    resume = {\n",
    "        \"id\": f\"candidate_{idx:04d}\",\n",
    "        \"skills\": skills,\n",
    "        \"projects\": projects,\n",
    "        \"work_experience\": work_experience,\n",
    "        \"test_score\": test_score,\n",
    "        \"preferred_domain\": domain\n",
    "    }\n",
    "    return resume\n",
    "\n",
    "\n",
    "# Generate N=2000 synthetic resumes\n",
    "N = 2000\n",
    "synthetic_resumes = [generate_resume(i, domain_requirements) for i in range(N)]\n",
    "\n",
    "# Save to JSON file\n",
    "with open(\"data/synthetic_resumes.json\", \"w\") as f:\n",
    "    json.dump(synthetic_resumes, f, indent=4)\n",
    "\n",
    "print(f\"âœ… Generated {N} synthetic resumes and saved to data/synthetic_resumes.json\")\n",
    "\n",
    "# Quick peek at one sample\n",
    "print(json.dumps(synthetic_resumes[0], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a4cb07d-8805-4e60-ab79-25d6c9545eb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Rule-based labels assigned and saved to data/labeled_resumes.json\n",
      "\n",
      "Label Distribution:\n",
      " Partial Fit    1157\n",
      "Not Fit         838\n",
      "Fit               5\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample Resume with Label:\n",
      " {\n",
      "  \"id\": \"candidate_0000\",\n",
      "  \"skills\": [\n",
      "    \"Express\",\n",
      "    \"HTML\",\n",
      "    \"Kubernetes\",\n",
      "    \"SQL\",\n",
      "    \"CSS\",\n",
      "    \"SQL\"\n",
      "  ],\n",
      "  \"projects\": [\n",
      "    \"Image Classification using CNN\",\n",
      "    \"ETL Pipeline with Spark\"\n",
      "  ],\n",
      "  \"work_experience\": [\n",
      "    {\n",
      "      \"title\": \"Cloud Engineer\",\n",
      "      \"years\": 2\n",
      "    },\n",
      "    {\n",
      "      \"title\": \"Backend Developer\",\n",
      "      \"years\": 3\n",
      "    }\n",
      "  ],\n",
      "  \"test_score\": 54,\n",
      "  \"preferred_domain\": \"Cloud Engineering\",\n",
      "  \"matched_skills\": [\n",
      "    \"Kubernetes\"\n",
      "  ],\n",
      "  \"missing_skills\": [\n",
      "    \"Azure\",\n",
      "    \"CI/CD\",\n",
      "    \"Docker\",\n",
      "    \"AWS\",\n",
      "    \"Terraform\",\n",
      "    \"Linux\"\n",
      "  ],\n",
      "  \"skill_match_ratio\": 0.14,\n",
      "  \"test_score_norm\": 0.54,\n",
      "  \"project_count\": 2,\n",
      "  \"label\": \"Partial Fit\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# Step 2 â€” Create Ground Truth Labels (Rule-based)\n",
    "# ================================\n",
    "\n",
    "# Load domain requirements\n",
    "domain_req_map = {}\n",
    "for domain, req in domain_requirements.items():\n",
    "    domain_req_map[domain] = set(req[\"required_skills\"])\n",
    "\n",
    "def assign_label(resume, domain_req_map):\n",
    "    domain = resume[\"preferred_domain\"]\n",
    "    required_skills = domain_req_map[domain]\n",
    "    candidate_skills = set(resume[\"skills\"])\n",
    "\n",
    "    # Matched & missing skills\n",
    "    matched_skills = candidate_skills.intersection(required_skills)\n",
    "    missing_skills = required_skills - candidate_skills\n",
    "\n",
    "    # Ratios & counts\n",
    "    skill_match_ratio = len(matched_skills) / len(required_skills) if required_skills else 0\n",
    "    test_score_norm = resume[\"test_score\"] / 100\n",
    "    project_count = len(resume[\"projects\"])\n",
    "\n",
    "    # Apply labeling rules\n",
    "    if (skill_match_ratio >= 0.70) and (test_score_norm >= 0.75) and (project_count >= 1):\n",
    "        label = \"Fit\"\n",
    "    elif (0.40 <= skill_match_ratio < 0.70) or (0.50 <= test_score_norm < 0.75):\n",
    "        label = \"Partial Fit\"\n",
    "    else:\n",
    "        label = \"Not Fit\"\n",
    "\n",
    "    # Add extra fields\n",
    "    resume[\"matched_skills\"] = list(matched_skills)\n",
    "    resume[\"missing_skills\"] = list(missing_skills)\n",
    "    resume[\"skill_match_ratio\"] = round(skill_match_ratio, 2)\n",
    "    resume[\"test_score_norm\"] = round(test_score_norm, 2)\n",
    "    resume[\"project_count\"] = project_count\n",
    "    resume[\"label\"] = label\n",
    "\n",
    "    return resume\n",
    "\n",
    "\n",
    "# Apply labeling to all resumes\n",
    "labeled_resumes = [assign_label(r, domain_req_map) for r in synthetic_resumes]\n",
    "\n",
    "# Save labeled dataset\n",
    "with open(\"data/labeled_resumes.json\", \"w\") as f:\n",
    "    json.dump(labeled_resumes, f, indent=4)\n",
    "\n",
    "print(\"âœ… Rule-based labels assigned and saved to data/labeled_resumes.json\")\n",
    "\n",
    "# Quick distribution check\n",
    "label_counts = pd.Series([r[\"label\"] for r in labeled_resumes]).value_counts()\n",
    "print(\"\\nLabel Distribution:\\n\", label_counts)\n",
    "\n",
    "# Peek at one labeled resume\n",
    "print(\"\\nSample Resume with Label:\\n\", json.dumps(labeled_resumes[0], indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b2e680d-54f0-450e-a7d8-4a94b3d2732a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Cleaned dataset: 2000 resumes (from 2000)\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# Step A â€” Data Cleaning after Generation\n",
    "# ================================\n",
    "\n",
    "import re\n",
    "from difflib import get_close_matches\n",
    "\n",
    "# Canonical vocab (from domain requirements)\n",
    "canonical_skills = sorted(set(sum([req[\"required_skills\"] for req in domain_requirements.values()], [])))\n",
    "\n",
    "# Stopwords for projects\n",
    "stopwords = {\"and\", \"the\", \"project\", \"using\"}\n",
    "\n",
    "# Canonical job titles\n",
    "canonical_titles = [\"data scientist\", \"data analyst\", \"machine learning engineer\", \"intern\", \n",
    "                    \"backend developer\", \"frontend developer\", \"cloud engineer\"]\n",
    "\n",
    "def normalize_skill(skill, canonical_vocab):\n",
    "    s = skill.strip().lower()\n",
    "    # Try exact canonical match\n",
    "    if s in [c.lower() for c in canonical_vocab]:\n",
    "        return s\n",
    "    # Try fuzzy matching (closest skill)\n",
    "    match = get_close_matches(s, [c.lower() for c in canonical_vocab], n=1, cutoff=0.8)\n",
    "    if match:\n",
    "        return match[0]\n",
    "    return s   # keep as-is if no good match\n",
    "\n",
    "def clean_projects(projects):\n",
    "    cleaned = []\n",
    "    for p in projects:\n",
    "        p = p.lower()\n",
    "        p = re.sub(r\"[^a-z0-9 ]\", \" \", p)  # remove punctuation\n",
    "        tokens = [t for t in p.split() if t not in stopwords]\n",
    "        if len(tokens) >= 2:\n",
    "            cleaned.append(\" \".join(tokens))\n",
    "    return list(set(cleaned))  # deduplicate\n",
    "\n",
    "def normalize_title(title):\n",
    "    t = title.lower().strip()\n",
    "    match = get_close_matches(t, canonical_titles, n=1, cutoff=0.7)\n",
    "    return match[0] if match else t\n",
    "\n",
    "def clean_years(years):\n",
    "    try:\n",
    "        y = float(re.sub(\"[^0-9.]\", \"\", str(years)))\n",
    "        return max(y, 0)  # clamp negatives\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def clean_resume(resume):\n",
    "    # Skills\n",
    "    resume[\"skills\"] = list({normalize_skill(s, canonical_skills) for s in resume[\"skills\"]})\n",
    "    \n",
    "    # Projects\n",
    "    resume[\"projects\"] = clean_projects(resume[\"projects\"])\n",
    "    \n",
    "    # Work experience\n",
    "    cleaned_exp = []\n",
    "    for exp in resume[\"work_experience\"]:\n",
    "        cleaned_exp.append({\n",
    "            \"title\": normalize_title(exp[\"title\"]),\n",
    "            \"years\": clean_years(exp[\"years\"])\n",
    "        })\n",
    "    resume[\"work_experience\"] = cleaned_exp\n",
    "    \n",
    "    # Test score\n",
    "    score = resume.get(\"test_score\", 0)\n",
    "    score = max(0, min(score, 100))  # clamp\n",
    "    resume[\"test_score\"] = int(score)\n",
    "    resume[\"test_score_norm\"] = round(score/100, 2)\n",
    "    \n",
    "    return resume\n",
    "\n",
    "# Apply cleaning\n",
    "cleaned_resumes = [clean_resume(r) for r in labeled_resumes]\n",
    "\n",
    "# Remove corrupted/duplicates\n",
    "seen = set()\n",
    "final_resumes = []\n",
    "for r in cleaned_resumes:\n",
    "    key = (tuple(sorted(r[\"skills\"])), tuple(r[\"projects\"]), r[\"test_score\"], r[\"preferred_domain\"])\n",
    "    if not r[\"skills\"] and not r[\"projects\"] and r[\"test_score\"] == 0:\n",
    "        continue  # drop corrupted\n",
    "    if r[\"preferred_domain\"] == \"\":\n",
    "        continue\n",
    "    if key in seen:\n",
    "        continue\n",
    "    seen.add(key)\n",
    "    final_resumes.append(r)\n",
    "\n",
    "print(f\"âœ… Cleaned dataset: {len(final_resumes)} resumes (from {len(labeled_resumes)})\")\n",
    "\n",
    "# Save cleaned data\n",
    "with open(\"data/cleaned_resumes.json\", \"w\") as f:\n",
    "    json.dump(final_resumes, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9b01943-4b1c-460b-888f-c7547fb17392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before balancing: Counter({'Partial Fit': 1157, 'Not Fit': 838, 'Fit': 5})\n",
      "After balancing: Counter({'Partial Fit': 1157, 'Not Fit': 1157, 'Fit': 1157})\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# Step B â€” Label Balancing\n",
    "# ================================\n",
    "\n",
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "labels = [r[\"label\"] for r in final_resumes]\n",
    "counts = Counter(labels)\n",
    "print(\"Before balancing:\", counts)\n",
    "\n",
    "max_count = max(counts.values())\n",
    "balanced_resumes = []\n",
    "\n",
    "for label, count in counts.items():\n",
    "    group = [r for r in final_resumes if r[\"label\"] == label]\n",
    "    if count < max_count:\n",
    "        # Oversample minority\n",
    "        extra = random.choices(group, k=max_count - count)\n",
    "        balanced_resumes.extend(group + extra)\n",
    "    else:\n",
    "        balanced_resumes.extend(group)\n",
    "\n",
    "balanced_counts = Counter([r[\"label\"] for r in balanced_resumes])\n",
    "print(\"After balancing:\", balanced_counts)\n",
    "\n",
    "# Save balanced dataset\n",
    "with open(\"data/balanced_resumes.json\", \"w\") as f:\n",
    "    json.dump(balanced_resumes, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36a1eaa5-aea5-499b-a1a6-00f62437a8b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Built skill vocabulary of size 46\n",
      "\n",
      "=== Sample Resume Features ===\n",
      "Skill vector length: 46\n",
      "Matched: []\n",
      "Missing: ['Azure', 'Kubernetes', 'CI/CD', 'Docker', 'AWS', 'Terraform', 'Linux']\n",
      "Skill match ratio: 0.0\n",
      "Project count: 2 â†’ scaled: 0.43\n",
      "Years experience: 5.0 â†’ scaled: 0.5\n",
      "Normalized test score: 0.54\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# Step 3 â€” Preprocessing & Helper Functions\n",
    "# ================================\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 3.1 Build skill vocabulary\n",
    "def build_skill_vocab(resumes, domain_requirements):\n",
    "    all_skills = set()\n",
    "    for r in resumes:\n",
    "        all_skills.update(r[\"skills\"])\n",
    "    for domain, req in domain_requirements.items():\n",
    "        all_skills.update(req[\"required_skills\"])\n",
    "    skill_vocab = sorted(all_skills)\n",
    "    return skill_vocab\n",
    "\n",
    "skill_vocab = build_skill_vocab(final_resumes, domain_requirements)\n",
    "skill_index = {s: i for i, s in enumerate(skill_vocab)}\n",
    "skill_vocab_size = len(skill_vocab)\n",
    "\n",
    "print(f\"âœ… Built skill vocabulary of size {skill_vocab_size}\")\n",
    "\n",
    "\n",
    "# 3.2 Skill encoding function\n",
    "def encode_skills(candidate_skills, skill_index):\n",
    "    vector = np.zeros(len(skill_index), dtype=int)\n",
    "    for s in candidate_skills:\n",
    "        if s in skill_index:\n",
    "            vector[skill_index[s]] = 1\n",
    "    return vector\n",
    "\n",
    "\n",
    "# 3.3 Matched & missing skills\n",
    "def matched_missing_skills(candidate_skills, domain_required_skills):\n",
    "    candidate_set = set(candidate_skills)\n",
    "    required_set = set(domain_required_skills)\n",
    "    matched = list(candidate_set.intersection(required_set))\n",
    "    missing = list(required_set - candidate_set)\n",
    "    ratio = len(matched) / len(required_set) if required_set else 0\n",
    "    return matched, missing, round(ratio, 2)\n",
    "\n",
    "\n",
    "# 3.4 Project & experience features\n",
    "def extract_project_experience_features(resume):\n",
    "    project_count = len(resume.get(\"projects\", []))\n",
    "    years_experience = sum(item.get(\"years\", 0) for item in resume.get(\"work_experience\", []))\n",
    "    return project_count, years_experience\n",
    "\n",
    "\n",
    "# 3.5 Test score normalization\n",
    "def normalize_test_score(score):\n",
    "    return round(score / 100, 2)\n",
    "\n",
    "\n",
    "# 3.6 Numeric feature scaling (fit & transform on dataset)\n",
    "def fit_numeric_scalers(resumes):\n",
    "    project_counts = []\n",
    "    years_exp = []\n",
    "    \n",
    "    for r in resumes:\n",
    "        p, y = extract_project_experience_features(r)\n",
    "        project_counts.append(p)\n",
    "        years_exp.append(y)\n",
    "    \n",
    "    project_scaler = StandardScaler()\n",
    "    years_scaler = StandardScaler()\n",
    "    \n",
    "    project_scaler.fit(np.array(project_counts).reshape(-1, 1))\n",
    "    years_scaler.fit(np.array(years_exp).reshape(-1, 1))\n",
    "    \n",
    "    return project_scaler, years_scaler\n",
    "\n",
    "def transform_numeric_features(resume, project_scaler, years_scaler):\n",
    "    project_count, years_exp = extract_project_experience_features(resume)\n",
    "    project_scaled = project_scaler.transform([[project_count]])[0][0]\n",
    "    years_scaled = years_scaler.transform([[years_exp]])[0][0]\n",
    "    return project_scaled, years_scaled\n",
    "\n",
    "\n",
    "# ================================\n",
    "# âœ… Test the helper functions\n",
    "# ================================\n",
    "# Fit scalers on dataset\n",
    "project_scaler, years_scaler = fit_numeric_scalers(final_resumes)\n",
    "\n",
    "sample = final_resumes[0]\n",
    "\n",
    "# Encode skills\n",
    "skill_vector = encode_skills(sample[\"skills\"], skill_index)\n",
    "\n",
    "# Matched & missing\n",
    "domain = sample[\"preferred_domain\"]\n",
    "matched, missing, ratio = matched_missing_skills(sample[\"skills\"], domain_requirements[domain][\"required_skills\"])\n",
    "\n",
    "# Numeric features\n",
    "p_count, y_exp = extract_project_experience_features(sample)\n",
    "p_scaled, y_scaled = transform_numeric_features(sample, project_scaler, years_scaler)\n",
    "\n",
    "# Test score norm\n",
    "score_norm = normalize_test_score(sample[\"test_score\"])\n",
    "\n",
    "print(\"\\n=== Sample Resume Features ===\")\n",
    "print(\"Skill vector length:\", len(skill_vector))\n",
    "print(\"Matched:\", matched)\n",
    "print(\"Missing:\", missing)\n",
    "print(\"Skill match ratio:\", ratio)\n",
    "print(\"Project count:\", p_count, \"â†’ scaled:\", round(p_scaled, 2))\n",
    "print(\"Years experience:\", y_exp, \"â†’ scaled:\", round(y_scaled, 2))\n",
    "print(\"Normalized test score:\", score_norm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2069441-7e17-4eb7-9e6e-59f5498a8026",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
