{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff01c8f-d924-4bfb-b002-41cfc85f119f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resume Classifier Implementation - Steps 0 & 1\n",
    "# File: resume_classifier.ipynb\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "# Step 0 - Project Setup\n",
    "print(\"=== Step 0: Project Setup ===\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "# Create project structure\n",
    "project_folders = ['data', 'models', 'notebooks', 'src', 'data/domain_requirements']\n",
    "for folder in project_folders:\n",
    "    Path(folder).mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"Created folder: {folder}\")\n",
    "\n",
    "print(\"Project structure created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a99063c-f967-4581-8502-a196d0861d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 - Generate/Collect Balanced Dataset\n",
    "print(\"\\n=== Step 1: Balanced Dataset Generation ===\")\n",
    "\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1.2 Domain Requirements\n",
    "domain_requirements = {\n",
    "    \"data_science\": {\n",
    "        \"domain\": \"Data Science\",\n",
    "        \"required_skills\": [\"Python\", \"Pandas\", \"NumPy\", \"Scikit-learn\", \"PyTorch\", \"Docker\", \"Deep Learning\"]\n",
    "    },\n",
    "    \"web_development\": {\n",
    "        \"domain\": \"Web Development\", \n",
    "        \"required_skills\": [\"JavaScript\", \"React\", \"Node.js\", \"HTML\", \"CSS\", \"MongoDB\", \"Express\"]\n",
    "    },\n",
    "    \"mobile_development\": {\n",
    "        \"domain\": \"Mobile Development\",\n",
    "        \"required_skills\": [\"Java\", \"Kotlin\", \"Swift\", \"React Native\", \"Flutter\", \"iOS\", \"Android\"]\n",
    "    },\n",
    "    \"devops\": {\n",
    "        \"domain\": \"DevOps\",\n",
    "        \"required_skills\": [\"Docker\", \"Kubernetes\", \"AWS\", \"Jenkins\", \"Terraform\", \"Linux\", \"CI/CD\"]\n",
    "    },\n",
    "    \"cybersecurity\": {\n",
    "        \"domain\": \"Cybersecurity\",\n",
    "        \"required_skills\": [\"Network Security\", \"Penetration Testing\", \"CISSP\", \"Firewall\", \"Encryption\", \"Python\", \"Risk Assessment\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save domain requirements\n",
    "import os\n",
    "os.makedirs('data/domain_requirements', exist_ok=True)\n",
    "for domain_key, requirements in domain_requirements.items():\n",
    "    file_path = f\"data/domain_requirements/{domain_key}.json\"\n",
    "    with open(file_path, 'w') as f:\n",
    "        json.dump(requirements, f, indent=2)\n",
    "    print(f\"Saved: {file_path}\")\n",
    "\n",
    "def generate_balanced_resumes(n_samples=2000, target_distribution=None):\n",
    "    \"\"\"\n",
    "    Generate synthetic resume data with balanced labels\n",
    "    \n",
    "    target_distribution: dict with 'fit', 'partial_fit', 'not_fit' ratios\n",
    "    Default is roughly equal distribution\n",
    "    \"\"\"\n",
    "    \n",
    "    if target_distribution is None:\n",
    "        target_distribution = {\n",
    "            'fit': 0.33,\n",
    "            'partial_fit': 0.34, \n",
    "            'not_fit': 0.33\n",
    "        }\n",
    "    \n",
    "    # Calculate target counts\n",
    "    n_fit = int(n_samples * target_distribution['fit'])\n",
    "    n_partial = int(n_samples * target_distribution['partial_fit'])\n",
    "    n_not_fit = n_samples - n_fit - n_partial\n",
    "    \n",
    "    print(f\"Target distribution:\")\n",
    "    print(f\"  Fit: {n_fit} ({n_fit/n_samples:.1%})\")\n",
    "    print(f\"  Partial Fit: {n_partial} ({n_partial/n_samples:.1%})\")\n",
    "    print(f\"  Not Fit: {n_not_fit} ({n_not_fit/n_samples:.1%})\")\n",
    "    \n",
    "    # Skill pools for different domains\n",
    "    all_skills = {\n",
    "        \"data_science\": [\"Python\", \"R\", \"SQL\", \"Pandas\", \"NumPy\", \"Scikit-learn\", \"TensorFlow\", \"PyTorch\", \n",
    "                        \"Matplotlib\", \"Seaborn\", \"Jupyter\", \"Docker\", \"Deep Learning\", \"Machine Learning\", \n",
    "                        \"Statistics\", \"Data Visualization\", \"Big Data\", \"Spark\", \"Hadoop\"],\n",
    "        \"web_dev\": [\"JavaScript\", \"React\", \"Vue.js\", \"Angular\", \"Node.js\", \"Express\", \"HTML\", \"CSS\", \n",
    "                   \"MongoDB\", \"PostgreSQL\", \"MySQL\", \"Redis\", \"GraphQL\", \"REST API\", \"TypeScript\", \n",
    "                   \"Webpack\", \"Git\", \"Bootstrap\", \"Sass\"],\n",
    "        \"mobile\": [\"Java\", \"Kotlin\", \"Swift\", \"React Native\", \"Flutter\", \"Dart\", \"iOS\", \"Android\", \n",
    "                  \"Xcode\", \"Android Studio\", \"Firebase\", \"SQLite\", \"Core Data\", \"UIKit\", \"SwiftUI\"],\n",
    "        \"devops\": [\"Docker\", \"Kubernetes\", \"AWS\", \"Azure\", \"GCP\", \"Jenkins\", \"Terraform\", \"Ansible\", \n",
    "                  \"Linux\", \"Bash\", \"Python\", \"CI/CD\", \"Git\", \"Monitoring\", \"Nagios\", \"Prometheus\"],\n",
    "        \"security\": [\"Network Security\", \"Penetration Testing\", \"CISSP\", \"CEH\", \"Firewall\", \"Encryption\", \n",
    "                    \"Python\", \"Wireshark\", \"Metasploit\", \"Nmap\", \"Risk Assessment\", \"Compliance\", \"SIEM\"]\n",
    "    }\n",
    "    \n",
    "    # Project templates\n",
    "    project_templates = {\n",
    "        \"data_science\": [\"Customer Churn Prediction\", \"Sales Forecasting Model\", \"Recommendation System\", \n",
    "                        \"Fraud Detection Algorithm\", \"Image Classification\", \"Natural Language Processing\"],\n",
    "        \"web_dev\": [\"E-commerce Website\", \"Social Media Platform\", \"Portfolio Website\", \"Blog Platform\", \n",
    "                   \"Task Management App\", \"Real-time Chat Application\"],\n",
    "        \"mobile\": [\"Weather App\", \"Fitness Tracker\", \"Food Delivery App\", \"Social Media App\", \n",
    "                  \"Game Application\", \"Banking App\"],\n",
    "        \"devops\": [\"CI/CD Pipeline Setup\", \"Infrastructure as Code\", \"Container Orchestration\", \n",
    "                  \"Monitoring Dashboard\", \"Automated Deployment\", \"Cloud Migration\"],\n",
    "        \"security\": [\"Vulnerability Assessment\", \"Security Audit\", \"Network Monitoring System\", \n",
    "                    \"Incident Response Plan\", \"Security Training Program\", \"Compliance Framework\"]\n",
    "    }\n",
    "    \n",
    "    # Job titles\n",
    "    job_titles = {\n",
    "        \"data_science\": [\"Data Scientist\", \"ML Engineer\", \"Data Analyst\", \"Research Scientist\"],\n",
    "        \"web_dev\": [\"Frontend Developer\", \"Backend Developer\", \"Full Stack Developer\", \"Web Developer\"],\n",
    "        \"mobile\": [\"iOS Developer\", \"Android Developer\", \"Mobile Developer\", \"App Developer\"],\n",
    "        \"devops\": [\"DevOps Engineer\", \"Site Reliability Engineer\", \"Cloud Engineer\", \"Infrastructure Engineer\"],\n",
    "        \"security\": [\"Security Analyst\", \"Cybersecurity Engineer\", \"Security Consultant\", \"SOC Analyst\"]\n",
    "    }\n",
    "    \n",
    "    skill_key_mapping = {\n",
    "        \"data_science\": \"data_science\",\n",
    "        \"web_development\": \"web_dev\", \n",
    "        \"mobile_development\": \"mobile\",\n",
    "        \"devops\": \"devops\",\n",
    "        \"cybersecurity\": \"security\"\n",
    "    }\n",
    "    \n",
    "    domains = list(domain_requirements.keys())\n",
    "    resumes = []\n",
    "    \n",
    "    def generate_resume_for_category(category, candidate_id):\n",
    "        \"\"\"Generate a resume targeting a specific fit category\"\"\"\n",
    "        \n",
    "        # Choose preferred domain\n",
    "        preferred_domain_key = random.choice(domains)\n",
    "        preferred_domain = domain_requirements[preferred_domain_key][\"domain\"]\n",
    "        required_skills = domain_requirements[preferred_domain_key][\"required_skills\"]\n",
    "        \n",
    "        domain_skills = all_skills[skill_key_mapping.get(preferred_domain_key, \"data_science\")]\n",
    "        other_skills = []\n",
    "        for skill_set in all_skills.values():\n",
    "            other_skills.extend(skill_set)\n",
    "        other_skills = list(set(other_skills) - set(domain_skills))\n",
    "        \n",
    "        # Generate skills based on target category\n",
    "        if category == 'fit':\n",
    "            # High skill match (>=70%) and high test score (>=75)\n",
    "            required_count = max(1, int(len(required_skills) * random.uniform(0.7, 1.0)))\n",
    "            selected_required = random.sample(required_skills, required_count)\n",
    "            \n",
    "            # Add more domain skills\n",
    "            additional_domain = random.sample(\n",
    "                [s for s in domain_skills if s not in selected_required], \n",
    "                random.randint(2, 5)\n",
    "            )\n",
    "            selected_skills = selected_required + additional_domain\n",
    "            \n",
    "            # High test score (75-100)\n",
    "            test_score = int(random.uniform(75, 100))\n",
    "            \n",
    "            # At least 1 project\n",
    "            n_projects = random.randint(1, 4)\n",
    "            \n",
    "        elif category == 'partial_fit':\n",
    "            # Medium skill match (40-70%) OR medium test score (50-75%)\n",
    "            if random.choice([True, False]):\n",
    "                # Medium skill match with varying test score\n",
    "                required_count = max(1, int(len(required_skills) * random.uniform(0.4, 0.69)))\n",
    "                selected_required = random.sample(required_skills, required_count)\n",
    "                additional_domain = random.sample(\n",
    "                    [s for s in domain_skills if s not in selected_required], \n",
    "                    random.randint(1, 3)\n",
    "                )\n",
    "                selected_skills = selected_required + additional_domain\n",
    "                test_score = int(random.uniform(30, 100))  # Any test score\n",
    "            else:\n",
    "                # Any skill match with medium test score\n",
    "                required_count = max(0, int(len(required_skills) * random.uniform(0.0, 1.0)))\n",
    "                if required_count > 0:\n",
    "                    selected_required = random.sample(required_skills, required_count)\n",
    "                else:\n",
    "                    selected_required = []\n",
    "                additional_domain = random.sample(\n",
    "                    [s for s in domain_skills if s not in selected_required], \n",
    "                    random.randint(1, 4)\n",
    "                )\n",
    "                selected_skills = selected_required + additional_domain\n",
    "                test_score = int(random.uniform(50, 74))  # Medium test score\n",
    "            \n",
    "            n_projects = random.randint(0, 3)\n",
    "            \n",
    "        else:  # not_fit\n",
    "            # Low skill match (<40%) OR low test score (<50)\n",
    "            if random.choice([True, False]):\n",
    "                # Low skill match with any test score\n",
    "                required_count = max(0, int(len(required_skills) * random.uniform(0.0, 0.39)))\n",
    "                if required_count > 0:\n",
    "                    selected_required = random.sample(required_skills, required_count)\n",
    "                else:\n",
    "                    selected_required = []\n",
    "                \n",
    "                # Add mostly other skills or few domain skills\n",
    "                if random.choice([True, False]):\n",
    "                    # Mostly other skills\n",
    "                    selected_skills = selected_required + random.sample(other_skills, random.randint(2, 6))\n",
    "                else:\n",
    "                    # Few domain skills\n",
    "                    additional_domain = random.sample(\n",
    "                        [s for s in domain_skills if s not in selected_required], \n",
    "                        random.randint(0, 2)\n",
    "                    )\n",
    "                    selected_skills = selected_required + additional_domain\n",
    "                \n",
    "                test_score = int(random.uniform(0, 100))  # Any test score\n",
    "            else:\n",
    "                # Any skill match with low test score\n",
    "                required_count = max(0, int(len(required_skills) * random.uniform(0.0, 1.0)))\n",
    "                if required_count > 0:\n",
    "                    selected_required = random.sample(required_skills, required_count)\n",
    "                else:\n",
    "                    selected_required = []\n",
    "                additional_domain = random.sample(\n",
    "                    [s for s in domain_skills if s not in selected_required], \n",
    "                    random.randint(1, 4)\n",
    "                )\n",
    "                selected_skills = selected_required + additional_domain\n",
    "                test_score = int(random.uniform(0, 49))  # Low test score\n",
    "            \n",
    "            n_projects = random.randint(0, 2)\n",
    "        \n",
    "        # Add some other skills randomly\n",
    "        if len(selected_skills) < 8:\n",
    "            additional_other = random.sample(other_skills, random.randint(0, 3))\n",
    "            selected_skills.extend(additional_other)\n",
    "        \n",
    "        # Generate projects\n",
    "        domain_projects = project_templates[skill_key_mapping.get(preferred_domain_key, \"data_science\")]\n",
    "        selected_projects = random.sample(domain_projects, min(n_projects, len(domain_projects)))\n",
    "        \n",
    "        # Generate work experience\n",
    "        n_jobs = random.randint(1, 4)\n",
    "        work_experience = []\n",
    "        domain_job_titles = job_titles[skill_key_mapping.get(preferred_domain_key, \"data_science\")]\n",
    "\n",
    "        for _ in range(n_jobs):\n",
    "            title = random.choice(domain_job_titles)\n",
    "            years = random.randint(1, 8)\n",
    "            work_experience.append({\"title\": title, \"years\": years})\n",
    "        \n",
    "        # Create resume\n",
    "        resume = {\n",
    "            \"skills\": list(set(selected_skills)),  # Remove duplicates\n",
    "            \"projects\": selected_projects,\n",
    "            \"work_experience\": work_experience,\n",
    "            \"test_score\": test_score,\n",
    "            \"preferred_domain\": preferred_domain,\n",
    "            \"id\": f\"candidate_{candidate_id:04d}\"\n",
    "        }\n",
    "        \n",
    "        return resume\n",
    "    \n",
    "    # Generate resumes for each category\n",
    "    candidate_id = 1\n",
    "    \n",
    "    # Generate Fit candidates\n",
    "    for _ in range(n_fit):\n",
    "        resume = generate_resume_for_category('fit', candidate_id)\n",
    "        resumes.append(resume)\n",
    "        candidate_id += 1\n",
    "    \n",
    "    # Generate Partial Fit candidates\n",
    "    for _ in range(n_partial):\n",
    "        resume = generate_resume_for_category('partial_fit', candidate_id)\n",
    "        resumes.append(resume)\n",
    "        candidate_id += 1\n",
    "    \n",
    "    # Generate Not Fit candidates\n",
    "    for _ in range(n_not_fit):\n",
    "        resume = generate_resume_for_category('not_fit', candidate_id)\n",
    "        resumes.append(resume)\n",
    "        candidate_id += 1\n",
    "    \n",
    "    # Shuffle the resumes\n",
    "    random.shuffle(resumes)\n",
    "    \n",
    "    return resumes\n",
    "\n",
    "def calculate_labels(resumes):\n",
    "    \"\"\"Calculate labels for resumes based on the given rules\"\"\"\n",
    "    labeled_resumes = []\n",
    "    \n",
    "    for resume in resumes:\n",
    "        preferred_domain_key = None\n",
    "        for key, domain_info in domain_requirements.items():\n",
    "            if domain_info[\"domain\"] == resume[\"preferred_domain\"]:\n",
    "                preferred_domain_key = key\n",
    "                break\n",
    "        \n",
    "        if preferred_domain_key is None:\n",
    "            continue\n",
    "            \n",
    "        required_skills = set(domain_requirements[preferred_domain_key][\"required_skills\"])\n",
    "        candidate_skills = set(resume[\"skills\"])\n",
    "        \n",
    "        # Calculate skill match ratio\n",
    "        skill_matches = len(required_skills.intersection(candidate_skills))\n",
    "        skill_match_ratio = skill_matches / len(required_skills)\n",
    "        \n",
    "        # Normalize test score\n",
    "        test_score_norm = resume[\"test_score\"] / 100.0\n",
    "        \n",
    "        # Count projects\n",
    "        project_count = len(resume[\"projects\"])\n",
    "        \n",
    "        # Apply rules\n",
    "        if skill_match_ratio >= 0.70 and test_score_norm >= 0.75 and project_count >= 1:\n",
    "            label = \"Fit\"\n",
    "        elif (0.40 <= skill_match_ratio < 0.70) or (0.50 <= test_score_norm < 0.75):\n",
    "            label = \"Partial Fit\"\n",
    "        else:\n",
    "            label = \"Not Fit\"\n",
    "        \n",
    "        # Add calculated metrics to resume\n",
    "        resume_with_label = resume.copy()\n",
    "        resume_with_label.update({\n",
    "            \"skill_match_ratio\": skill_match_ratio,\n",
    "            \"test_score_norm\": test_score_norm,\n",
    "            \"project_count\": project_count,\n",
    "            \"label\": label\n",
    "        })\n",
    "        \n",
    "        labeled_resumes.append(resume_with_label)\n",
    "    \n",
    "    return labeled_resumes\n",
    "\n",
    "# Generate balanced dataset\n",
    "print(\"Generating balanced synthetic resumes...\")\n",
    "synthetic_resumes = generate_balanced_resumes(2000)\n",
    "\n",
    "# Calculate labels and add metrics\n",
    "labeled_resumes = calculate_labels(synthetic_resumes)\n",
    "\n",
    "# Save to JSON file\n",
    "os.makedirs('data', exist_ok=True)\n",
    "with open('data/balanced_synthetic_resumes.json', 'w') as f:\n",
    "    json.dump(labeled_resumes, f, indent=2)\n",
    "\n",
    "print(f\"Generated {len(labeled_resumes)} synthetic resumes\")\n",
    "print(\"Saved to: data/balanced_synthetic_resumes.json\")\n",
    "\n",
    "# Display sample resume\n",
    "print(\"\\n=== Sample Resume ===\")\n",
    "sample_resume = random.choice(labeled_resumes)\n",
    "print(json.dumps({k: v for k, v in sample_resume.items() if k not in ['skills', 'projects', 'work_experience']}, indent=2))\n",
    "print(f\"Skills: {sample_resume['skills'][:5]}...\" if len(sample_resume['skills']) > 5 else f\"Skills: {sample_resume['skills']}\")\n",
    "\n",
    "# Generate comprehensive statistics\n",
    "print(\"\\n=== Balanced Dataset Statistics ===\")\n",
    "df = pd.DataFrame(labeled_resumes)\n",
    "\n",
    "print(f\"Total resumes: {len(df)}\")\n",
    "print(f\"Average test score: {df['test_score'].mean():.2f}\")\n",
    "print(f\"Test score std: {df['test_score'].std():.2f}\")\n",
    "\n",
    "print(\"\\nLabel distribution:\")\n",
    "label_counts = df['label'].value_counts()\n",
    "print(label_counts)\n",
    "for label in label_counts.index:\n",
    "    print(f\"{label}: {label_counts[label]/len(df):.1%}\")\n",
    "\n",
    "print(\"\\nDomain distribution:\")\n",
    "print(df['preferred_domain'].value_counts())\n",
    "\n",
    "print(\"\\nTest score distribution by label:\")\n",
    "for label in df['label'].unique():\n",
    "    label_scores = df[df['label'] == label]['test_score']\n",
    "    print(f\"{label}: {label_scores.mean():.1f} ± {label_scores.std():.1f}\")\n",
    "\n",
    "print(\"\\nSkill match ratio by label:\")\n",
    "for label in df['label'].unique():\n",
    "    label_ratios = df[df['label'] == label]['skill_match_ratio']\n",
    "    print(f\"{label}: {label_ratios.mean():.3f} ± {label_ratios.std():.3f}\")\n",
    "\n",
    "# Plot comprehensive statistics\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "# Test score distribution\n",
    "axes[0, 0].hist(df['test_score'], bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "axes[0, 0].set_xlabel('Test Score')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].set_title('Test Score Distribution')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Label distribution\n",
    "label_counts.plot(kind='bar', ax=axes[0, 1], color=['green', 'orange', 'red'])\n",
    "axes[0, 1].set_title('Label Distribution')\n",
    "axes[0, 1].set_ylabel('Count')\n",
    "axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Test scores by label\n",
    "df.boxplot(column='test_score', by='label', ax=axes[0, 2])\n",
    "axes[0, 2].set_title('Test Scores by Label')\n",
    "axes[0, 2].set_xlabel('Label')\n",
    "\n",
    "# Skill match ratio distribution\n",
    "axes[1, 0].hist(df['skill_match_ratio'], bins=30, alpha=0.7, color='lightgreen', edgecolor='black')\n",
    "axes[1, 0].set_xlabel('Skill Match Ratio')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "axes[1, 0].set_title('Skill Match Ratio Distribution')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Skill match ratio by label\n",
    "df.boxplot(column='skill_match_ratio', by='label', ax=axes[1, 1])\n",
    "axes[1, 1].set_title('Skill Match Ratio by Label')\n",
    "axes[1, 1].set_xlabel('Label')\n",
    "\n",
    "# Project count by label\n",
    "df.boxplot(column='project_count', by='label', ax=axes[1, 2])\n",
    "axes[1, 2].set_title('Project Count by Label')\n",
    "axes[1, 2].set_xlabel('Label')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('data/balanced_dataset_statistics.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Verify label accuracy\n",
    "print(\"\\n=== Label Verification ===\")\n",
    "print(\"Checking if generated labels match the rules...\")\n",
    "\n",
    "correct_labels = 0\n",
    "for resume in labeled_resumes:\n",
    "    skill_match_ratio = resume['skill_match_ratio']\n",
    "    test_score_norm = resume['test_score_norm']\n",
    "    project_count = resume['project_count']\n",
    "    \n",
    "    # Apply rules\n",
    "    if skill_match_ratio >= 0.70 and test_score_norm >= 0.75 and project_count >= 1:\n",
    "        expected_label = \"Fit\"\n",
    "    elif (0.40 <= skill_match_ratio < 0.70) or (0.50 <= test_score_norm < 0.75):\n",
    "        expected_label = \"Partial Fit\"\n",
    "    else:\n",
    "        expected_label = \"Not Fit\"\n",
    "    \n",
    "    if resume['label'] == expected_label:\n",
    "        correct_labels += 1\n",
    "\n",
    "print(f\"Label accuracy: {correct_labels}/{len(labeled_resumes)} ({correct_labels/len(labeled_resumes):.1%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aaefdab-e3c2-4187-8eb3-986c2c27fcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 - Create Ground Truth Labels (Rule-based)\n",
    "print(\"\\n=== Step 2: Ground Truth Labels ===\")\n",
    "\n",
    "def create_ground_truth_labels(resumes, domain_requirements):\n",
    "    \"\"\"\n",
    "    Create ground truth labels using rule-based approach\n",
    "    \n",
    "    Rules:\n",
    "    - Fit: skill_match_ratio >= 0.70 AND test_score_norm >= 0.75 AND project_count >= 1\n",
    "    - Partial Fit: (0.40 <= skill_match_ratio < 0.70) OR (0.50 <= test_score_norm < 0.75)\n",
    "    - Not Fit: skill_match_ratio < 0.40 OR test_score_norm < 0.50\n",
    "    \"\"\"\n",
    "    \n",
    "    labeled_resumes = []\n",
    "    label_stats = {\"Fit\": 0, \"Partial Fit\": 0, \"Not Fit\": 0}\n",
    "    \n",
    "    for resume in resumes:\n",
    "        # Get domain requirements for this candidate's preferred domain\n",
    "        domain_key = None\n",
    "        for key, req in domain_requirements.items():\n",
    "            if req[\"domain\"] == resume[\"preferred_domain\"]:\n",
    "                domain_key = key\n",
    "                break\n",
    "        \n",
    "        if domain_key is None:\n",
    "            print(f\"Warning: No requirements found for domain {resume['preferred_domain']}\")\n",
    "            continue\n",
    "            \n",
    "        required_skills = set(domain_requirements[domain_key][\"required_skills\"])\n",
    "        candidate_skills = set(resume[\"skills\"])\n",
    "        \n",
    "        # Calculate skill match ratio\n",
    "        matched_skills = len(required_skills.intersection(candidate_skills))\n",
    "        total_required_skills = len(required_skills)\n",
    "        skill_match_ratio = matched_skills / total_required_skills if total_required_skills > 0 else 0\n",
    "        \n",
    "        # Calculate normalized test score\n",
    "        test_score_norm = resume[\"test_score\"] / 100.0\n",
    "        \n",
    "        # Count projects\n",
    "        project_count = len(resume[\"projects\"])\n",
    "        \n",
    "        # Apply labeling rules\n",
    "        if (skill_match_ratio >= 0.70) and (test_score_norm >= 0.75) and (project_count >= 1):\n",
    "            label = \"Fit\"\n",
    "        elif (0.40 <= skill_match_ratio < 0.70) or (0.50 <= test_score_norm < 0.75):\n",
    "            label = \"Partial Fit\"\n",
    "        elif (skill_match_ratio < 0.40) or (test_score_norm < 0.50):\n",
    "            label = \"Not Fit\"\n",
    "        else:\n",
    "            label = \"Partial Fit\"  # Default case\n",
    "        \n",
    "        # Add computed metrics and label to resume\n",
    "        labeled_resume = resume.copy()\n",
    "        labeled_resume.update({\n",
    "            \"label\": label,\n",
    "            \"skill_match_ratio\": round(skill_match_ratio, 3),\n",
    "            \"test_score_norm\": round(test_score_norm, 3),\n",
    "            \"project_count\": project_count,\n",
    "            \"matched_skills\": matched_skills,\n",
    "            \"total_required_skills\": total_required_skills\n",
    "        })\n",
    "        \n",
    "        labeled_resumes.append(labeled_resume)\n",
    "        label_stats[label] += 1\n",
    "    \n",
    "    return labeled_resumes, label_stats\n",
    "\n",
    "# Apply labeling to synthetic resumes\n",
    "print(\"Applying rule-based labeling...\")\n",
    "labeled_resumes, label_statistics = create_ground_truth_labels(synthetic_resumes, domain_requirements)\n",
    "\n",
    "# Save labeled dataset\n",
    "with open('data/labeled_resumes.json', 'w') as f:\n",
    "    json.dump(labeled_resumes, f, indent=2)\n",
    "\n",
    "print(f\"Created labels for {len(labeled_resumes)} resumes\")\n",
    "print(\"Saved to: data/labeled_resumes.json\")\n",
    "\n",
    "# Display labeling statistics\n",
    "print(\"\\n=== Label Distribution ===\")\n",
    "total_resumes = sum(label_statistics.values())\n",
    "for label, count in label_statistics.items():\n",
    "    percentage = (count / total_resumes) * 100\n",
    "    print(f\"{label}: {count} ({percentage:.1f}%)\")\n",
    "\n",
    "# Show sample labeled resumes\n",
    "print(\"\\n=== Sample Labeled Resumes ===\")\n",
    "for label in [\"Fit\", \"Partial Fit\", \"Not Fit\"]:\n",
    "    sample = next((r for r in labeled_resumes if r[\"label\"] == label), None)\n",
    "    if sample:\n",
    "        print(f\"\\n{label} Example:\")\n",
    "        print(f\"  ID: {sample['id']}\")\n",
    "        print(f\"  Domain: {sample['preferred_domain']}\")\n",
    "        print(f\"  Skills: {len(sample['skills'])} total\")\n",
    "        print(f\"  Skill match ratio: {sample['skill_match_ratio']} ({sample['matched_skills']}/{sample['total_required_skills']})\")\n",
    "        print(f\"  Test score: {sample['test_score']} (norm: {sample['test_score_norm']})\")\n",
    "        print(f\"  Projects: {sample['project_count']}\")\n",
    "\n",
    "# Detailed analysis by domain\n",
    "print(\"\\n=== Label Distribution by Domain ===\")\n",
    "df_labeled = pd.DataFrame(labeled_resumes)\n",
    "\n",
    "domain_label_crosstab = pd.crosstab(df_labeled['preferred_domain'], df_labeled['label'])\n",
    "print(domain_label_crosstab)\n",
    "\n",
    "# Calculate percentages within each domain\n",
    "domain_label_pct = pd.crosstab(df_labeled['preferred_domain'], df_labeled['label'], normalize='index') * 100\n",
    "print(\"\\nPercentages within each domain:\")\n",
    "print(domain_label_pct.round(1))\n",
    "\n",
    "# Visualize label distributions\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Overall label distribution\n",
    "plt.subplot(2, 3, 1)\n",
    "labels = list(label_statistics.keys())\n",
    "counts = list(label_statistics.values())\n",
    "colors = ['lightgreen', 'orange', 'lightcoral']\n",
    "plt.pie(counts, labels=labels, autopct='%1.1f%%', colors=colors)\n",
    "plt.title('Overall Label Distribution')\n",
    "\n",
    "# Label distribution by domain\n",
    "plt.subplot(2, 3, 2)\n",
    "domain_label_crosstab.plot(kind='bar', ax=plt.gca(), color=colors)\n",
    "plt.title('Label Distribution by Domain')\n",
    "plt.xlabel('Domain')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title='Label')\n",
    "\n",
    "# Skill match ratio distribution by label\n",
    "plt.subplot(2, 3, 3)\n",
    "for label in labels:\n",
    "    data = df_labeled[df_labeled['label'] == label]['skill_match_ratio']\n",
    "    plt.hist(data, alpha=0.6, label=label, bins=20)\n",
    "plt.xlabel('Skill Match Ratio')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Skill Match Ratio by Label')\n",
    "plt.legend()\n",
    "\n",
    "# Test score distribution by label\n",
    "plt.subplot(2, 3, 4)\n",
    "for label in labels:\n",
    "    data = df_labeled[df_labeled['label'] == label]['test_score']\n",
    "    plt.hist(data, alpha=0.6, label=label, bins=20)\n",
    "plt.xlabel('Test Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Test Score Distribution by Label')\n",
    "plt.legend()\n",
    "\n",
    "# Scatter plot: skill match ratio vs test score\n",
    "plt.subplot(2, 3, 5)\n",
    "for i, label in enumerate(labels):\n",
    "    data = df_labeled[df_labeled['label'] == label]\n",
    "    plt.scatter(data['skill_match_ratio'], data['test_score_norm'], \n",
    "               alpha=0.6, label=label, color=colors[i])\n",
    "plt.xlabel('Skill Match Ratio')\n",
    "plt.ylabel('Test Score (Normalized)')\n",
    "plt.title('Skill Match vs Test Score')\n",
    "plt.legend()\n",
    "\n",
    "# Project count by label\n",
    "plt.subplot(2, 3, 6)\n",
    "project_counts = df_labeled.groupby(['label', 'project_count']).size().unstack(fill_value=0)\n",
    "project_counts.plot(kind='bar', ax=plt.gca())\n",
    "plt.title('Project Count Distribution by Label')\n",
    "plt.xlabel('Label')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title='Project Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('data/labeling_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Rule validation statistics\n",
    "print(\"\\n=== Rule Validation Statistics ===\")\n",
    "print(f\"Average skill match ratio by label:\")\n",
    "skill_avg = df_labeled.groupby('label')['skill_match_ratio'].agg(['mean', 'std'])\n",
    "print(skill_avg.round(3))\n",
    "\n",
    "print(f\"\\nAverage test score by label:\")\n",
    "score_avg = df_labeled.groupby('label')['test_score'].agg(['mean', 'std'])\n",
    "print(score_avg.round(1))\n",
    "\n",
    "print(f\"\\nAverage project count by label:\")\n",
    "project_avg = df_labeled.groupby('label')['project_count'].agg(['mean', 'std'])\n",
    "print(project_avg.round(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521428c7-288b-4631-b3dc-2b1d2cca805f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3 - Preprocessing & Helper Functions\n",
    "print(\"\\n=== Step 3: Preprocessing & Helper Functions ===\")\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "\n",
    "# 3.1 Build skill vocabulary\n",
    "def build_skill_vocabulary(resumes, domain_requirements):\n",
    "    \"\"\"\n",
    "    Build comprehensive skill vocabulary from all resumes and domain requirements\n",
    "    Returns sorted list of unique skills\n",
    "    \"\"\"\n",
    "    all_skills = set()\n",
    "    \n",
    "    # Add skills from all resumes\n",
    "    for resume in resumes:\n",
    "        all_skills.update(resume.get('skills', []))\n",
    "    \n",
    "    # Add required skills from all domains\n",
    "    for domain_data in domain_requirements.values():\n",
    "        all_skills.update(domain_data.get('required_skills', []))\n",
    "    \n",
    "    # Clean and normalize skills\n",
    "    cleaned_skills = set()\n",
    "    for skill in all_skills:\n",
    "        # Basic text normalization\n",
    "        cleaned_skill = skill.strip().lower()\n",
    "        if cleaned_skill and len(cleaned_skill) > 1:  # Remove empty or single char\n",
    "            cleaned_skills.add(cleaned_skill)\n",
    "    \n",
    "    skill_vocab = sorted(list(cleaned_skills))\n",
    "    return skill_vocab\n",
    "\n",
    "# 3.2 Skill encoding function\n",
    "def encode_skills(candidate_skills, skill_vocab):\n",
    "    \"\"\"\n",
    "    Convert candidate skills list to binary vector\n",
    "    Input: candidate skills list\n",
    "    Output: binary vector of length skill_vocab_size where position i is 1 if skill present\n",
    "    \"\"\"\n",
    "    skill_vector = np.zeros(len(skill_vocab), dtype=int)\n",
    "    \n",
    "    # Normalize candidate skills\n",
    "    normalized_candidate_skills = {skill.strip().lower() for skill in candidate_skills}\n",
    "    \n",
    "    for i, vocab_skill in enumerate(skill_vocab):\n",
    "        if vocab_skill in normalized_candidate_skills:\n",
    "            skill_vector[i] = 1\n",
    "            \n",
    "    return skill_vector\n",
    "\n",
    "# 3.3 Matched & missing skills (per domain)\n",
    "def compute_skill_matches(candidate_skills, required_skills):\n",
    "    \"\"\"\n",
    "    Compute matched and missing skills for a specific domain\n",
    "    Returns: matched_skills (list), missing_skills (list), skill_match_ratio (float)\n",
    "    \"\"\"\n",
    "    # Normalize both sets for comparison\n",
    "    candidate_set = {skill.strip().lower() for skill in candidate_skills}\n",
    "    required_set = {skill.strip().lower() for skill in required_skills}\n",
    "    \n",
    "    # Compute intersections\n",
    "    matched_skills = list(candidate_set.intersection(required_set))\n",
    "    missing_skills = list(required_set - candidate_set)\n",
    "    \n",
    "    # Calculate ratio\n",
    "    skill_match_ratio = len(matched_skills) / len(required_set) if required_set else 0.0\n",
    "    \n",
    "    return matched_skills, missing_skills, skill_match_ratio\n",
    "\n",
    "# NEW FUNCTION: 3.3b Alternative domain suggestion\n",
    "def suggest_alternative_domains(candidate_skills, current_domain, domain_requirements, top_n=3):\n",
    "    \"\"\"\n",
    "    Suggest alternative domains based on skill match for Partial Fit/Not Fit candidates\n",
    "    \n",
    "    Args:\n",
    "        candidate_skills: List of candidate's skills\n",
    "        current_domain: Current preferred domain name\n",
    "        domain_requirements: Dict of all domain requirements\n",
    "        top_n: Number of top suggestions to return\n",
    "    \n",
    "    Returns: \n",
    "        List of dicts with domain suggestions sorted by match ratio (highest first)\n",
    "        Each dict contains: domain, skill_match_ratio, matched_skills, missing_skills, counts\n",
    "    \"\"\"\n",
    "    suggestions = []\n",
    "    \n",
    "    for domain_key, domain_info in domain_requirements.items():\n",
    "        domain_name = domain_info[\"domain\"]\n",
    "        \n",
    "        # Skip the current domain\n",
    "        if domain_name == current_domain:\n",
    "            continue\n",
    "        \n",
    "        required_skills = domain_info[\"required_skills\"]\n",
    "        matched, missing, ratio = compute_skill_matches(candidate_skills, required_skills)\n",
    "        \n",
    "        suggestions.append({\n",
    "            'domain': domain_name,\n",
    "            'domain_key': domain_key,\n",
    "            'skill_match_ratio': ratio,\n",
    "            'matched_skills': matched,\n",
    "            'missing_skills': missing,\n",
    "            'matched_count': len(matched),\n",
    "            'required_count': len(required_skills)\n",
    "        })\n",
    "    \n",
    "    # Sort by skill match ratio (highest first)\n",
    "    suggestions.sort(key=lambda x: x['skill_match_ratio'], reverse=True)\n",
    "    \n",
    "    return suggestions[:top_n]\n",
    "\n",
    "# 3.4 Project & experience features\n",
    "def extract_project_features(projects):\n",
    "    \"\"\"\n",
    "    Extract features from projects list\n",
    "    Returns: project_count (int), project_embeddings (optional)\n",
    "    \"\"\"\n",
    "    project_count = len(projects) if projects else 0\n",
    "    \n",
    "    # Simple project text concatenation for basic text features\n",
    "    project_text = \" \".join(projects) if projects else \"\"\n",
    "    \n",
    "    return project_count, project_text\n",
    "\n",
    "def extract_experience_features(work_experience):\n",
    "    \"\"\"\n",
    "    Extract features from work experience\n",
    "    Returns: years_experience (float), max_years (int), experience_text (str)\n",
    "    \"\"\"\n",
    "    if not work_experience:\n",
    "        return 0.0, 0, \"\"\n",
    "    \n",
    "    # Total years of experience\n",
    "    years_experience = sum(item.get('years', 0) for item in work_experience)\n",
    "    \n",
    "    # Maximum years in any single role\n",
    "    max_years = max(item.get('years', 0) for item in work_experience)\n",
    "    \n",
    "    # Concatenate job titles for text features\n",
    "    job_titles = [item.get('title', '') for item in work_experience]\n",
    "    experience_text = \" \".join(job_titles)\n",
    "    \n",
    "    return float(years_experience), max_years, experience_text\n",
    "\n",
    "# 3.5 Test score normalization\n",
    "def normalize_test_score(test_score):\n",
    "    \"\"\"\n",
    "    Normalize test score to [0,1] range\n",
    "    Example: 88 → 88 ÷ 100 = 0.88\n",
    "    \"\"\"\n",
    "    # Ensure test_score is numeric and within valid range\n",
    "    if test_score is None:\n",
    "        return 0.0\n",
    "    \n",
    "    # Clamp to [0, 100] range\n",
    "    clamped_score = max(0, min(100, float(test_score)))\n",
    "    \n",
    "    # Normalize to [0,1]\n",
    "    test_score_norm = clamped_score / 100.0\n",
    "    \n",
    "    return test_score_norm\n",
    "\n",
    "# 3.6 Numeric feature scaling (will be fitted on training data)\n",
    "class ResumeFeatureScaler:\n",
    "    \"\"\"\n",
    "    Scaler for numeric resume features\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.project_scaler = StandardScaler()\n",
    "        self.experience_scaler = StandardScaler()\n",
    "        self.is_fitted = False\n",
    "    \n",
    "    def fit(self, numeric_features):\n",
    "        \"\"\"\n",
    "        Fit scalers on training data\n",
    "        numeric_features: array of shape (n_samples, n_features)\n",
    "        Expected features: [years_experience, max_years, project_count]\n",
    "        \"\"\"\n",
    "        if len(numeric_features) == 0:\n",
    "            return self\n",
    "            \n",
    "        numeric_array = np.array(numeric_features)\n",
    "        \n",
    "        # Fit separate scalers for different feature types\n",
    "        if numeric_array.shape[1] >= 3:\n",
    "            # Years experience and max years\n",
    "            experience_data = numeric_array[:, :2].reshape(-1, 2)\n",
    "            self.experience_scaler.fit(experience_data)\n",
    "            \n",
    "            # Project count\n",
    "            project_data = numeric_array[:, 2:3].reshape(-1, 1)\n",
    "            self.project_scaler.fit(project_data)\n",
    "            \n",
    "        self.is_fitted = True\n",
    "        return self\n",
    "    \n",
    "    def transform(self, numeric_features):\n",
    "        \"\"\"\n",
    "        Transform numeric features using fitted scalers\n",
    "        \"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Scaler must be fitted before transform\")\n",
    "            \n",
    "        if len(numeric_features) == 0:\n",
    "            return np.array([])\n",
    "            \n",
    "        numeric_array = np.array(numeric_features)\n",
    "        \n",
    "        if numeric_array.ndim == 1:\n",
    "            numeric_array = numeric_array.reshape(1, -1)\n",
    "        \n",
    "        scaled_features = []\n",
    "        \n",
    "        if numeric_array.shape[1] >= 3:\n",
    "            # Scale experience features\n",
    "            experience_data = numeric_array[:, :2]\n",
    "            scaled_experience = self.experience_scaler.transform(experience_data)\n",
    "            \n",
    "            # Scale project features  \n",
    "            project_data = numeric_array[:, 2:3]\n",
    "            scaled_projects = self.project_scaler.transform(project_data)\n",
    "            \n",
    "            # Combine scaled features\n",
    "            scaled_features = np.concatenate([scaled_experience, scaled_projects], axis=1)\n",
    "        \n",
    "        return scaled_features\n",
    "    \n",
    "    def fit_transform(self, numeric_features):\n",
    "        \"\"\"\n",
    "        Fit and transform in one step\n",
    "        \"\"\"\n",
    "        return self.fit(numeric_features).transform(numeric_features)\n",
    "\n",
    "# UPDATED: Complete feature engineering pipeline with alternative domains\n",
    "def extract_all_features(resume, skill_vocab, domain_requirements):\n",
    "    \"\"\"\n",
    "    Extract all features from a single resume\n",
    "    Returns: feature dictionary with all computed features including alternative domain suggestions\n",
    "    \"\"\"\n",
    "    # Get domain requirements\n",
    "    domain_key = None\n",
    "    for key, req in domain_requirements.items():\n",
    "        if req[\"domain\"] == resume[\"preferred_domain\"]:\n",
    "            domain_key = key\n",
    "            break\n",
    "    \n",
    "    if domain_key is None:\n",
    "        raise ValueError(f\"No requirements found for domain {resume['preferred_domain']}\")\n",
    "    \n",
    "    required_skills = domain_requirements[domain_key][\"required_skills\"]\n",
    "    \n",
    "    # Extract individual feature components\n",
    "    candidate_skills = resume.get('skills', [])\n",
    "    projects = resume.get('projects', [])\n",
    "    work_experience = resume.get('work_experience', [])\n",
    "    test_score = resume.get('test_score', 0)\n",
    "    \n",
    "    # Compute skill features\n",
    "    skill_vector = encode_skills(candidate_skills, skill_vocab)\n",
    "    matched_skills, missing_skills, skill_match_ratio = compute_skill_matches(\n",
    "        candidate_skills, required_skills\n",
    "    )\n",
    "    \n",
    "    # Compute project features\n",
    "    project_count, project_text = extract_project_features(projects)\n",
    "    \n",
    "    # Compute experience features\n",
    "    years_experience, max_years, experience_text = extract_experience_features(work_experience)\n",
    "    \n",
    "    # Normalize test score\n",
    "    test_score_norm = normalize_test_score(test_score)\n",
    "    \n",
    "    # Combine numeric features for scaling\n",
    "    numeric_features = [years_experience, max_years, project_count]\n",
    "    \n",
    "    # NEW: Compute alternative domain suggestions\n",
    "    alternative_domains = suggest_alternative_domains(\n",
    "        candidate_skills, \n",
    "        resume['preferred_domain'], \n",
    "        domain_requirements,\n",
    "        top_n=3\n",
    "    )\n",
    "    \n",
    "    # Return comprehensive feature dictionary\n",
    "    return {\n",
    "        'skill_vector': skill_vector,\n",
    "        'skill_match_ratio': skill_match_ratio,\n",
    "        'matched_skills': matched_skills,\n",
    "        'missing_skills': missing_skills,\n",
    "        'project_count': project_count,\n",
    "        'project_text': project_text,\n",
    "        'years_experience': years_experience,\n",
    "        'max_years': max_years,\n",
    "        'experience_text': experience_text,\n",
    "        'test_score': test_score,\n",
    "        'test_score_norm': test_score_norm,\n",
    "        'numeric_features': numeric_features,\n",
    "        'domain': resume['preferred_domain'],\n",
    "        'id': resume['id'],\n",
    "        'alternative_domains': alternative_domains  # NEW FIELD\n",
    "    }\n",
    "\n",
    "# Apply feature engineering to all resumes\n",
    "print(\"Building skill vocabulary...\")\n",
    "skill_vocab = build_skill_vocabulary(labeled_resumes, domain_requirements)\n",
    "print(f\"Built skill vocabulary with {len(skill_vocab)} unique skills\")\n",
    "\n",
    "print(\"\\nExtracting features from all resumes...\")\n",
    "all_features = []\n",
    "for resume in labeled_resumes:\n",
    "    try:\n",
    "        features = extract_all_features(resume, skill_vocab, domain_requirements)\n",
    "        features['label'] = resume['label']  # Add label for supervised learning\n",
    "        all_features.append(features)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing resume {resume.get('id', 'unknown')}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"Successfully extracted features from {len(all_features)} resumes\")\n",
    "\n",
    "# Prepare numeric features for scaling\n",
    "print(\"\\nPreparing numeric features for scaling...\")\n",
    "numeric_feature_matrix = []\n",
    "for features in all_features:\n",
    "    numeric_feature_matrix.append(features['numeric_features'])\n",
    "\n",
    "# Initialize and fit scaler\n",
    "scaler = ResumeFeatureScaler()\n",
    "scaler.fit(numeric_feature_matrix)\n",
    "\n",
    "# Apply scaling to all features\n",
    "for features in all_features:\n",
    "    scaled_numeric = scaler.transform([features['numeric_features']])\n",
    "    features['scaled_numeric_features'] = scaled_numeric[0]\n",
    "\n",
    "print(\"Numeric feature scaling completed\")\n",
    "\n",
    "# Save feature engineering artifacts\n",
    "print(\"\\nSaving feature engineering artifacts...\")\n",
    "\n",
    "# Save skill vocabulary\n",
    "with open('data/skill_vocab.json', 'w') as f:\n",
    "    json.dump(skill_vocab, f, indent=2)\n",
    "\n",
    "# Save processed features (sample for inspection)\n",
    "sample_features = all_features[:5]  # Save first 5 for inspection\n",
    "with open('data/sample_features.json', 'w') as f:\n",
    "    # Convert numpy arrays to lists for JSON serialization\n",
    "    for features in sample_features:\n",
    "        features['skill_vector'] = features['skill_vector'].tolist()\n",
    "        features['scaled_numeric_features'] = features['scaled_numeric_features'].tolist()\n",
    "    json.dump(sample_features, f, indent=2)\n",
    "\n",
    "# Display feature engineering results\n",
    "print(\"\\n=== Feature Engineering Results ===\")\n",
    "print(f\"Skill vocabulary size: {len(skill_vocab)}\")\n",
    "print(f\"Sample skills: {skill_vocab[:10]}\")\n",
    "\n",
    "print(f\"\\nFeature extraction completed for {len(all_features)} resumes\")\n",
    "\n",
    "# Show sample feature summary\n",
    "if all_features:\n",
    "    sample = all_features[0]\n",
    "    print(f\"\\n=== Sample Feature Summary (ID: {sample['id']}) ===\")\n",
    "    print(f\"Domain: {sample['domain']}\")\n",
    "    print(f\"Skill vector shape: ({len(sample['skill_vector'])},)\")\n",
    "    print(f\"Skill match ratio: {sample['skill_match_ratio']:.3f}\")\n",
    "    print(f\"Matched skills: {len(sample['matched_skills'])}\")\n",
    "    print(f\"Missing skills: {len(sample['missing_skills'])}\")\n",
    "    print(f\"Project count: {sample['project_count']}\")\n",
    "    print(f\"Years experience: {sample['years_experience']}\")\n",
    "    print(f\"Test score (raw): {sample['test_score']}\")\n",
    "    print(f\"Test score (normalized): {sample['test_score_norm']:.3f}\")\n",
    "    print(f\"Scaled numeric features: {sample['scaled_numeric_features']}\")\n",
    "    \n",
    "    # NEW: Display alternative domain suggestions\n",
    "    print(f\"\\n=== Alternative Domain Suggestions ===\")\n",
    "    for i, alt_domain in enumerate(sample['alternative_domains'], 1):\n",
    "        print(f\"{i}. {alt_domain['domain']}: {alt_domain['skill_match_ratio']:.3f} match \"\n",
    "              f\"({alt_domain['matched_count']}/{alt_domain['required_count']} skills)\")\n",
    "\n",
    "# Feature distribution analysis\n",
    "print(\"\\n=== Feature Distribution Analysis ===\")\n",
    "df_features = pd.DataFrame([\n",
    "    {\n",
    "        'label': f['label'],\n",
    "        'skill_match_ratio': f['skill_match_ratio'],\n",
    "        'project_count': f['project_count'],\n",
    "        'years_experience': f['years_experience'],\n",
    "        'test_score_norm': f['test_score_norm'],\n",
    "        'domain': f['domain']\n",
    "    }\n",
    "    for f in all_features\n",
    "])\n",
    "\n",
    "print(\"Feature statistics by label:\")\n",
    "print(df_features.groupby('label')[['skill_match_ratio', 'project_count', 'years_experience', 'test_score_norm']].agg(['mean', 'std']).round(3))\n",
    "\n",
    "# NEW: Analyze alternative domain suggestions by label\n",
    "print(\"\\n=== Alternative Domain Analysis by Label ===\")\n",
    "for label in ['Fit', 'Partial Fit', 'Not Fit']:\n",
    "    label_features = [f for f in all_features if f['label'] == label]\n",
    "    if label_features:\n",
    "        avg_top_match = np.mean([f['alternative_domains'][0]['skill_match_ratio'] \n",
    "                                  for f in label_features if f['alternative_domains']])\n",
    "        print(f\"{label}: Average top alternative domain match = {avg_top_match:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebf044d-b36c-4f24-850d-951c42aae589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4 - Final Feature Vector Construction\n",
    "print(\"\\n=== Step 4: Final Feature Vector Construction ===\")\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pickle\n",
    "import joblib\n",
    "\n",
    "class FeatureVectorBuilder:\n",
    "    \"\"\"\n",
    "    Builds final feature vectors for model input using parallel branches\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, skill_vocab_size, use_text_embeddings=False, text_embedding_dim=128):\n",
    "        self.skill_vocab_size = skill_vocab_size\n",
    "        self.use_text_embeddings = use_text_embeddings\n",
    "        self.text_embedding_dim = text_embedding_dim\n",
    "        \n",
    "        # Initialize text vectorizers (will be fitted on training data)\n",
    "        self.project_vectorizer = TfidfVectorizer(\n",
    "            max_features=64, \n",
    "            stop_words='english',\n",
    "            ngram_range=(1, 2),\n",
    "            min_df=2\n",
    "        )\n",
    "        self.experience_vectorizer = TfidfVectorizer(\n",
    "            max_features=64,\n",
    "            stop_words='english', \n",
    "            ngram_range=(1, 2),\n",
    "            min_df=2\n",
    "        )\n",
    "        \n",
    "        self.is_fitted = False\n",
    "        \n",
    "    def fit_text_vectorizers(self, all_features):\n",
    "        \"\"\"\n",
    "        Fit text vectorizers on training data\n",
    "        \"\"\"\n",
    "        # Extract all project and experience texts\n",
    "        project_texts = []\n",
    "        experience_texts = []\n",
    "        \n",
    "        for features in all_features:\n",
    "            project_text = features.get('project_text', '')\n",
    "            experience_text = features.get('experience_text', '')\n",
    "            \n",
    "            # Use placeholder if empty to avoid fitting issues\n",
    "            project_texts.append(project_text if project_text else 'no projects')\n",
    "            experience_texts.append(experience_text if experience_text else 'no experience')\n",
    "        \n",
    "        # Fit vectorizers\n",
    "        self.project_vectorizer.fit(project_texts)\n",
    "        self.experience_vectorizer.fit(experience_texts)\n",
    "        \n",
    "        self.is_fitted = True\n",
    "        print(\"Text vectorizers fitted successfully\")\n",
    "    \n",
    "    def build_skill_branch(self, features):\n",
    "        \"\"\"\n",
    "        Branch 1: Skills branch\n",
    "        Returns: skill_vector (binary) + skill_match_ratio\n",
    "        \"\"\"\n",
    "        skill_vector = features['skill_vector']  # Already computed in Step 3\n",
    "        skill_match_ratio = np.array([features['skill_match_ratio']])  # Scalar as vector\n",
    "        \n",
    "        # Combine skill vector with match ratio\n",
    "        skill_branch = np.concatenate([skill_vector, skill_match_ratio])\n",
    "        \n",
    "        return skill_branch\n",
    "    \n",
    "    def build_numeric_branch(self, features):\n",
    "        \"\"\"\n",
    "        Branch 2: Numeric branch\n",
    "        Returns: [test_score_norm, project_count_scaled, years_experience_scaled, skill_match_ratio]\n",
    "        \"\"\"\n",
    "        test_score_norm = features['test_score_norm']\n",
    "        skill_match_ratio = features['skill_match_ratio']\n",
    "        \n",
    "        # Get scaled numeric features: [years_experience_scaled, max_years_scaled, project_count_scaled]\n",
    "        scaled_features = features['scaled_numeric_features']\n",
    "        years_experience_scaled = scaled_features[0]\n",
    "        project_count_scaled = scaled_features[2]  # Skip max_years for now, use project_count\n",
    "        \n",
    "        # Build numeric vector: [test_score_norm, project_count_scaled, years_experience_scaled, skill_match_ratio]\n",
    "        numeric_branch = np.array([\n",
    "            test_score_norm,\n",
    "            project_count_scaled, \n",
    "            years_experience_scaled,\n",
    "            skill_match_ratio\n",
    "        ])\n",
    "        \n",
    "        return numeric_branch\n",
    "    \n",
    "    def build_text_branch(self, features):\n",
    "        \"\"\"\n",
    "        Branch 3: Text branch (optional)\n",
    "        Returns: text embeddings from projects and experience\n",
    "        \"\"\"\n",
    "        if not self.use_text_embeddings or not self.is_fitted:\n",
    "            return np.array([])\n",
    "        \n",
    "        project_text = features.get('project_text', '')\n",
    "        experience_text = features.get('experience_text', '')\n",
    "        \n",
    "        # Use placeholder if empty\n",
    "        if not project_text:\n",
    "            project_text = 'no projects'\n",
    "        if not experience_text:\n",
    "            experience_text = 'no experience'\n",
    "        \n",
    "        # Transform to TF-IDF vectors\n",
    "        project_vector = self.project_vectorizer.transform([project_text]).toarray().flatten()\n",
    "        experience_vector = self.experience_vectorizer.transform([experience_text]).toarray().flatten()\n",
    "        \n",
    "        # Combine text features\n",
    "        text_branch = np.concatenate([project_vector, experience_vector])\n",
    "        \n",
    "        return text_branch\n",
    "    \n",
    "    def build_final_vector(self, features):\n",
    "        \"\"\"\n",
    "        Concatenate all branches into final feature vector\n",
    "        Returns: final_vector for model input\n",
    "        \"\"\"\n",
    "        # Build individual branches\n",
    "        skill_branch = self.build_skill_branch(features)\n",
    "        numeric_branch = self.build_numeric_branch(features)\n",
    "        \n",
    "        # Start with skill and numeric branches\n",
    "        branches = [skill_branch, numeric_branch]\n",
    "        \n",
    "        # Add text branch if enabled\n",
    "        if self.use_text_embeddings and self.is_fitted:\n",
    "            text_branch = self.build_text_branch(features)\n",
    "            if len(text_branch) > 0:\n",
    "                branches.append(text_branch)\n",
    "        \n",
    "        # Concatenate all branches\n",
    "        final_vector = np.concatenate(branches)\n",
    "        \n",
    "        return final_vector\n",
    "    \n",
    "    def get_feature_dimensions(self):\n",
    "        \"\"\"\n",
    "        Return dimensions of each branch and final vector\n",
    "        \"\"\"\n",
    "        skill_dim = self.skill_vocab_size + 1  # +1 for skill_match_ratio\n",
    "        numeric_dim = 4  # [test_score_norm, project_count_scaled, years_experience_scaled, skill_match_ratio]\n",
    "        text_dim = 128 if self.use_text_embeddings else 0  # 64 + 64 for project + experience TF-IDF\n",
    "        \n",
    "        final_dim = skill_dim + numeric_dim + text_dim\n",
    "        \n",
    "        return {\n",
    "            'skill_branch_dim': skill_dim,\n",
    "            'numeric_branch_dim': numeric_dim, \n",
    "            'text_branch_dim': text_dim,\n",
    "            'final_vector_dim': final_dim\n",
    "        }\n",
    "\n",
    "# Initialize feature vector builder\n",
    "print(\"Initializing feature vector builder...\")\n",
    "skill_vocab_size = len(skill_vocab)\n",
    "use_text_features = True  # Enable text embeddings\n",
    "\n",
    "vector_builder = FeatureVectorBuilder(\n",
    "    skill_vocab_size=skill_vocab_size,\n",
    "    use_text_embeddings=use_text_features,\n",
    "    text_embedding_dim=128\n",
    ")\n",
    "\n",
    "# Fit text vectorizers on all features\n",
    "if use_text_features:\n",
    "    print(\"Fitting text vectorizers...\")\n",
    "    vector_builder.fit_text_vectorizers(all_features)\n",
    "\n",
    "# Build feature vectors for all resumes\n",
    "print(\"Building final feature vectors...\")\n",
    "feature_vectors = []\n",
    "labels = []\n",
    "\n",
    "for features in all_features:\n",
    "    try:\n",
    "        final_vector = vector_builder.build_final_vector(features)\n",
    "        feature_vectors.append(final_vector)\n",
    "        labels.append(features['label'])\n",
    "    except Exception as e:\n",
    "        print(f\"Error building vector for {features.get('id', 'unknown')}: {e}\")\n",
    "        continue\n",
    "\n",
    "# Convert to numpy arrays\n",
    "X = np.array(feature_vectors)\n",
    "y = np.array(labels)\n",
    "\n",
    "print(f\"Built feature vectors for {len(feature_vectors)} resumes\")\n",
    "\n",
    "# Display feature vector dimensions\n",
    "dimensions = vector_builder.get_feature_dimensions()\n",
    "print(f\"\\n=== Feature Vector Dimensions ===\")\n",
    "print(f\"Skill branch: {dimensions['skill_branch_dim']} features\")\n",
    "print(f\"  - Skill vocabulary: {skill_vocab_size} binary features\")\n",
    "print(f\"  - Skill match ratio: 1 scalar feature\")\n",
    "print(f\"Numeric branch: {dimensions['numeric_branch_dim']} features\")\n",
    "print(f\"  - test_score_norm: 1 feature\")\n",
    "print(f\"  - project_count_scaled: 1 feature\") \n",
    "print(f\"  - years_experience_scaled: 1 feature\")\n",
    "print(f\"  - skill_match_ratio: 1 feature\")\n",
    "print(f\"Text branch: {dimensions['text_branch_dim']} features\")\n",
    "print(f\"  - Project TF-IDF: {64 if use_text_features else 0} features\")\n",
    "print(f\"  - Experience TF-IDF: {64 if use_text_features else 0} features\")\n",
    "print(f\"\\nFinal vector dimensions: {dimensions['final_vector_dim']} features\")\n",
    "print(f\"Actual X shape: {X.shape}\")\n",
    "\n",
    "# Analyze feature vector properties\n",
    "print(f\"\\n=== Feature Vector Analysis ===\")\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")\n",
    "print(f\"X dtype: {X.dtype}\")\n",
    "print(f\"Feature vector sparsity: {np.mean(X == 0):.3f} (fraction of zeros)\")\n",
    "\n",
    "# Label distribution\n",
    "unique_labels, label_counts = np.unique(y, return_counts=True)\n",
    "print(f\"\\nLabel distribution:\")\n",
    "for label, count in zip(unique_labels, label_counts):\n",
    "    print(f\"  {label}: {count} ({count/len(y)*100:.1f}%)\")\n",
    "\n",
    "# Sample feature vector breakdown\n",
    "if len(feature_vectors) > 0:\n",
    "    sample_idx = 0\n",
    "    sample_features = all_features[sample_idx]\n",
    "    sample_vector = feature_vectors[sample_idx]\n",
    "    \n",
    "    print(f\"\\n=== Sample Feature Vector Breakdown (ID: {sample_features['id']}) ===\")\n",
    "    \n",
    "    # Skill branch breakdown\n",
    "    skill_branch = vector_builder.build_skill_branch(sample_features)\n",
    "    print(f\"Skill branch ({len(skill_branch)} features):\")\n",
    "    print(f\"  - Skill vector sum: {np.sum(skill_branch[:-1])} active skills\")\n",
    "    print(f\"  - Skill match ratio: {skill_branch[-1]:.3f}\")\n",
    "    \n",
    "    # Numeric branch breakdown  \n",
    "    numeric_branch = vector_builder.build_numeric_branch(sample_features)\n",
    "    print(f\"Numeric branch ({len(numeric_branch)} features):\")\n",
    "    print(f\"  - Test score norm: {numeric_branch[0]:.3f}\")\n",
    "    print(f\"  - Project count scaled: {numeric_branch[1]:.3f}\")\n",
    "    print(f\"  - Years experience scaled: {numeric_branch[2]:.3f}\")\n",
    "    print(f\"  - Skill match ratio: {numeric_branch[3]:.3f}\")\n",
    "    \n",
    "    # Text branch breakdown\n",
    "    if use_text_features:\n",
    "        text_branch = vector_builder.build_text_branch(sample_features)\n",
    "        print(f\"Text branch ({len(text_branch)} features):\")\n",
    "        print(f\"  - Project TF-IDF non-zero: {np.count_nonzero(text_branch[:64])}\")\n",
    "        print(f\"  - Experience TF-IDF non-zero: {np.count_nonzero(text_branch[64:])}\")\n",
    "\n",
    "# Save feature vectors and artifacts\n",
    "print(f\"\\n=== Saving Feature Vector Artifacts ===\")\n",
    "\n",
    "# Save feature vectors\n",
    "np.save('data/X_features.npy', X)\n",
    "np.save('data/y_labels.npy', y)\n",
    "\n",
    "# Save feature builder\n",
    "with open('data/feature_vector_builder.pkl', 'wb') as f:\n",
    "    pickle.dump(vector_builder, f)\n",
    "\n",
    "# Save label mapping\n",
    "label_to_idx = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "idx_to_label = {idx: label for idx, label in enumerate(unique_labels)}\n",
    "\n",
    "with open('data/label_mapping.json', 'w') as f:\n",
    "    json.dump({\n",
    "        'label_to_idx': label_to_idx,\n",
    "        'idx_to_label': idx_to_label,\n",
    "        'unique_labels': unique_labels.tolist()\n",
    "    }, f, indent=2)\n",
    "\n",
    "# Save feature dimensions info\n",
    "with open('data/feature_dimensions.json', 'w') as f:\n",
    "    json.dump(dimensions, f, indent=2)\n",
    "\n",
    "print(\"Saved feature vector artifacts:\")\n",
    "print(\"- X_features.npy: Feature matrix\")\n",
    "print(\"- y_labels.npy: Label array\") \n",
    "print(\"- feature_vector_builder.pkl: Trained feature builder\")\n",
    "print(\"- label_mapping.json: Label encoding mappings\")\n",
    "print(\"- feature_dimensions.json: Feature dimensions info\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d432458-4ad1-43e8-8e2d-0343d48a40e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5 - Model Architecture (Keras/TensorFlow)\n",
    "print(\"\\n=== Step 5: Model Architecture ===\")\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, Model, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "import seaborn as sns\n",
    "\n",
    "# Set tensorflow random seed for reproducibility\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "class ResumeClassifierModel:\n",
    "    \"\"\"\n",
    "    Hybrid neural network for resume classification with parallel branches\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, skill_vocab_size, numeric_dim, text_dim=0, num_classes=3):\n",
    "        self.skill_vocab_size = skill_vocab_size\n",
    "        self.numeric_dim = numeric_dim\n",
    "        self.text_dim = text_dim\n",
    "        self.num_classes = num_classes\n",
    "        self.use_text_branch = text_dim > 0\n",
    "        \n",
    "        self.model = None\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.is_compiled = False\n",
    "        \n",
    "    def build_model(self):\n",
    "        \"\"\"\n",
    "        Build hybrid model architecture with parallel branches\n",
    "        \"\"\"\n",
    "        # 5.1 Inputs\n",
    "        skill_input = Input(shape=(self.skill_vocab_size + 1,), name='skill_input')  # +1 for skill_match_ratio\n",
    "        numeric_input = Input(shape=(self.numeric_dim,), name='numeric_input')\n",
    "        \n",
    "        inputs = [skill_input, numeric_input]\n",
    "        \n",
    "        # 5.2 Skills branch (dense)\n",
    "        # Note: We're using skill_vocab_size + 1 because we concatenated skill_match_ratio\n",
    "        x1 = layers.Dense(256, activation='relu', name='skill_dense1')(skill_input)\n",
    "        x1 = layers.Dropout(0.3, name='skill_dropout1')(x1)\n",
    "        x1 = layers.Dense(128, activation='relu', name='skill_dense2')(x1)\n",
    "        \n",
    "        # 5.3 Numeric branch (dense)\n",
    "        x2 = layers.Dense(32, activation='relu', name='numeric_dense1')(numeric_input)\n",
    "        x2 = layers.Dense(16, activation='relu', name='numeric_dense2')(x2)\n",
    "        \n",
    "        branches_to_concat = [x1, x2]\n",
    "        \n",
    "        # 5.4 Project/text branch (if using embeddings)\n",
    "        if self.use_text_branch:\n",
    "            text_input = Input(shape=(self.text_dim,), name='text_input')\n",
    "            inputs.append(text_input)\n",
    "            \n",
    "            x3 = layers.Dense(128, activation='relu', name='text_dense1')(text_input)\n",
    "            x3 = layers.Dense(64, activation='relu', name='text_dense2')(x3)\n",
    "            branches_to_concat.append(x3)\n",
    "        \n",
    "        # 5.5 Concatenate\n",
    "        concat = layers.concatenate(branches_to_concat, name='concat_branches')\n",
    "        h = layers.Dense(128, activation='relu', name='final_dense1')(concat)\n",
    "        h = layers.Dropout(0.3, name='final_dropout')(h)\n",
    "        h = layers.Dense(64, activation='relu', name='final_dense2')(h)\n",
    "        \n",
    "        # 5.6 Output\n",
    "        output = layers.Dense(self.num_classes, activation='softmax', name='output')(h)\n",
    "        \n",
    "        # Create model\n",
    "        self.model = Model(inputs=inputs, outputs=output, name='resume_classifier')\n",
    "        \n",
    "        return self.model\n",
    "    \n",
    "    def compile_model(self, learning_rate=1e-3):\n",
    "        \"\"\"\n",
    "        5.7 Compile model with specified optimizer and loss\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            self.build_model()\n",
    "        \n",
    "        self.model.compile(\n",
    "            loss='categorical_crossentropy',\n",
    "            optimizer=Adam(learning_rate=learning_rate),\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        self.is_compiled = True\n",
    "        print(\"Model compiled successfully\")\n",
    "        \n",
    "    def get_model_summary(self):\n",
    "        \"\"\"\n",
    "        Display model architecture summary\n",
    "        \"\"\"\n",
    "        if self.model is not None:\n",
    "            return self.model.summary()\n",
    "        else:\n",
    "            print(\"Model not built yet\")\n",
    "    \n",
    "    def prepare_inputs(self, X, feature_builder):\n",
    "        \"\"\"\n",
    "        Split feature vector back into separate inputs for the model\n",
    "        \"\"\"\n",
    "        skill_dim = self.skill_vocab_size + 1  # +1 for skill_match_ratio\n",
    "        numeric_dim = self.numeric_dim\n",
    "        \n",
    "        # Extract skill features (first skill_dim features)\n",
    "        skill_features = X[:, :skill_dim]\n",
    "        \n",
    "        # Extract numeric features (next numeric_dim features)\n",
    "        numeric_features = X[:, skill_dim:skill_dim + numeric_dim]\n",
    "        \n",
    "        inputs = [skill_features, numeric_features]\n",
    "        \n",
    "        # Extract text features if using text branch\n",
    "        if self.use_text_branch:\n",
    "            text_features = X[:, skill_dim + numeric_dim:]\n",
    "            inputs.append(text_features)\n",
    "        \n",
    "        return inputs\n",
    "\n",
    "# Load feature dimensions\n",
    "with open('data/feature_dimensions.json', 'r') as f:\n",
    "    dimensions = json.load(f)\n",
    "\n",
    "# Load label mapping\n",
    "with open('data/label_mapping.json', 'r') as f:\n",
    "    label_mapping = json.load(f)\n",
    "\n",
    "# Initialize model\n",
    "print(\"Initializing resume classifier model...\")\n",
    "model_classifier = ResumeClassifierModel(\n",
    "    skill_vocab_size=len(skill_vocab),\n",
    "    numeric_dim=dimensions['numeric_branch_dim'],\n",
    "    text_dim=dimensions['text_branch_dim'] if use_text_features else 0,\n",
    "    num_classes=len(label_mapping['unique_labels'])\n",
    ")\n",
    "\n",
    "# Build and compile model\n",
    "print(\"Building model architecture...\")\n",
    "model = model_classifier.build_model()\n",
    "model_classifier.compile_model()\n",
    "\n",
    "# Display model summary\n",
    "print(\"\\n=== Model Architecture Summary ===\")\n",
    "model_classifier.get_model_summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04277fd-f416-4591-a764-0f423f6250ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6 - Training Procedure\n",
    "print(\"\\n=== Step 6: Training Procedure ===\")\n",
    "\n",
    "# Encode labels for training\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "y_categorical = tf.keras.utils.to_categorical(y_encoded, num_classes=len(label_mapping['unique_labels']))\n",
    "\n",
    "print(f\"Label encoding mapping:\")\n",
    "for i, label in enumerate(label_encoder.classes_):\n",
    "    print(f\"  {label} -> {i}\")\n",
    "\n",
    "# 6.1 Train/val/test split: 70/15/15 stratified by label\n",
    "print(\"\\nSplitting dataset (70/15/15)...\")\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y_categorical, \n",
    "    test_size=0.3,  # 30% for temp (15% val + 15% test)\n",
    "    random_state=SEED,\n",
    "    stratify=y_encoded\n",
    ")\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp,\n",
    "    test_size=0.5,  # 50% of 30% = 15% each\n",
    "    random_state=SEED,\n",
    "    stratify=y_temp.argmax(axis=1)\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Validation set: {X_val.shape[0]} samples\") \n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "\n",
    "# Prepare inputs for each split\n",
    "train_inputs = model_classifier.prepare_inputs(X_train, vector_builder)\n",
    "val_inputs = model_classifier.prepare_inputs(X_val, vector_builder)\n",
    "test_inputs = model_classifier.prepare_inputs(X_test, vector_builder)\n",
    "\n",
    "print(f\"Input shapes for training:\")\n",
    "for i, inp in enumerate(train_inputs):\n",
    "    print(f\"  Input {i}: {inp.shape}\")\n",
    "\n",
    "# 6.5 Compute class weights\n",
    "print(\"\\nComputing class weights...\")\n",
    "class_weights_array = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_encoded),\n",
    "    y=y_train.argmax(axis=1)\n",
    ")\n",
    "\n",
    "class_weight_dict = {i: weight for i, weight in enumerate(class_weights_array)}\n",
    "print(\"Class weights:\", class_weight_dict)\n",
    "\n",
    "# Display class distribution\n",
    "train_labels = y_train.argmax(axis=1)\n",
    "unique_train_labels, train_counts = np.unique(train_labels, return_counts=True)\n",
    "print(\"Training set class distribution:\")\n",
    "for label_idx, count in zip(unique_train_labels, train_counts):\n",
    "    label_name = label_encoder.classes_[label_idx]\n",
    "    print(f\"  {label_name}: {count} ({count/len(train_labels)*100:.1f}%)\")\n",
    "\n",
    "# 6.4 Setup callbacks\n",
    "print(\"\\nSetting up training callbacks...\")\n",
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=5,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ModelCheckpoint(\n",
    "        filepath='models/best_resume_classifier.h5',\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "# 6.2 & 6.3 Training hyperparameters\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 50\n",
    "\n",
    "print(f\"\\nTraining hyperparameters:\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Max epochs: {EPOCHS}\")\n",
    "print(f\"  Early stopping patience: 5\")\n",
    "\n",
    "# 6.6 Train the model\n",
    "print(f\"\\n=== Starting Model Training ===\")\n",
    "print(\"Training in progress...\")\n",
    "\n",
    "history = model.fit(\n",
    "    train_inputs,\n",
    "    y_train,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=(val_inputs, y_val),\n",
    "    class_weight=class_weight_dict,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nTraining completed!\")\n",
    "\n",
    "# Save training history\n",
    "with open('data/training_history.json', 'w') as f:\n",
    "    # Convert numpy arrays to lists for JSON serialization\n",
    "    history_dict = {}\n",
    "    for key, values in history.history.items():\n",
    "        history_dict[key] = [float(v) for v in values]\n",
    "    json.dump(history_dict, f, indent=2)\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Loss plot\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss', marker='o')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss', marker='s')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy plot\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy', marker='o')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy', marker='s')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Learning rate plot (if available)\n",
    "plt.subplot(1, 3, 3)\n",
    "epochs_range = range(1, len(history.history['loss']) + 1)\n",
    "plt.plot(epochs_range, history.history['loss'], 'b-', label='Training Loss')\n",
    "plt.plot(epochs_range, history.history['val_loss'], 'r-', label='Validation Loss')\n",
    "plt.title('Training vs Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('data/training_history.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42003bf5-7f78-446d-8c9c-319b9f630870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7 - Enhanced Metrics & Evaluation\n",
    "# Evaluate on test set\n",
    "print(f\"\\n=== Model Evaluation ===\")\n",
    "test_loss, test_accuracy = model.evaluate(test_inputs, y_test, verbose=0)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Predictions on test set\n",
    "y_pred_probs = model.predict(test_inputs, verbose=0)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Classification report\n",
    "print(f\"\\n=== Classification Report ===\")\n",
    "target_names = label_encoder.classes_\n",
    "print(classification_report(y_true, y_pred, target_names=target_names))\n",
    "\n",
    "# Confusion matrix\n",
    "print(f\"\\n=== Confusion Matrix ===\")\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print(cm)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=target_names, yticklabels=target_names)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.tight_layout()\n",
    "plt.savefig('data/confusion_matrix.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# F1 scores\n",
    "f1_macro = f1_score(y_true, y_pred, average='macro')\n",
    "f1_weighted = f1_score(y_true, y_pred, average='weighted')\n",
    "print(f\"F1 Score (Macro): {f1_macro:.4f}\")\n",
    "print(f\"F1 Score (Weighted): {f1_weighted:.4f}\")\n",
    "\n",
    "# Save model and artifacts\n",
    "print(f\"\\n=== Saving Model Artifacts ===\")\n",
    "model.save('models/resume_classifier_model.h5')\n",
    "\n",
    "# Save label encoder\n",
    "with open('models/label_encoder.pkl', 'wb') as f:\n",
    "    pickle.dump(label_encoder, f)\n",
    "\n",
    "# Save evaluation metrics\n",
    "evaluation_metrics = {\n",
    "    'test_loss': float(test_loss),\n",
    "    'test_accuracy': float(test_accuracy),\n",
    "    'f1_macro': float(f1_macro),\n",
    "    'f1_weighted': float(f1_weighted),\n",
    "    'confusion_matrix': cm.tolist(),\n",
    "    'classification_report': classification_report(y_true, y_pred, target_names=target_names, output_dict=True)\n",
    "}\n",
    "\n",
    "with open('data/evaluation_metrics.json', 'w') as f:\n",
    "    json.dump(evaluation_metrics, f, indent=2)\n",
    "\n",
    "print(\"Saved model artifacts:\")\n",
    "print(\"- models/resume_classifier_model.h5: Trained model\")\n",
    "print(\"- models/best_resume_classifier.h5: Best model checkpoint\")\n",
    "print(\"- models/label_encoder.pkl: Label encoder\")\n",
    "print(\"- data/training_history.json: Training history\")\n",
    "print(\"- data/evaluation_metrics.json: Model evaluation results\")\n",
    "\n",
    "print(\"\\n=== Step 7: Enhanced Metrics & Evaluation ===\")\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support, roc_curve, auc\n",
    "from sklearn.calibration import CalibratedClassifierCV, calibration_curve\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class ModelEvaluator:\n",
    "    \"\"\"\n",
    "    Comprehensive model evaluation with calibration and threshold analysis\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, label_encoder):\n",
    "        self.model = model\n",
    "        self.label_encoder = label_encoder\n",
    "        self.class_names = label_encoder.classes_\n",
    "        \n",
    "    def evaluate_comprehensive(self, test_inputs, y_true, y_pred_probs):\n",
    "        \"\"\"\n",
    "        Comprehensive evaluation with all metrics\n",
    "        \"\"\"\n",
    "        y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "        y_true_labels = np.argmax(y_true, axis=1)\n",
    "        \n",
    "        # 7.1 Primary metrics\n",
    "        precision, recall, f1, support = precision_recall_fscore_support(\n",
    "            y_true_labels, y_pred, average=None, labels=range(len(self.class_names))\n",
    "        )\n",
    "        \n",
    "        # Per-class metrics\n",
    "        per_class_metrics = {}\n",
    "        for i, class_name in enumerate(self.class_names):\n",
    "            per_class_metrics[class_name] = {\n",
    "                'precision': float(precision[i]),\n",
    "                'recall': float(recall[i]),\n",
    "                'f1': float(f1[i]),\n",
    "                'support': int(support[i])\n",
    "            }\n",
    "        \n",
    "        # Macro and weighted averages\n",
    "        precision_macro = precision.mean()\n",
    "        recall_macro = recall.mean()\n",
    "        f1_macro = f1.mean()\n",
    "        \n",
    "        precision_weighted = np.average(precision, weights=support)\n",
    "        recall_weighted = np.average(recall, weights=support)\n",
    "        f1_weighted = np.average(f1, weights=support)\n",
    "        \n",
    "        print(\"=== Per-Class Metrics ===\")\n",
    "        for class_name, metrics in per_class_metrics.items():\n",
    "            print(f\"{class_name:12} - Precision: {metrics['precision']:.3f}, \"\n",
    "                  f\"Recall: {metrics['recall']:.3f}, F1: {metrics['f1']:.3f}, \"\n",
    "                  f\"Support: {metrics['support']}\")\n",
    "        \n",
    "        print(f\"\\n=== Macro Averages ===\")\n",
    "        print(f\"Precision: {precision_macro:.3f}\")\n",
    "        print(f\"Recall: {recall_macro:.3f}\")\n",
    "        print(f\"F1: {f1_macro:.3f}\")\n",
    "        \n",
    "        print(f\"\\n=== Weighted Averages ===\")\n",
    "        print(f\"Precision: {precision_weighted:.3f}\")\n",
    "        print(f\"Recall: {recall_weighted:.3f}\")\n",
    "        print(f\"F1: {f1_weighted:.3f}\")\n",
    "        \n",
    "        return per_class_metrics, {\n",
    "            'macro': {'precision': precision_macro, 'recall': recall_macro, 'f1': f1_macro},\n",
    "            'weighted': {'precision': precision_weighted, 'recall': recall_weighted, 'f1': f1_weighted}\n",
    "        }\n",
    "    \n",
    "    def analyze_calibration(self, test_inputs, y_true, y_pred_probs):\n",
    "        \"\"\"\n",
    "        7.4 Calibration analysis with probability histograms\n",
    "        \"\"\"\n",
    "        print(\"\\n=== Calibration Analysis ===\")\n",
    "        \n",
    "        # Plot predicted probability histograms\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        \n",
    "        # Overall probability distribution\n",
    "        axes[0, 0].hist(y_pred_probs.max(axis=1), bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "        axes[0, 0].set_xlabel('Max Predicted Probability')\n",
    "        axes[0, 0].set_ylabel('Frequency')\n",
    "        axes[0, 0].set_title('Distribution of Max Predicted Probabilities')\n",
    "        axes[0, 0].axvline(x=0.55, color='red', linestyle='--', label='Uncertainty Threshold')\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Per-class probability distributions\n",
    "        for i, class_name in enumerate(self.class_names):\n",
    "            class_probs = y_pred_probs[:, i]\n",
    "            axes[0, 1].hist(class_probs, bins=20, alpha=0.6, label=f'{class_name}', density=True)\n",
    "        \n",
    "        axes[0, 1].set_xlabel('Predicted Probability')\n",
    "        axes[0, 1].set_ylabel('Density')\n",
    "        axes[0, 1].set_title('Probability Distribution by Class')\n",
    "        axes[0, 1].legend()\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Calibration plot for each class\n",
    "        y_true_labels = np.argmax(y_true, axis=1)\n",
    "        \n",
    "        for i, class_name in enumerate(self.class_names):\n",
    "            # Create binary classification for this class\n",
    "            y_binary = (y_true_labels == i).astype(int)\n",
    "            y_prob = y_pred_probs[:, i]\n",
    "            \n",
    "            # Calibration curve\n",
    "            fraction_of_positives, mean_predicted_value = calibration_curve(\n",
    "                y_binary, y_prob, n_bins=10, strategy='uniform'\n",
    "            )\n",
    "            \n",
    "            axes[1, 0].plot(mean_predicted_value, fraction_of_positives, 'o-', \n",
    "                           label=f'{class_name}', linewidth=2)\n",
    "        \n",
    "        axes[1, 0].plot([0, 1], [0, 1], 'k--', label='Perfect Calibration')\n",
    "        axes[1, 0].set_xlabel('Mean Predicted Probability')\n",
    "        axes[1, 0].set_ylabel('Fraction of Positives')\n",
    "        axes[1, 0].set_title('Calibration Plot')\n",
    "        axes[1, 0].legend()\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Confidence vs Accuracy\n",
    "        max_probs = y_pred_probs.max(axis=1)\n",
    "        predictions = np.argmax(y_pred_probs, axis=1)\n",
    "        correct = (predictions == y_true_labels).astype(int)\n",
    "        \n",
    "        # Bin by confidence\n",
    "        confidence_bins = np.linspace(0, 1, 11)\n",
    "        bin_accuracies = []\n",
    "        bin_confidences = []\n",
    "        \n",
    "        for i in range(len(confidence_bins) - 1):\n",
    "            bin_mask = (max_probs >= confidence_bins[i]) & (max_probs < confidence_bins[i+1])\n",
    "            if bin_mask.sum() > 0:\n",
    "                bin_accuracy = correct[bin_mask].mean()\n",
    "                bin_confidence = max_probs[bin_mask].mean()\n",
    "                bin_accuracies.append(bin_accuracy)\n",
    "                bin_confidences.append(bin_confidence)\n",
    "        \n",
    "        axes[1, 1].plot(bin_confidences, bin_accuracies, 'o-', linewidth=2, label='Model')\n",
    "        axes[1, 1].plot([0, 1], [0, 1], 'k--', label='Perfect Calibration')\n",
    "        axes[1, 1].set_xlabel('Confidence')\n",
    "        axes[1, 1].set_ylabel('Accuracy')\n",
    "        axes[1, 1].set_title('Confidence vs Accuracy')\n",
    "        axes[1, 1].legend()\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('data/calibration_analysis.png', dpi=150, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        return max_probs\n",
    "    \n",
    "    def threshold_analysis(self, y_pred_probs, uncertainty_threshold=0.55):\n",
    "        \"\"\"\n",
    "        7.5 Threshold analysis for uncertainty detection\n",
    "        \"\"\"\n",
    "        print(f\"\\n=== Threshold Analysis (Uncertainty < {uncertainty_threshold}) ===\")\n",
    "        \n",
    "        max_probs = y_pred_probs.max(axis=1)\n",
    "        predictions = np.argmax(y_pred_probs, axis=1)\n",
    "        \n",
    "        # Count uncertain predictions\n",
    "        uncertain_mask = max_probs < uncertainty_threshold\n",
    "        uncertain_count = uncertain_mask.sum()\n",
    "        \n",
    "        print(f\"Uncertain predictions: {uncertain_count} ({uncertain_count/len(y_pred_probs)*100:.1f}%)\")\n",
    "        print(f\"Confident predictions: {len(y_pred_probs) - uncertain_count} ({(1-uncertain_count/len(y_pred_probs))*100:.1f}%)\")\n",
    "        \n",
    "        # Analyze uncertain predictions by class\n",
    "        if uncertain_count > 0:\n",
    "            uncertain_predictions = predictions[uncertain_mask]\n",
    "            uncertain_probs = max_probs[uncertain_mask]\n",
    "            \n",
    "            print(f\"\\nUncertain predictions by class:\")\n",
    "            for i, class_name in enumerate(self.class_names):\n",
    "                class_uncertain = (uncertain_predictions == i).sum()\n",
    "                print(f\"  {class_name}: {class_uncertain} ({class_uncertain/uncertain_count*100:.1f}% of uncertain)\")\n",
    "            \n",
    "            print(f\"\\nConfidence statistics for uncertain predictions:\")\n",
    "            print(f\"  Mean confidence: {uncertain_probs.mean():.3f}\")\n",
    "            print(f\"  Min confidence: {uncertain_probs.min():.3f}\")\n",
    "            print(f\"  Max confidence: {uncertain_probs.max():.3f}\")\n",
    "        \n",
    "        return uncertain_mask, max_probs\n",
    "\n",
    "# Run comprehensive evaluation\n",
    "print(\"Running comprehensive evaluation...\")\n",
    "evaluator = ModelEvaluator(model, label_encoder)\n",
    "\n",
    "# Get predictions on test set\n",
    "y_pred_probs = model.predict(test_inputs, verbose=0)\n",
    "\n",
    "# 7.1-7.3 Comprehensive metrics\n",
    "per_class_metrics, avg_metrics = evaluator.evaluate_comprehensive(test_inputs, y_test, y_pred_probs)\n",
    "\n",
    "# 7.4 Calibration analysis\n",
    "max_confidences = evaluator.analyze_calibration(test_inputs, y_test, y_pred_probs)\n",
    "\n",
    "# 7.5 Threshold analysis\n",
    "uncertain_mask, confidences = evaluator.threshold_analysis(y_pred_probs, uncertainty_threshold=0.55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb28922f-e49a-462a-b5a0-15a338c80729",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffed31d-8d6f-4284-a7e8-f3a2484067e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8 - Save Model & Artifacts\n",
    "print(f\"\\n=== Step 8: Save Model & Artifacts ===\")\n",
    "\n",
    "# Create models directory if it doesn't exist\n",
    "Path('models').mkdir(exist_ok=True)\n",
    "Path('artifacts').mkdir(exist_ok=True)\n",
    "\n",
    "# Save model weights and architecture\n",
    "print(\"Saving model architecture and weights...\")\n",
    "model.save('models/resume_classifier_complete.h5')\n",
    "print(\"✓ Saved: models/resume_classifier_complete.h5\")\n",
    "\n",
    "# Save TensorFlow SavedModel format (for production deployment)\n",
    "# For TensorFlow SavedModel format in Keras 3\n",
    "model.export('models/resume_classifier_savedmodel')\n",
    "print(\"✓ Saved: models/resume_classifier_savedmodel/ (TensorFlow SavedModel)\")\n",
    "\n",
    "# Save all preprocessing artifacts\n",
    "print(\"Saving preprocessing artifacts...\")\n",
    "\n",
    "# Save scalers\n",
    "joblib.dump(scaler, 'artifacts/feature_scaler.pkl')\n",
    "print(\"✓ Saved: artifacts/feature_scaler.pkl\")\n",
    "\n",
    "# Save skill vocabulary\n",
    "with open('artifacts/skill_vocabulary.json', 'w') as f:\n",
    "    json.dump(skill_vocab, f, indent=2)\n",
    "print(\"✓ Saved: artifacts/skill_vocabulary.json\")\n",
    "\n",
    "# Save label encoder\n",
    "joblib.dump(label_encoder, 'artifacts/label_encoder.pkl')\n",
    "print(\"✓ Saved: artifacts/label_encoder.pkl\")\n",
    "\n",
    "# Save feature vector builder\n",
    "joblib.dump(vector_builder, 'artifacts/feature_vector_builder.pkl')\n",
    "print(\"✓ Saved: artifacts/feature_vector_builder.pkl\")\n",
    "\n",
    "# Save domain requirements\n",
    "with open('artifacts/domain_requirements.json', 'w') as f:\n",
    "    json.dump(domain_requirements, f, indent=2)\n",
    "print(\"✓ Saved: artifacts/domain_requirements.json\")\n",
    "\n",
    "# Save complete classification pipeline\n",
    "joblib.dump(classification_pipeline, 'artifacts/classification_pipeline.pkl')\n",
    "print(\"✓ Saved: artifacts/classification_pipeline.pkl\")\n",
    "\n",
    "# Save explanation templates and configuration\n",
    "explanation_config = {\n",
    "    \"score_thresholds\": {\n",
    "        \"excellent\": 85,\n",
    "        \"high\": 75, \n",
    "        \"good\": 60,\n",
    "        \"fair\": 50\n",
    "    },\n",
    "    \"skill_ratio_thresholds\": {\n",
    "        \"most\": 0.8,\n",
    "        \"many\": 0.6,\n",
    "        \"some\": 0.4\n",
    "    },\n",
    "    \"experience_thresholds\": {\n",
    "        \"solid\": 3,\n",
    "        \"some\": 1\n",
    "    },\n",
    "    \"confidence_precision\": 3,\n",
    "    \"explanation_template\": \"template_based_explanation\"\n",
    "}\n",
    "\n",
    "with open('artifacts/explanation_config.json', 'w') as f:\n",
    "    json.dump(explanation_config, f, indent=2)\n",
    "print(\"✓ Saved: artifacts/explanation_config.json\")\n",
    "\n",
    "# Create model manifest/metadata\n",
    "model_manifest = {\n",
    "    \"model_name\": \"resume_classifier\",\n",
    "    \"version\": \"1.0\",\n",
    "    \"created_date\": datetime.now().isoformat(),\n",
    "    \"model_architecture\": \"hybrid_neural_network\",\n",
    "    \"input_features\": {\n",
    "        \"skill_vocabulary_size\": len(skill_vocab),\n",
    "        \"numeric_features\": 4,\n",
    "        \"text_features\": 128 if use_text_features else 0,\n",
    "        \"total_features\": dimensions['final_vector_dim']\n",
    "    },\n",
    "    \"output_classes\": label_encoder.classes_.tolist(),\n",
    "    \"training_samples\": len(X_train),\n",
    "    \"validation_samples\": len(X_val),\n",
    "    \"test_samples\": len(X_test),\n",
    "    \"test_accuracy\": float(test_accuracy),\n",
    "    \"artifacts\": {\n",
    "        \"model_weights\": \"models/resume_classifier_complete.h5\",\n",
    "        \"savedmodel\": \"models/resume_classifier_savedmodel/\",\n",
    "        \"feature_scaler\": \"artifacts/feature_scaler.pkl\",\n",
    "        \"skill_vocabulary\": \"artifacts/skill_vocabulary.json\",\n",
    "        \"label_encoder\": \"artifacts/label_encoder.pkl\",\n",
    "        \"feature_builder\": \"artifacts/feature_vector_builder.pkl\",\n",
    "        \"domain_requirements\": \"artifacts/domain_requirements.json\",\n",
    "        \"pipeline\": \"artifacts/classification_pipeline.pkl\",\n",
    "        \"explanation_config\": \"artifacts/explanation_config.json\"\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('artifacts/model_manifest.json', 'w') as f:\n",
    "    json.dump(model_manifest, f, indent=2)\n",
    "print(\"✓ Saved: artifacts/model_manifest.json\")\n",
    "\n",
    "# Test loading pipeline from artifacts\n",
    "print(f\"\\n=== Testing Artifact Loading ===\")\n",
    "try:\n",
    "    # Test loading the complete pipeline\n",
    "    loaded_pipeline = joblib.load('artifacts/classification_pipeline.pkl')\n",
    "    \n",
    "    # Test classification with loaded pipeline\n",
    "    test_resume = sample_resumes[0]\n",
    "    loaded_result = loaded_pipeline.classify_resume(test_resume)\n",
    "    \n",
    "    if 'error' not in loaded_result:\n",
    "        print(\"✓ Successfully loaded and tested complete pipeline\")\n",
    "        print(f\"  Test prediction: {loaded_result['label']} ({loaded_result['confidence']})\")\n",
    "    else:\n",
    "        print(f\"✗ Pipeline test failed: {loaded_result['error']}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"✗ Failed to load pipeline: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d59976-9551-4fef-b787-7d0aba42b0c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a675d3-bcae-4f43-919c-9c880b4ccfa7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f528637-432f-497e-84af-1c5845a1b6cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c0e463-6a5b-4989-ad06-064e589eafaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332ed6f7-a29e-4308-8ade-97748872dcc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 9 - Interpretability & Explanation\n",
    "# print(\"\\n=== Step 9: Interpretability & Explanation ===\")\n",
    "\n",
    "# class ResumeExplainer:\n",
    "#     \"\"\"\n",
    "#     Provides human-readable explanations for resume classification decisions\n",
    "#     \"\"\"\n",
    "    \n",
    "#     def __init__(self, model, feature_builder, label_encoder, skill_vocab, domain_requirements):\n",
    "#         self.model = model\n",
    "#         self.feature_builder = feature_builder\n",
    "#         self.label_encoder = label_encoder\n",
    "#         self.skill_vocab = skill_vocab\n",
    "#         self.domain_requirements = domain_requirements\n",
    "        \n",
    "#     def explain_prediction(self, resume_features, prediction_probs, show_shap=False):\n",
    "#         \"\"\"\n",
    "#         9.1-9.2 Generate human-readable explanation with matched/missing skills\n",
    "#         UPDATED: Now includes alternative domain suggestions for Partial Fit/Not Fit\n",
    "#         \"\"\"\n",
    "#         # Get prediction\n",
    "#         pred_idx = np.argmax(prediction_probs)\n",
    "#         predicted_label = self.label_encoder.classes_[pred_idx]\n",
    "#         confidence = prediction_probs[pred_idx]\n",
    "        \n",
    "#         # 9.1 Matched & missing skills (already computed in features)\n",
    "#         matched_skills = resume_features.get('matched_skills', [])\n",
    "#         missing_skills = resume_features.get('missing_skills', [])\n",
    "        \n",
    "#         # 9.2 Rule-based template explanation\n",
    "#         test_score = resume_features.get('test_score', 0)\n",
    "#         test_score_norm = resume_features.get('test_score_norm', 0)\n",
    "#         skill_match_ratio = resume_features.get('skill_match_ratio', 0)\n",
    "#         project_count = resume_features.get('project_count', 0)\n",
    "#         years_experience = resume_features.get('years_experience', 0)\n",
    "#         domain = resume_features.get('domain', 'Unknown')\n",
    "        \n",
    "#         # Build explanation components\n",
    "#         score_desc = \"High\" if test_score >= 75 else \"Medium\" if test_score >= 50 else \"Low\"\n",
    "#         skills_desc = f\"covers {len(matched_skills)}/{len(matched_skills) + len(missing_skills)} required skills\"\n",
    "        \n",
    "#         # Top missing skills (first 3)\n",
    "#         top_missing = missing_skills[:3] if missing_skills else []\n",
    "#         missing_desc = f\", but lacks {', '.join(top_missing)}\" if top_missing else \"\"\n",
    "        \n",
    "#         # Experience description\n",
    "#         exp_desc = f\"{years_experience:.0f} year{'s' if years_experience != 1 else ''}\"\n",
    "        \n",
    "#         # Build main explanation\n",
    "#         explanation = (f\"{score_desc} test score ({test_score:.0f}/100) and {skills_desc}\"\n",
    "#                       f\"{missing_desc}. Projects: {project_count}; Experience: {exp_desc}. \"\n",
    "#                       f\"Model confidence: {confidence:.2f} → {predicted_label}.\")\n",
    "        \n",
    "#         # NEW: Add alternative domain suggestions for Partial Fit and Not Fit\n",
    "#         alternative_suggestions = None\n",
    "#         if predicted_label in [\"Partial Fit\", \"Not Fit\"]:\n",
    "#             alternative_suggestions = resume_features.get('alternative_domains', [])\n",
    "            \n",
    "#             # Add domain suggestion to explanation if available\n",
    "#             if alternative_suggestions and len(alternative_suggestions) > 0:\n",
    "#                 top_domain = alternative_suggestions[0]\n",
    "#                 match_pct = int(top_domain['skill_match_ratio'] * 100)\n",
    "#                 explanation += (f\" Consider {top_domain['domain']} roles \"\n",
    "#                               f\"({match_pct}% skill match with {top_domain['matched_count']}\"\n",
    "#                               f\"/{top_domain['required_count']} required skills).\")\n",
    "        \n",
    "#         return {\n",
    "#             'predicted_label': predicted_label,\n",
    "#             'confidence': float(confidence),\n",
    "#             'matched_skills': matched_skills,\n",
    "#             'missing_skills': missing_skills,\n",
    "#             'feature_summary': {\n",
    "#                 'skill_match_ratio': float(skill_match_ratio),\n",
    "#                 'years_experience': float(years_experience),\n",
    "#                 'test_score': float(test_score),\n",
    "#                 'test_score_norm': float(test_score_norm),\n",
    "#                 'project_count': int(project_count),\n",
    "#                 'domain': domain\n",
    "#             },\n",
    "#             'explanation': explanation,\n",
    "#             'alternative_domains': alternative_suggestions  # NEW FIELD\n",
    "#         }\n",
    "    \n",
    "#     def sensitivity_test(self, resume_features, perturbation_percent=0.1):\n",
    "#         \"\"\"\n",
    "#         9.4 Sensitivity test - perturb test_score by ±10% and observe changes\n",
    "#         \"\"\"\n",
    "#         original_score = resume_features.get('test_score', 0)\n",
    "        \n",
    "#         # Test perturbations\n",
    "#         perturbations = [\n",
    "#             ('original', original_score),\n",
    "#             ('10% higher', original_score * (1 + perturbation_percent)),\n",
    "#             ('10% lower', original_score * (1 - perturbation_percent))\n",
    "#         ]\n",
    "        \n",
    "#         results = []\n",
    "        \n",
    "#         for desc, perturbed_score in perturbations:\n",
    "#             # Create perturbed features\n",
    "#             perturbed_features = resume_features.copy()\n",
    "#             perturbed_features['test_score'] = min(100, max(0, perturbed_score))  # Clamp to [0,100]\n",
    "#             perturbed_features['test_score_norm'] = perturbed_features['test_score'] / 100.0\n",
    "            \n",
    "#             # Update numeric features for the model\n",
    "#             perturbed_features['numeric_features'] = [\n",
    "#                 perturbed_features['years_experience'],\n",
    "#                 perturbed_features.get('max_years', 0),\n",
    "#                 perturbed_features['project_count']\n",
    "#             ]\n",
    "            \n",
    "#             # Rebuild feature vector (simplified for sensitivity test)\n",
    "#             try:\n",
    "#                 # This is a simplified version - in practice, you'd rebuild the full vector\n",
    "#                 feature_vector = self.feature_builder.build_final_vector(perturbed_features)\n",
    "#                 model_inputs = model_classifier.prepare_inputs(feature_vector.reshape(1, -1), self.feature_builder)\n",
    "#                 pred_probs = self.model.predict(model_inputs, verbose=0)[0]\n",
    "#                 pred_label = self.label_encoder.classes_[np.argmax(pred_probs)]\n",
    "#                 confidence = pred_probs.max()\n",
    "                \n",
    "#                 results.append({\n",
    "#                     'description': desc,\n",
    "#                     'test_score': perturbed_features['test_score'],\n",
    "#                     'predicted_label': pred_label,\n",
    "#                     'confidence': float(confidence)\n",
    "#                 })\n",
    "#             except Exception as e:\n",
    "#                 results.append({\n",
    "#                     'description': desc,\n",
    "#                     'test_score': perturbed_features['test_score'],\n",
    "#                     'error': str(e)\n",
    "#                 })\n",
    "        \n",
    "#         # Check for sensitivity\n",
    "#         original_label = results[0]['predicted_label']\n",
    "#         is_sensitive = any(r.get('predicted_label') != original_label for r in results[1:])\n",
    "        \n",
    "#         return results, is_sensitive\n",
    "    \n",
    "#     # NEW METHOD: Format alternative domains for display\n",
    "#     def format_alternative_domains(self, alternative_domains, top_n=3):\n",
    "#         \"\"\"\n",
    "#         Format alternative domain suggestions in a readable way\n",
    "        \n",
    "#         Args:\n",
    "#             alternative_domains: List of alternative domain suggestions\n",
    "#             top_n: Number of top suggestions to format\n",
    "        \n",
    "#         Returns:\n",
    "#             Formatted string describing alternative domains\n",
    "#         \"\"\"\n",
    "#         if not alternative_domains or len(alternative_domains) == 0:\n",
    "#             return \"No strong alternative domain matches found.\"\n",
    "        \n",
    "#         formatted_lines = []\n",
    "#         for i, domain_info in enumerate(alternative_domains[:top_n], 1):\n",
    "#             domain_name = domain_info['domain']\n",
    "#             match_ratio = domain_info['skill_match_ratio']\n",
    "#             matched_count = domain_info['matched_count']\n",
    "#             required_count = domain_info['required_count']\n",
    "#             matched_skills = domain_info['matched_skills'][:3]  # Top 3 matched\n",
    "            \n",
    "#             line = (f\"{i}. {domain_name}: {match_ratio:.1%} match \"\n",
    "#                    f\"({matched_count}/{required_count} skills) - \"\n",
    "#                    f\"Has: {', '.join(matched_skills) if matched_skills else 'none'}\")\n",
    "#             formatted_lines.append(line)\n",
    "        \n",
    "#         return \"\\n\".join(formatted_lines)\n",
    "\n",
    "# # Initialize explainer\n",
    "# explainer = ResumeExplainer(\n",
    "#     model=model,\n",
    "#     feature_builder=vector_builder, \n",
    "#     label_encoder=label_encoder,\n",
    "#     skill_vocab=skill_vocab,\n",
    "#     domain_requirements=domain_requirements\n",
    "# )\n",
    "\n",
    "# #  Test explanations on sample predictions\n",
    "# print(\"\\n=== Sample Explanations ===\")\n",
    "# sample_indices = [0, 10, 20]  # Test first few samples\n",
    "\n",
    "# for i, idx in enumerate(sample_indices):\n",
    "#     if idx < len(test_inputs[0]):\n",
    "#         # Get original features for this sample\n",
    "#         original_idx = len(X_train) + len(X_val) + idx  # Adjust for train/val offset\n",
    "#         if original_idx < len(all_features):\n",
    "#             sample_features = all_features[original_idx]\n",
    "#             sample_probs = y_pred_probs[idx]\n",
    "            \n",
    "#             print(f\"\\n--- Sample {i+1} (ID: {sample_features['id']}) ---\")\n",
    "            \n",
    "#             # Generate explanation\n",
    "#             explanation = explainer.explain_prediction(sample_features, sample_probs)\n",
    "            \n",
    "#             print(f\"Prediction: {explanation['predicted_label']} (confidence: {explanation['confidence']:.3f})\")\n",
    "#             print(f\"Domain: {explanation['feature_summary']['domain']}\")\n",
    "#             print(f\"Matched skills ({len(explanation['matched_skills'])}): {', '.join(explanation['matched_skills'][:5])}...\")\n",
    "#             print(f\"Missing skills ({len(explanation['missing_skills'])}): {', '.join(explanation['missing_skills'][:3])}...\")\n",
    "#             print(f\"Explanation: {explanation['explanation']}\")\n",
    "            \n",
    "#             # NEW: Show alternative domain suggestions for Partial Fit/Not Fit\n",
    "#             if explanation['alternative_domains']:\n",
    "#                 print(f\"\\n Alternative Domain Suggestions:\")\n",
    "#                 formatted_alternatives = explainer.format_alternative_domains(\n",
    "#                     explanation['alternative_domains'], top_n=3\n",
    "#                 )\n",
    "#                 print(f\"  {formatted_alternatives.replace(chr(10), chr(10) + '  ')}\")\n",
    "            \n",
    "#             # Sensitivity test\n",
    "#             sensitivity_results, is_sensitive = explainer.sensitivity_test(sample_features)\n",
    "#             print(f\"\\nSensitivity test:\")\n",
    "#             for result in sensitivity_results:\n",
    "#                 if 'error' not in result:\n",
    "#                     print(f\"  {result['description']}: {result['predicted_label']} ({result['confidence']:.3f})\")\n",
    "            \n",
    "#             if is_sensitive:\n",
    "#                 print(\"  ⚠️  BORDERLINE: Small test score changes affect prediction\")\n",
    "\n",
    "# # Save comprehensive evaluation results\n",
    "# print(f\"\\n=== Saving Enhanced Evaluation Results ===\")\n",
    "\n",
    "# enhanced_evaluation = {\n",
    "#     'per_class_metrics': per_class_metrics,\n",
    "#     'average_metrics': avg_metrics,\n",
    "#     'uncertainty_analysis': {\n",
    "#         'threshold': 0.55,\n",
    "#         'uncertain_count': int(uncertain_mask.sum()),\n",
    "#         'uncertain_percentage': float(uncertain_mask.sum() / len(uncertain_mask) * 100),\n",
    "#         'mean_confidence': float(confidences.mean()),\n",
    "#         'std_confidence': float(confidences.std())\n",
    "#     },\n",
    "#     'calibration_stats': {\n",
    "#         'mean_max_probability': float(max_confidences.mean()),\n",
    "#         'std_max_probability': float(max_confidences.std())\n",
    "#     }\n",
    "# }\n",
    "\n",
    "# with open('data/enhanced_evaluation.json', 'w') as f:\n",
    "#     json.dump(enhanced_evaluation, f, indent=2)\n",
    "\n",
    "# print(\"Saved enhanced evaluation artifacts:\")\n",
    "# print(\"- data/enhanced_evaluation.json: Comprehensive metrics\")\n",
    "# print(\"- data/calibration_analysis.png: Calibration plots\")\n",
    "\n",
    "# # NEW: Save statistics about alternative domain suggestions\n",
    "# print(\"\\n=== Alternative Domain Suggestion Statistics ===\")\n",
    "# alt_domain_stats = {\n",
    "#     'by_label': {}\n",
    "# }\n",
    "\n",
    "# for label in ['Fit', 'Partial Fit', 'Not Fit']:\n",
    "#     label_features = [f for f in all_features if f.get('label') == label]\n",
    "#     if label_features:\n",
    "#         # Calculate average match ratio for top alternative\n",
    "#         avg_top_match = np.mean([f['alternative_domains'][0]['skill_match_ratio'] \n",
    "#                                   for f in label_features if f.get('alternative_domains')])\n",
    "        \n",
    "#         # Count how many have good alternatives (>50% match)\n",
    "#         good_alternatives = sum(1 for f in label_features \n",
    "#                                if f.get('alternative_domains') and \n",
    "#                                f['alternative_domains'][0]['skill_match_ratio'] > 0.5)\n",
    "        \n",
    "#         alt_domain_stats['by_label'][label] = {\n",
    "#             'count': len(label_features),\n",
    "#             'avg_top_alternative_match': float(avg_top_match),\n",
    "#             'candidates_with_good_alternatives': good_alternatives,\n",
    "#             'percentage_with_good_alternatives': float(good_alternatives / len(label_features) * 100)\n",
    "#         }\n",
    "        \n",
    "#         print(f\"{label}:\")\n",
    "#         print(f\"  Average top alternative match: {avg_top_match:.3f}\")\n",
    "#         print(f\"  Candidates with >50% alternative match: {good_alternatives} \"\n",
    "#               f\"({good_alternatives/len(label_features)*100:.1f}%)\")\n",
    "\n",
    "# # Save alternative domain statistics\n",
    "# with open('data/alternative_domain_stats.json', 'w') as f:\n",
    "#     json.dump(alt_domain_stats, f, indent=2)\n",
    "\n",
    "# print(\"\\nSaved alternative domain statistics to: data/alternative_domain_stats.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116835ec-ec09-42ca-8192-ad7ac2e62d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 10 - Postprocessing: Building Final JSON Output\n",
    "print(\"\\n=== Step 10: Final JSON Output Generation ===\")\n",
    "\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "\n",
    "class ResumeClassificationPipeline:\n",
    "    \"\"\"\n",
    "    Complete pipeline for resume classification with JSON output\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, feature_builder, label_encoder, skill_vocab, \n",
    "                 domain_requirements, scaler):\n",
    "        self.model = model\n",
    "        self.feature_builder = feature_builder\n",
    "        self.label_encoder = label_encoder\n",
    "        self.skill_vocab = skill_vocab\n",
    "        self.domain_requirements = domain_requirements\n",
    "        self.scaler = scaler\n",
    "        \n",
    "    def classify_resume(self, resume_json, include_raw_scores=True, precision=3):\n",
    "        \"\"\"\n",
    "        Complete pipeline: raw resume JSON → final classification JSON\n",
    "        UPDATED: Now includes alternative domain suggestions for Partial Fit/Not Fit\n",
    "        \n",
    "        Example pipeline from Step 9:\n",
    "        1. Run class_probs = model.predict(final_vector)\n",
    "        2. pred_idx = argmax(class_probs); label = classes[pred_idx]\n",
    "        3. confidence = float(class_probs[pred_idx])\n",
    "        4. matched_skills, missing_skills from Step 3\n",
    "        5. feature_summary = {...}\n",
    "        6. explanation = construct from template\n",
    "        7. alternative_domains (NEW) for Partial Fit/Not Fit\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Extract all features using Step 3 pipeline\n",
    "            resume_features = extract_all_features(resume_json, self.skill_vocab, self.domain_requirements)\n",
    "            \n",
    "            # Apply scaling to numeric features\n",
    "            scaled_numeric = self.scaler.transform([resume_features['numeric_features']])\n",
    "            resume_features['scaled_numeric_features'] = scaled_numeric[0]\n",
    "            \n",
    "            # 1. Build final feature vector and get model prediction\n",
    "            final_vector = self.feature_builder.build_final_vector(resume_features)\n",
    "            model_inputs = model_classifier.prepare_inputs(final_vector.reshape(1, -1), self.feature_builder)\n",
    "            class_probs = self.model.predict(model_inputs, verbose=0)[0]\n",
    "            \n",
    "            # 2. Get prediction and label\n",
    "            pred_idx = np.argmax(class_probs)\n",
    "            label = self.label_encoder.classes_[pred_idx]\n",
    "            \n",
    "            # 3. Get confidence (formatted to specified precision)\n",
    "            confidence = float(class_probs[pred_idx])\n",
    "            confidence = round(confidence, precision)\n",
    "            \n",
    "            # 4. Matched & missing skills (from Step 3)\n",
    "            matched_skills = resume_features['matched_skills']\n",
    "            missing_skills = resume_features['missing_skills']\n",
    "            \n",
    "            # 5. Feature summary with proper numeric formatting\n",
    "            skill_match_ratio = resume_features['skill_match_ratio']\n",
    "            years_experience = resume_features['years_experience'] \n",
    "            test_score_raw = resume_features['test_score']\n",
    "            test_score_norm = resume_features['test_score_norm']\n",
    "            project_count = resume_features['project_count']\n",
    "            \n",
    "            # Format skill_match_ratio: 8÷20 = 0.4 → format as 0.40 or 0.400\n",
    "            formatted_skill_ratio = round(skill_match_ratio, precision)\n",
    "            \n",
    "            feature_summary = {\n",
    "                \"skill_match_ratio\": formatted_skill_ratio,\n",
    "                \"years_experience\": int(years_experience),\n",
    "                \"test_score_norm\": round(test_score_norm, precision),\n",
    "                \"project_count\": int(project_count)\n",
    "            }\n",
    "            \n",
    "            # Include raw test score if requested\n",
    "            if include_raw_scores:\n",
    "                feature_summary[\"test_score_raw\"] = int(test_score_raw)\n",
    "            \n",
    "            # NEW: 7. Get alternative domain suggestions for Partial Fit/Not Fit\n",
    "            alternative_domains = None\n",
    "            if label in [\"Partial Fit\", \"Not Fit\"]:\n",
    "                alternative_domains = self._format_alternative_domains(\n",
    "                    resume_features.get('alternative_domains', []),\n",
    "                    precision\n",
    "                )\n",
    "            \n",
    "            # 6. Generate explanation using template (with alternative domains)\n",
    "            explanation = self._generate_explanation(\n",
    "                test_score_raw, skill_match_ratio, matched_skills, missing_skills,\n",
    "                project_count, years_experience, label, confidence,\n",
    "                alternative_domains  # NEW: Pass alternative domains\n",
    "            )\n",
    "            \n",
    "            # Build final JSON output\n",
    "            result = {\n",
    "                \"label\": label,\n",
    "                \"confidence\": confidence,\n",
    "                \"matched_skills\": matched_skills,\n",
    "                \"missing_skills\": missing_skills,\n",
    "                \"feature_summary\": feature_summary,\n",
    "                \"explanation\": explanation,\n",
    "                \"metadata\": {\n",
    "                    \"domain\": resume_features['domain'],\n",
    "                    \"candidate_id\": resume_features['id'],\n",
    "                    \"classification_timestamp\": datetime.now().isoformat(),\n",
    "                    \"model_version\": \"1.0\"\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            # NEW: Add alternative domain suggestions if applicable\n",
    "            if alternative_domains:\n",
    "                result[\"alternative_domain_suggestions\"] = alternative_domains\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"error\": f\"Classification failed: {str(e)}\",\n",
    "                \"candidate_id\": resume_json.get('id', 'unknown'),\n",
    "                \"timestamp\": datetime.now().isoformat()\n",
    "            }\n",
    "    \n",
    "    # NEW METHOD: Format alternative domains for JSON output\n",
    "    def _format_alternative_domains(self, suggestions, precision=3):\n",
    "        \"\"\"\n",
    "        Format alternative domain suggestions for JSON output\n",
    "        \n",
    "        Args:\n",
    "            suggestions: List of alternative domain suggestions from Step 3\n",
    "            precision: Number of decimal places for ratios\n",
    "        \n",
    "        Returns:\n",
    "            List of formatted domain suggestions\n",
    "        \"\"\"\n",
    "        if not suggestions:\n",
    "            return None\n",
    "        \n",
    "        formatted = []\n",
    "        \n",
    "        for i, suggestion in enumerate(suggestions, 1):\n",
    "            formatted.append({\n",
    "                \"rank\": i,\n",
    "                \"domain\": suggestion['domain'],\n",
    "                \"skill_match_ratio\": round(suggestion['skill_match_ratio'], precision),\n",
    "                \"matched_skills_count\": suggestion['matched_count'],\n",
    "                \"required_skills_count\": suggestion['required_count'],\n",
    "                \"matched_skills\": suggestion['matched_skills'][:5],  # Top 5\n",
    "                \"key_missing_skills\": suggestion['missing_skills'][:3]  # Top 3\n",
    "            })\n",
    "        \n",
    "        return formatted\n",
    "    \n",
    "    def _generate_explanation(self, test_score, skill_match_ratio, matched_skills, \n",
    "                            missing_skills, project_count, years_experience, \n",
    "                            predicted_label, confidence, alternative_domains=None):\n",
    "        \"\"\"\n",
    "        Generate human-readable explanation using template from Step 8\n",
    "        UPDATED: Now includes alternative domain suggestion in explanation\n",
    "        \"\"\"\n",
    "        # Score description\n",
    "        if test_score >= 85:\n",
    "            score_desc = \"Excellent\"\n",
    "        elif test_score >= 75:\n",
    "            score_desc = \"High\"\n",
    "        elif test_score >= 60:\n",
    "            score_desc = \"Good\" \n",
    "        elif test_score >= 50:\n",
    "            score_desc = \"Fair\"\n",
    "        else:\n",
    "            score_desc = \"Low\"\n",
    "        \n",
    "        # Skills description\n",
    "        total_required = len(matched_skills) + len(missing_skills)\n",
    "        skills_fraction = f\"({len(matched_skills)}/{total_required} matched)\"\n",
    "        \n",
    "        if skill_match_ratio >= 0.8:\n",
    "            skills_desc = f\"covers most required skills {skills_fraction}\"\n",
    "        elif skill_match_ratio >= 0.6:\n",
    "            skills_desc = f\"covers many required skills {skills_fraction}\"\n",
    "        elif skill_match_ratio >= 0.4:\n",
    "            skills_desc = f\"covers some required skills {skills_fraction}\"\n",
    "        else:\n",
    "            skills_desc = f\"covers few required skills {skills_fraction}\"\n",
    "        \n",
    "        # Missing skills (top 3)\n",
    "        top_missing = missing_skills[:3]\n",
    "        missing_desc = f\", but lacks {', '.join(top_missing)}\" if top_missing else \"\"\n",
    "        \n",
    "        # Experience description\n",
    "        if years_experience >= 3:\n",
    "            exp_desc = f\"{int(years_experience)} years of solid experience\"\n",
    "        elif years_experience >= 1:\n",
    "            exp_desc = f\"{int(years_experience)} year{'s' if years_experience != 1 else ''} of experience\"\n",
    "        else:\n",
    "            exp_desc = \"limited professional experience\"\n",
    "        \n",
    "        # Project description\n",
    "        if project_count >= 3:\n",
    "            proj_desc = f\"strong portfolio ({project_count} projects)\"\n",
    "        elif project_count >= 1:\n",
    "            proj_desc = f\"{project_count} project{'s' if project_count != 1 else ''}\"\n",
    "        else:\n",
    "            proj_desc = \"no projects listed\"\n",
    "        \n",
    "        # Recommendation based on missing skills\n",
    "        recommendation = \"\"\n",
    "        if predicted_label == \"Partial Fit\" and missing_skills:\n",
    "            key_missing = [skill for skill in missing_skills[:2]]  # Top 2 missing\n",
    "            if key_missing:\n",
    "                recommendation = f\" Recommend gaining experience in {', '.join(key_missing)}.\"\n",
    "        \n",
    "        # NEW: Add alternative domain suggestion for Partial Fit/Not Fit\n",
    "        domain_suggestion = \"\"\n",
    "        if predicted_label in [\"Partial Fit\", \"Not Fit\"] and alternative_domains:\n",
    "            top_domain = alternative_domains[0]\n",
    "            match_pct = int(top_domain['skill_match_ratio'] * 100)\n",
    "            domain_suggestion = (f\" Consider applying for {top_domain['domain']} roles \"\n",
    "                               f\"({match_pct}% skill match with {top_domain['matched_skills_count']}\"\n",
    "                               f\"/{top_domain['required_skills_count']} required skills).\")\n",
    "        \n",
    "        # Combine into explanation\n",
    "        explanation = (f\"{score_desc} test score ({int(test_score)}/100) and {skills_desc}\"\n",
    "                      f\"{missing_desc}. Has {proj_desc} and {exp_desc}. \"\n",
    "                      f\"Model confidence: {confidence:.2f} → {predicted_label}.\"\n",
    "                      f\"{recommendation}{domain_suggestion}\")\n",
    "        \n",
    "        return explanation\n",
    "    \n",
    "    def batch_classify(self, resume_list, output_file=None):\n",
    "        \"\"\"\n",
    "        Classify multiple resumes and optionally save to file\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for i, resume in enumerate(resume_list):\n",
    "            print(f\"Processing resume {i+1}/{len(resume_list)}: {resume.get('id', 'unknown')}\")\n",
    "            result = self.classify_resume(resume)\n",
    "            results.append(result)\n",
    "        \n",
    "        if output_file:\n",
    "            with open(output_file, 'w') as f:\n",
    "                json.dump(results, f, indent=2)\n",
    "            print(f\"Saved {len(results)} results to {output_file}\")\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Initialize complete classification pipeline\n",
    "print(\"Initializing complete classification pipeline...\")\n",
    "classification_pipeline = ResumeClassificationPipeline(\n",
    "    model=model,\n",
    "    feature_builder=vector_builder,\n",
    "    label_encoder=label_encoder,\n",
    "    skill_vocab=skill_vocab,\n",
    "    domain_requirements=domain_requirements,\n",
    "    scaler=scaler\n",
    ")\n",
    "\n",
    "# Test on sample resumes from our dataset\n",
    "print(\"\\n=== Testing JSON Output Generation ===\")\n",
    "sample_resumes = labeled_resumes[:5]  # Test first 5 resumes\n",
    "\n",
    "print(\"Generating JSON outputs for sample resumes...\")\n",
    "sample_results = []\n",
    "\n",
    "for i, resume in enumerate(sample_resumes):\n",
    "    print(f\"\\n--- Sample {i+1}: {resume['id']} ({resume['preferred_domain']}) ---\")\n",
    "    \n",
    "    # Classify resume and get JSON output\n",
    "    result = classification_pipeline.classify_resume(resume, include_raw_scores=True, precision=3)\n",
    "    \n",
    "    if 'error' not in result:\n",
    "        print(f\"Prediction: {result['label']} (confidence: {result['confidence']})\")\n",
    "        print(f\"Matched skills: {len(result['matched_skills'])}, Missing: {len(result['missing_skills'])}\")\n",
    "        print(f\"Feature summary: {result['feature_summary']}\")\n",
    "        print(f\"Explanation: {result['explanation']}\")\n",
    "        \n",
    "        # NEW: Display alternative domain suggestions if present\n",
    "        if 'alternative_domain_suggestions' in result:\n",
    "            print(f\"\\nAlternative Domain Suggestions:\")\n",
    "            for alt in result['alternative_domain_suggestions']:\n",
    "                print(f\"  {alt['rank']}. {alt['domain']}: {alt['skill_match_ratio']:.1%} match \"\n",
    "                      f\"({alt['matched_skills_count']}/{alt['required_skills_count']} skills)\")\n",
    "    else:\n",
    "        print(f\"Error: {result['error']}\")\n",
    "    \n",
    "    sample_results.append(result)\n",
    "\n",
    "# Save sample results\n",
    "with open('data/sample_json_outputs.json', 'w') as f:\n",
    "    json.dump(sample_results, f, indent=2)\n",
    "\n",
    "print(\"\\n=== Saved sample JSON outputs to: data/sample_json_outputs.json ===\")\n",
    "\n",
    "# Demonstrate exact arithmetic formatting from spec\n",
    "print(f\"\\n=== Numeric Formatting Examples (as per Step 9 spec) ===\")\n",
    "for result in sample_results[:2]:\n",
    "    if 'error' not in result:\n",
    "        skill_ratio = result['feature_summary']['skill_match_ratio']\n",
    "        matched_count = len(result['matched_skills'])\n",
    "        total_count = matched_count + len(result['missing_skills'])\n",
    "        \n",
    "        print(f\"\\nResume {result['metadata']['candidate_id']}:\")\n",
    "        print(f\"  Matched skills: {matched_count}, Required: {total_count}\")\n",
    "        print(f\"  Arithmetic: {matched_count} ÷ {total_count} = {matched_count/total_count:.3f}\")\n",
    "        print(f\"  Formatted in JSON: {skill_ratio}\")\n",
    "\n",
    "# NEW: Show example of complete JSON with alternative domains\n",
    "print(f\"\\n=== Example Complete JSON Output ===\")\n",
    "# Find a Partial Fit or Not Fit example\n",
    "example_result = None\n",
    "for result in sample_results:\n",
    "    if 'error' not in result and result['label'] in ['Partial Fit', 'Not Fit']:\n",
    "        example_result = result\n",
    "        break\n",
    "\n",
    "if example_result:\n",
    "    print(json.dumps(example_result, indent=2))\n",
    "else:\n",
    "    print(\"No Partial Fit/Not Fit examples in sample set\")\n",
    "\n",
    "# NEW: Statistics on alternative domain suggestions\n",
    "print(f\"\\n=== Alternative Domain Suggestion Statistics ===\")\n",
    "partial_not_fit_count = sum(1 for r in sample_results \n",
    "                            if 'error' not in r and r['label'] in ['Partial Fit', 'Not Fit'])\n",
    "with_alternatives = sum(1 for r in sample_results \n",
    "                       if 'error' not in r and 'alternative_domain_suggestions' in r)\n",
    "\n",
    "print(f\"Partial Fit/Not Fit candidates: {partial_not_fit_count}\")\n",
    "print(f\"Candidates with alternative suggestions: {with_alternatives}\")\n",
    "\n",
    "if with_alternatives > 0:\n",
    "    # Calculate average top alternative match\n",
    "    top_matches = [r['alternative_domain_suggestions'][0]['skill_match_ratio'] \n",
    "                   for r in sample_results \n",
    "                   if 'error' not in r and 'alternative_domain_suggestions' in r]\n",
    "    avg_top_match = np.mean(top_matches)\n",
    "    print(f\"Average top alternative domain match: {avg_top_match:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7336f247-175c-4c35-9ebf-b5b0969ae664",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9184f9-c764-424a-9c49-aacea57df78d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2598516-1257-4c06-922f-9ab6668768c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # Step 12 - Complete Tests, Monitoring & Iterative Improvements\n",
    "# print(\"\\n=== Step 12: Tests, Monitoring & Iterative Improvements ===\")\n",
    "\n",
    "# import unittest\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import json\n",
    "# import time\n",
    "# from datetime import datetime, timedelta\n",
    "# from unittest.mock import Mock, patch\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "# # First, let's define the core functions that the tests depend on\n",
    "# def encode_skills(candidate_skills, skill_vocab):\n",
    "#     \"\"\"\n",
    "#     Encode candidate skills as binary vector\n",
    "#     \"\"\"\n",
    "#     candidate_skills_lower = [skill.lower() for skill in candidate_skills]\n",
    "#     skill_vocab_lower = [skill.lower() for skill in skill_vocab]\n",
    "    \n",
    "#     encoded = np.zeros(len(skill_vocab), dtype=int)\n",
    "#     for i, skill in enumerate(skill_vocab_lower):\n",
    "#         if skill in candidate_skills_lower:\n",
    "#             encoded[i] = 1\n",
    "    \n",
    "#     return encoded\n",
    "\n",
    "# def compute_skill_matches(candidate_skills, required_skills):\n",
    "#     \"\"\"\n",
    "#     Compute matched and missing skills\n",
    "#     \"\"\"\n",
    "#     candidate_set = set(skill.lower() for skill in candidate_skills)\n",
    "#     required_set = set(skill.lower() for skill in required_skills)\n",
    "    \n",
    "#     matched = list(candidate_set.intersection(required_set))\n",
    "#     missing = list(required_set - candidate_set)\n",
    "#     ratio = len(matched) / len(required_skills) if required_skills else 0\n",
    "    \n",
    "#     return matched, missing, ratio\n",
    "\n",
    "# def normalize_test_score(score):\n",
    "#     \"\"\"\n",
    "#     Normalize test score to 0-1 range\n",
    "#     \"\"\"\n",
    "#     if score is None:\n",
    "#         return 0.0\n",
    "    \n",
    "#     # Handle non-numeric types\n",
    "#     try:\n",
    "#         score = float(score)\n",
    "#     except (TypeError, ValueError):\n",
    "#         return 0.0\n",
    "    \n",
    "#     # Clamp to valid range\n",
    "#     score = max(0, min(100, score))\n",
    "#     return score / 100.0\n",
    "\n",
    "# def extract_features(resume, domain_requirements):\n",
    "#     \"\"\"\n",
    "#     Extract features from resume for a specific domain\n",
    "#     \"\"\"\n",
    "#     preferred_domain = resume.get('preferred_domain', 'Data Science')\n",
    "    \n",
    "#     # Find domain requirements\n",
    "#     domain_key = None\n",
    "#     for key, domain_info in domain_requirements.items():\n",
    "#         if domain_info['domain'] == preferred_domain:\n",
    "#             domain_key = key\n",
    "#             break\n",
    "    \n",
    "#     if not domain_key:\n",
    "#         domain_key = list(domain_requirements.keys())[0]  # Fallback\n",
    "    \n",
    "#     required_skills = domain_requirements[domain_key]['required_skills']\n",
    "#     candidate_skills = resume.get('skills', [])\n",
    "    \n",
    "#     # Calculate features\n",
    "#     matched_skills, missing_skills, skill_match_ratio = compute_skill_matches(candidate_skills, required_skills)\n",
    "#     test_score_norm = normalize_test_score(resume.get('test_score', 0))\n",
    "#     project_count = len(resume.get('projects', []))\n",
    "#     total_experience = sum(exp.get('years', 0) for exp in resume.get('work_experience', []))\n",
    "    \n",
    "#     return {\n",
    "#         'skill_match_ratio': skill_match_ratio,\n",
    "#         'test_score_norm': test_score_norm,\n",
    "#         'project_count': project_count,\n",
    "#         'total_experience': total_experience,\n",
    "#         'matched_skills': matched_skills,\n",
    "#         'missing_skills': missing_skills\n",
    "#     }\n",
    "\n",
    "# class MockClassificationPipeline:\n",
    "#     \"\"\"\n",
    "#     Mock classification pipeline for testing\n",
    "#     \"\"\"\n",
    "#     def __init__(self):\n",
    "#         self.domain_requirements = {\n",
    "#             \"data_science\": {\n",
    "#                 \"domain\": \"Data Science\",\n",
    "#                 \"required_skills\": [\"Python\", \"SQL\", \"Machine Learning\", \"Statistics\"]\n",
    "#             },\n",
    "#             \"web_development\": {\n",
    "#                 \"domain\": \"Web Development\", \n",
    "#                 \"required_skills\": [\"JavaScript\", \"React\", \"Node.js\", \"HTML\", \"CSS\", \"MongoDB\", \"Express\"]\n",
    "#             }\n",
    "#         }\n",
    "    \n",
    "#     def classify_resume(self, resume):\n",
    "#         \"\"\"\n",
    "#         Mock classification that returns realistic results\n",
    "#         \"\"\"\n",
    "#         features = extract_features(resume, self.domain_requirements)\n",
    "        \n",
    "#         # Simple rule-based classification for testing\n",
    "#         if (features['skill_match_ratio'] >= 0.70 and \n",
    "#             features['test_score_norm'] >= 0.75 and \n",
    "#             features['project_count'] >= 1):\n",
    "#             label = \"Fit\"\n",
    "#             confidence = 0.85 + np.random.random() * 0.10\n",
    "#         elif ((0.40 <= features['skill_match_ratio'] < 0.70) or \n",
    "#               (0.50 <= features['test_score_norm'] < 0.75)):\n",
    "#             label = \"Partial Fit\"\n",
    "#             confidence = 0.60 + np.random.random() * 0.20\n",
    "#         else:\n",
    "#             label = \"Not Fit\"\n",
    "#             confidence = 0.70 + np.random.random() * 0.15\n",
    "        \n",
    "#         return {\n",
    "#             'label': label,\n",
    "#             'confidence': min(confidence, 1.0),\n",
    "#             'matched_skills': features['matched_skills'],\n",
    "#             'missing_skills': features['missing_skills'],\n",
    "#             'feature_summary': {\n",
    "#                 'skill_match_ratio': features['skill_match_ratio'],\n",
    "#                 'test_score_norm': features['test_score_norm'],\n",
    "#                 'project_count': features['project_count'],\n",
    "#                 'total_experience': features['total_experience']\n",
    "#             },\n",
    "#             'explanation': f\"Classified as {label} based on skill match ratio: {features['skill_match_ratio']:.2f}, test score: {features['test_score_norm']:.2f}\",\n",
    "#             'metadata': {\n",
    "#                 'timestamp': datetime.now().isoformat(),\n",
    "#                 'model_version': '1.0.0',\n",
    "#                 'processing_time_ms': np.random.randint(50, 200)\n",
    "#             }\n",
    "#         }\n",
    "\n",
    "# # Initialize mock pipeline\n",
    "# classification_pipeline = MockClassificationPipeline()\n",
    "\n",
    "# class TestResumeClassifier(unittest.TestCase):\n",
    "#     \"\"\"\n",
    "#     Unit tests for core functions\n",
    "#     \"\"\"\n",
    "    \n",
    "#     def setUp(self):\n",
    "#         self.sample_skills = [\"Python\", \"Machine Learning\", \"SQL\"]\n",
    "#         self.sample_domain_requirements = {\n",
    "#             \"data_science\": {\n",
    "#                 \"domain\": \"Data Science\",\n",
    "#                 \"required_skills\": [\"Python\", \"SQL\", \"Machine Learning\", \"Statistics\"]\n",
    "#             }\n",
    "#         }\n",
    "    \n",
    "#     def test_skill_encoding(self):\n",
    "#         \"\"\"\n",
    "#         Test skill encoding function\n",
    "#         \"\"\"\n",
    "#         skill_vocab = [\"python\", \"sql\", \"java\", \"javascript\"]\n",
    "#         candidate_skills = [\"Python\", \"SQL\"]\n",
    "        \n",
    "#         encoded = encode_skills(candidate_skills, skill_vocab)\n",
    "        \n",
    "#         # Should have 1s for python and sql, 0s for others\n",
    "#         expected = np.array([1, 1, 0, 0])\n",
    "#         np.testing.assert_array_equal(encoded, expected)\n",
    "#         print(\"✓ Skill encoding test passed\")\n",
    "    \n",
    "#     def test_skill_matching(self):\n",
    "#         \"\"\"\n",
    "#         Test matched/missing skills computation\n",
    "#         \"\"\"\n",
    "#         candidate_skills = [\"Python\", \"SQL\"]\n",
    "#         required_skills = [\"Python\", \"SQL\", \"Machine Learning\", \"Statistics\"]\n",
    "        \n",
    "#         matched, missing, ratio = compute_skill_matches(candidate_skills, required_skills)\n",
    "        \n",
    "#         self.assertEqual(len(matched), 2)\n",
    "#         self.assertEqual(len(missing), 2)\n",
    "#         self.assertEqual(ratio, 0.5)  # 2/4 = 0.5\n",
    "#         print(\"✓ Skill matching test passed\")\n",
    "    \n",
    "#     def test_test_score_normalization(self):\n",
    "#         \"\"\"\n",
    "#         Test test score normalization\n",
    "#         \"\"\"\n",
    "#         # Normal case\n",
    "#         self.assertEqual(normalize_test_score(88), 0.88)\n",
    "        \n",
    "#         # Edge cases\n",
    "#         self.assertEqual(normalize_test_score(0), 0.0)\n",
    "#         self.assertEqual(normalize_test_score(100), 1.0)\n",
    "#         self.assertEqual(normalize_test_score(None), 0.0)\n",
    "        \n",
    "#         # Clamping\n",
    "#         self.assertEqual(normalize_test_score(-10), 0.0)\n",
    "#         self.assertEqual(normalize_test_score(120), 1.0)\n",
    "#         print(\"✓ Test score normalization test passed\")\n",
    "    \n",
    "#     def test_feature_extraction(self):\n",
    "#         \"\"\"\n",
    "#         Test feature extraction function\n",
    "#         \"\"\"\n",
    "#         sample_resume = {\n",
    "#             \"skills\": [\"Python\", \"SQL\", \"Pandas\"],\n",
    "#             \"projects\": [\"Project 1\", \"Project 2\"],\n",
    "#             \"work_experience\": [{\"title\": \"Data Analyst\", \"years\": 2}, {\"title\": \"Developer\", \"years\": 1}],\n",
    "#             \"test_score\": 85,\n",
    "#             \"preferred_domain\": \"Data Science\"\n",
    "#         }\n",
    "        \n",
    "#         features = extract_features(sample_resume, self.sample_domain_requirements)\n",
    "        \n",
    "#         self.assertIn('skill_match_ratio', features)\n",
    "#         self.assertIn('test_score_norm', features)\n",
    "#         self.assertEqual(features['project_count'], 2)\n",
    "#         self.assertEqual(features['total_experience'], 3)\n",
    "#         print(\"✓ Feature extraction test passed\")\n",
    "\n",
    "# class TestIntegration(unittest.TestCase):\n",
    "#     \"\"\"\n",
    "#     Integration tests for complete pipeline\n",
    "#     \"\"\"\n",
    "    \n",
    "#     def setUp(self):\n",
    "#         self.sample_resume = {\n",
    "#             \"skills\": [\"Python\", \"Pandas\", \"SQL\"],\n",
    "#             \"projects\": [\"Data Analysis Project\", \"ML Pipeline\"],\n",
    "#             \"work_experience\": [{\"title\": \"Data Analyst\", \"years\": 2}],\n",
    "#             \"test_score\": 75,\n",
    "#             \"preferred_domain\": \"Data Science\",\n",
    "#             \"id\": \"test_candidate_001\"\n",
    "#         }\n",
    "    \n",
    "#     def test_complete_pipeline(self):\n",
    "#         \"\"\"\n",
    "#         Test complete classification pipeline\n",
    "#         \"\"\"\n",
    "#         result = classification_pipeline.classify_resume(self.sample_resume)\n",
    "        \n",
    "#         # Verify output structure\n",
    "#         required_keys = ['label', 'confidence', 'matched_skills', 'missing_skills', \n",
    "#                         'feature_summary', 'explanation', 'metadata']\n",
    "#         for key in required_keys:\n",
    "#             self.assertIn(key, result)\n",
    "        \n",
    "#         # Verify data types\n",
    "#         self.assertIsInstance(result['confidence'], float)\n",
    "#         self.assertIsInstance(result['matched_skills'], list)\n",
    "#         self.assertIsInstance(result['missing_skills'], list)\n",
    "#         self.assertIsInstance(result['explanation'], str)\n",
    "        \n",
    "#         # Verify value ranges\n",
    "#         self.assertGreaterEqual(result['confidence'], 0.0)\n",
    "#         self.assertLessEqual(result['confidence'], 1.0)\n",
    "        \n",
    "#         print(\"✓ Complete pipeline integration test passed\")\n",
    "#         print(f\"  Classification result: {result['label']} (confidence: {result['confidence']:.3f})\")\n",
    "\n",
    "# class TestDataValidation(unittest.TestCase):\n",
    "#     \"\"\"\n",
    "#     Tests for data validation and edge cases\n",
    "#     \"\"\"\n",
    "    \n",
    "#     def test_empty_resume(self):\n",
    "#         \"\"\"\n",
    "#         Test handling of empty/minimal resume\n",
    "#         \"\"\"\n",
    "#         empty_resume = {\"id\": \"empty_001\"}\n",
    "#         result = classification_pipeline.classify_resume(empty_resume)\n",
    "        \n",
    "#         self.assertIn('label', result)\n",
    "#         self.assertIn('confidence', result)\n",
    "#         print(\"✓ Empty resume handling test passed\")\n",
    "    \n",
    "#     def test_invalid_test_scores(self):\n",
    "#         \"\"\"\n",
    "#         Test handling of invalid test scores\n",
    "#         \"\"\"\n",
    "#         test_cases = [None, -50, 150, \"invalid\", []]\n",
    "        \n",
    "#         for invalid_score in test_cases:\n",
    "#             normalized = normalize_test_score(invalid_score)\n",
    "#             self.assertGreaterEqual(normalized, 0.0)\n",
    "#             self.assertLessEqual(normalized, 1.0)\n",
    "        \n",
    "#         print(\"✓ Invalid test score handling test passed\")\n",
    "    \n",
    "#     def test_case_insensitive_skills(self):\n",
    "#         \"\"\"\n",
    "#         Test case insensitive skill matching\n",
    "#         \"\"\"\n",
    "#         candidate_skills = [\"PYTHON\", \"sql\", \"Machine Learning\"]\n",
    "#         required_skills = [\"Python\", \"SQL\", \"Machine Learning\", \"Statistics\"]\n",
    "        \n",
    "#         matched, missing, ratio = compute_skill_matches(candidate_skills, required_skills)\n",
    "        \n",
    "#         self.assertEqual(len(matched), 3)  # All should match despite case differences\n",
    "#         print(\"✓ Case insensitive skill matching test passed\")\n",
    "\n",
    "# # Production monitoring utilities\n",
    "# class ProductionMonitor:\n",
    "#     \"\"\"\n",
    "#     Advanced monitoring for production deployment\n",
    "#     \"\"\"\n",
    "    \n",
    "#     def __init__(self):\n",
    "#         self.metrics = {\n",
    "#             'prediction_count': 0,\n",
    "#             'error_count': 0,\n",
    "#             'low_confidence_count': 0,\n",
    "#             'avg_processing_time': 0,\n",
    "#             'score_distribution': [],\n",
    "#             'confidence_distribution': [],\n",
    "#             'label_distribution': {'Fit': 0, 'Partial Fit': 0, 'Not Fit': 0}\n",
    "#         }\n",
    "#         self.recent_predictions = []\n",
    "#         self.error_log = []\n",
    "    \n",
    "#     def log_prediction(self, resume, result, processing_time=None):\n",
    "#         \"\"\"\n",
    "#         Log a prediction for monitoring\n",
    "#         \"\"\"\n",
    "#         self.metrics['prediction_count'] += 1\n",
    "        \n",
    "#         if processing_time:\n",
    "#             # Update average processing time\n",
    "#             current_avg = self.metrics['avg_processing_time']\n",
    "#             count = self.metrics['prediction_count']\n",
    "#             self.metrics['avg_processing_time'] = (current_avg * (count - 1) + processing_time) / count\n",
    "        \n",
    "#         # Track confidence\n",
    "#         confidence = result.get('confidence', 0)\n",
    "#         self.metrics['confidence_distribution'].append(confidence)\n",
    "        \n",
    "#         if confidence < 0.6:\n",
    "#             self.metrics['low_confidence_count'] += 1\n",
    "        \n",
    "#         # Track test scores\n",
    "#         if 'test_score' in resume:\n",
    "#             self.metrics['score_distribution'].append(resume['test_score'])\n",
    "        \n",
    "#         # Track labels\n",
    "#         label = result.get('label', 'Unknown')\n",
    "#         if label in self.metrics['label_distribution']:\n",
    "#             self.metrics['label_distribution'][label] += 1\n",
    "        \n",
    "#         # Keep recent predictions for drift detection\n",
    "#         prediction_data = {\n",
    "#             'timestamp': datetime.now(),\n",
    "#             'test_score': resume.get('test_score', 0),\n",
    "#             'confidence': confidence,\n",
    "#             'label': label,\n",
    "#             'skill_count': len(resume.get('skills', [])),\n",
    "#             'processing_time': processing_time\n",
    "#         }\n",
    "#         self.recent_predictions.append(prediction_data)\n",
    "        \n",
    "#         # Keep only last 1000 predictions\n",
    "#         if len(self.recent_predictions) > 1000:\n",
    "#             self.recent_predictions = self.recent_predictions[-1000:]\n",
    "    \n",
    "#     def log_error(self, error_type, error_message, resume_id=None):\n",
    "#         \"\"\"\n",
    "#         Log an error\n",
    "#         \"\"\"\n",
    "#         self.metrics['error_count'] += 1\n",
    "#         self.error_log.append({\n",
    "#             'timestamp': datetime.now(),\n",
    "#             'error_type': error_type,\n",
    "#             'error_message': error_message,\n",
    "#             'resume_id': resume_id\n",
    "#         })\n",
    "        \n",
    "#         # Keep only last 100 errors\n",
    "#         if len(self.error_log) > 100:\n",
    "#             self.error_log = self.error_log[-100:]\n",
    "    \n",
    "#     def get_metrics_summary(self):\n",
    "#         \"\"\"\n",
    "#         Get summary of current metrics\n",
    "#         \"\"\"\n",
    "#         if not self.recent_predictions:\n",
    "#             return {\"status\": \"no_data\"}\n",
    "        \n",
    "#         recent_confidences = [p['confidence'] for p in self.recent_predictions[-100:]]\n",
    "#         recent_scores = [p['test_score'] for p in self.recent_predictions[-100:] if p['test_score'] > 0]\n",
    "#         recent_times = [p['processing_time'] for p in self.recent_predictions[-100:] if p['processing_time']]\n",
    "        \n",
    "#         return {\n",
    "#             'total_predictions': self.metrics['prediction_count'],\n",
    "#             'error_rate': self.metrics['error_count'] / max(self.metrics['prediction_count'], 1),\n",
    "#             'low_confidence_rate': self.metrics['low_confidence_count'] / max(self.metrics['prediction_count'], 1),\n",
    "#             'avg_confidence': np.mean(recent_confidences) if recent_confidences else 0,\n",
    "#             'avg_test_score': np.mean(recent_scores) if recent_scores else 0,\n",
    "#             'avg_processing_time': np.mean(recent_times) if recent_times else 0,\n",
    "#             'label_distribution': self.metrics['label_distribution'],\n",
    "#             'recent_error_count': len([e for e in self.error_log if e['timestamp'] > datetime.now() - timedelta(hours=24)])\n",
    "#         }\n",
    "    \n",
    "#     def track_model_drift(self, baseline_stats=None):\n",
    "#         \"\"\"\n",
    "#         Detect model drift using statistical tests\n",
    "#         \"\"\"\n",
    "#         if len(self.recent_predictions) < 30:\n",
    "#             return {\"status\": \"insufficient_data\"}\n",
    "        \n",
    "#         if baseline_stats is None:\n",
    "#             baseline_stats = {\n",
    "#                 'mean_test_score': 65,\n",
    "#                 'mean_confidence': 0.75,\n",
    "#                 'label_distribution': {'Fit': 0.33, 'Partial Fit': 0.34, 'Not Fit': 0.33}\n",
    "#             }\n",
    "        \n",
    "#         recent_30 = self.recent_predictions[-30:]\n",
    "        \n",
    "#         # Test score distribution drift\n",
    "#         recent_scores = [p['test_score'] for p in recent_30 if p['test_score'] > 0]\n",
    "#         baseline_mean_score = baseline_stats.get('mean_test_score', 65)\n",
    "        \n",
    "#         if recent_scores:\n",
    "#             recent_mean_score = np.mean(recent_scores)\n",
    "#             score_drift = abs(recent_mean_score - baseline_mean_score) > 10  # Threshold\n",
    "#         else:\n",
    "#             recent_mean_score = 0\n",
    "#             score_drift = False\n",
    "        \n",
    "#         # Confidence distribution drift\n",
    "#         recent_confidences = [p['confidence'] for p in recent_30]\n",
    "#         baseline_confidence = baseline_stats.get('mean_confidence', 0.75)\n",
    "#         recent_mean_confidence = np.mean(recent_confidences)\n",
    "#         confidence_drift = abs(recent_mean_confidence - baseline_confidence) > 0.15\n",
    "        \n",
    "#         # Label distribution drift\n",
    "#         recent_label_counts = {'Fit': 0, 'Partial Fit': 0, 'Not Fit': 0}\n",
    "#         for p in recent_30:\n",
    "#             if p['label'] in recent_label_counts:\n",
    "#                 recent_label_counts[p['label']] += 1\n",
    "        \n",
    "#         recent_label_props = {k: v/len(recent_30) for k, v in recent_label_counts.items()}\n",
    "#         baseline_label_props = baseline_stats.get('label_distribution', {})\n",
    "        \n",
    "#         label_drift = False\n",
    "#         for label, recent_prop in recent_label_props.items():\n",
    "#             baseline_prop = baseline_label_props.get(label, 0.33)\n",
    "#             if abs(recent_prop - baseline_prop) > 0.20:  # 20% threshold\n",
    "#                 label_drift = True\n",
    "#                 break\n",
    "        \n",
    "#         drift_status = \"drift_detected\" if (score_drift or confidence_drift or label_drift) else \"stable\"\n",
    "        \n",
    "#         return {\n",
    "#             \"status\": drift_status,\n",
    "#             \"score_drift\": score_drift,\n",
    "#             \"confidence_drift\": confidence_drift,\n",
    "#             \"label_drift\": label_drift,\n",
    "#             \"recent_score_mean\": recent_mean_score,\n",
    "#             \"baseline_score_mean\": baseline_mean_score,\n",
    "#             \"recent_confidence_mean\": recent_mean_confidence,\n",
    "#             \"baseline_confidence_mean\": baseline_confidence,\n",
    "#             \"recent_label_distribution\": recent_label_props,\n",
    "#             \"baseline_label_distribution\": baseline_label_props\n",
    "#         }\n",
    "\n",
    "# class PerformanceBenchmark:\n",
    "#     \"\"\"\n",
    "#     Benchmark performance of the classification system\n",
    "#     \"\"\"\n",
    "    \n",
    "#     def __init__(self, pipeline):\n",
    "#         self.pipeline = pipeline\n",
    "    \n",
    "#     def run_performance_test(self, test_resumes, iterations=100):\n",
    "#         \"\"\"\n",
    "#         Run performance benchmark\n",
    "#         \"\"\"\n",
    "#         processing_times = []\n",
    "        \n",
    "#         print(f\"Running performance test with {iterations} iterations...\")\n",
    "        \n",
    "#         for i in range(iterations):\n",
    "#             resume = test_resumes[i % len(test_resumes)]\n",
    "            \n",
    "#             start_time = time.time()\n",
    "#             result = self.pipeline.classify_resume(resume)\n",
    "#             end_time = time.time()\n",
    "            \n",
    "#             processing_time = (end_time - start_time) * 1000  # Convert to ms\n",
    "#             processing_times.append(processing_time)\n",
    "            \n",
    "#             if (i + 1) % 20 == 0:\n",
    "#                 print(f\"  Completed {i + 1}/{iterations} iterations\")\n",
    "        \n",
    "#         return {\n",
    "#             'mean_time_ms': np.mean(processing_times),\n",
    "#             'median_time_ms': np.median(processing_times),\n",
    "#             'p95_time_ms': np.percentile(processing_times, 95),\n",
    "#             'p99_time_ms': np.percentile(processing_times, 99),\n",
    "#             'min_time_ms': np.min(processing_times),\n",
    "#             'max_time_ms': np.max(processing_times),\n",
    "#             'total_iterations': iterations\n",
    "#         }\n",
    "\n",
    "# # Run all tests\n",
    "# def run_all_tests():\n",
    "#     \"\"\"\n",
    "#     Run all test suites\n",
    "#     \"\"\"\n",
    "#     print(\"🧪 Running Unit Tests...\")\n",
    "    \n",
    "#     # Create test suite\n",
    "#     test_suite = unittest.TestSuite()\n",
    "    \n",
    "#     # Add test cases\n",
    "#     test_suite.addTest(unittest.makeSuite(TestResumeClassifier))\n",
    "#     test_suite.addTest(unittest.makeSuite(TestIntegration))\n",
    "#     test_suite.addTest(unittest.makeSuite(TestDataValidation))\n",
    "    \n",
    "#     # Run tests\n",
    "#     runner = unittest.TextTestRunner(verbosity=0)\n",
    "#     result = runner.run(test_suite)\n",
    "    \n",
    "#     print(f\"\\n📊 Test Results:\")\n",
    "#     print(f\"  Tests run: {result.testsRun}\")\n",
    "#     print(f\"  Failures: {len(result.failures)}\")\n",
    "#     print(f\"  Errors: {len(result.errors)}\")\n",
    "    \n",
    "#     if result.failures:\n",
    "#         print(f\"\\n❌ Failures:\")\n",
    "#         for test, traceback in result.failures:\n",
    "#             print(f\"  {test}: {traceback}\")\n",
    "    \n",
    "#     if result.errors:\n",
    "#         print(f\"\\n❌ Errors:\")\n",
    "#         for test, traceback in result.errors:\n",
    "#             print(f\"  {test}: {traceback}\")\n",
    "    \n",
    "#     success_rate = (result.testsRun - len(result.failures) - len(result.errors)) / result.testsRun\n",
    "#     print(f\"\\n✅ Overall Success Rate: {success_rate:.1%}\")\n",
    "    \n",
    "#     return result\n",
    "\n",
    "# # Demonstrate monitoring\n",
    "# def demonstrate_monitoring():\n",
    "#     \"\"\"\n",
    "#     Demonstrate production monitoring\n",
    "#     \"\"\"\n",
    "#     print(\"\\n🔍 Demonstrating Production Monitoring...\")\n",
    "    \n",
    "#     monitor = ProductionMonitor()\n",
    "    \n",
    "#     # Generate sample predictions to monitor\n",
    "#     sample_resumes = [\n",
    "#         {\n",
    "#             \"skills\": [\"Python\", \"SQL\", \"Machine Learning\"],\n",
    "#             \"projects\": [\"ML Project\"],\n",
    "#             \"work_experience\": [{\"title\": \"Data Scientist\", \"years\": 3}],\n",
    "#             \"test_score\": 85,\n",
    "#             \"preferred_domain\": \"Data Science\",\n",
    "#             \"id\": \"demo_001\"\n",
    "#         },\n",
    "#         {\n",
    "#             \"skills\": [\"JavaScript\", \"React\"],\n",
    "#             \"projects\": [],\n",
    "#             \"work_experience\": [{\"title\": \"Developer\", \"years\": 1}],\n",
    "#             \"test_score\": 45,\n",
    "#             \"preferred_domain\": \"Web Development\",\n",
    "#             \"id\": \"demo_002\"\n",
    "#         },\n",
    "#         {\n",
    "#             \"skills\": [\"Python\", \"Flask\", \"HTML\"],\n",
    "#             \"projects\": [\"Web App\", \"API\"],\n",
    "#             \"work_experience\": [{\"title\": \"Full Stack Developer\", \"years\": 2}],\n",
    "#             \"test_score\": 70,\n",
    "#             \"preferred_domain\": \"Web Development\",\n",
    "#             \"id\": \"demo_003\"\n",
    "#         }\n",
    "#     ]\n",
    "    \n",
    "#     # Process sample resumes\n",
    "#     for i, resume in enumerate(sample_resumes * 20):  # 60 total predictions\n",
    "#         start_time = time.time()\n",
    "#         result = classification_pipeline.classify_resume(resume)\n",
    "#         processing_time = (time.time() - start_time) * 1000\n",
    "        \n",
    "#         monitor.log_prediction(resume, result, processing_time)\n",
    "        \n",
    "#         # Simulate occasional errors\n",
    "#         if i % 15 == 0:\n",
    "#             monitor.log_error(\"ValidationError\", \"Invalid resume format\", resume.get('id'))\n",
    "    \n",
    "#     # Get metrics summary\n",
    "#     summary = monitor.get_metrics_summary()\n",
    "#     print(f\"\\n📈 Monitoring Summary:\")\n",
    "#     print(f\"  Total Predictions: {summary['total_predictions']}\")\n",
    "#     print(f\"  Error Rate: {summary['error_rate']:.1%}\")\n",
    "#     print(f\"  Low Confidence Rate: {summary['low_confidence_rate']:.1%}\")\n",
    "#     print(f\"  Avg Confidence: {summary['avg_confidence']:.3f}\")\n",
    "#     print(f\"  Avg Test Score: {summary['avg_test_score']:.1f}\")\n",
    "#     print(f\"  Avg Processing Time: {summary['avg_processing_time']:.1f} ms\")\n",
    "#     print(f\"  Label Distribution: {summary['label_distribution']}\")\n",
    "    \n",
    "#     # Check for model drift\n",
    "#     drift_result = monitor.track_model_drift()\n",
    "#     print(f\"\\n🔄 Model Drift Analysis:\")\n",
    "#     print(f\"  Status: {drift_result['status']}\")\n",
    "#     if drift_result['status'] == 'drift_detected':\n",
    "#         print(f\"  Score Drift: {drift_result['score_drift']}\")\n",
    "#         print(f\"  Confidence Drift: {drift_result['confidence_drift']}\")\n",
    "#         print(f\"  Label Drift: {drift_result['label_drift']}\")\n",
    "    \n",
    "#     return monitor\n",
    "\n",
    "# # Run performance benchmark\n",
    "# def run_performance_benchmark():\n",
    "#     \"\"\"\n",
    "#     Run performance benchmark\n",
    "#     \"\"\"\n",
    "#     print(\"\\n⚡ Running Performance Benchmark...\")\n",
    "    \n",
    "#     benchmark = PerformanceBenchmark(classification_pipeline)\n",
    "    \n",
    "#     # Create test resumes\n",
    "#     test_resumes = [\n",
    "#         {\n",
    "#             \"skills\": [\"Python\", \"SQL\"],\n",
    "#             \"projects\": [\"Project A\"],\n",
    "#             \"work_experience\": [{\"title\": \"Analyst\", \"years\": 2}],\n",
    "#             \"test_score\": 75,\n",
    "#             \"preferred_domain\": \"Data Science\",\n",
    "#             \"id\": f\"perf_test_{i}\"\n",
    "#         } for i in range(10)\n",
    "#     ]\n",
    "    \n",
    "#     results = benchmark.run_performance_test(test_resumes, iterations=100)\n",
    "    \n",
    "#     print(f\"\\n📊 Performance Results:\")\n",
    "#     print(f\"  Mean Time: {results['mean_time_ms']:.2f} ms\")\n",
    "#     print(f\"  Median Time: {results['median_time_ms']:.2f} ms\")\n",
    "#     print(f\"  95th Percentile: {results['p95_time_ms']:.2f} ms\")\n",
    "#     print(f\"  99th Percentile: {results['p99_time_ms']:.2f} ms\")\n",
    "#     print(f\"  Min Time: {results['min_time_ms']:.2f} ms\")\n",
    "#     print(f\"  Max Time: {results['max_time_ms']:.2f} ms\")\n",
    "    \n",
    "#     return results\n",
    "\n",
    "# # Main execution\n",
    "# if __name__ == \"__main__\":\n",
    "#     print(\"🚀 Starting Comprehensive Testing & Monitoring Suite\")\n",
    "    \n",
    "#     # Run tests\n",
    "#     test_results = run_all_tests()\n",
    "    \n",
    "#     # Demonstrate monitoring\n",
    "#     monitor = demonstrate_monitoring()\n",
    "    \n",
    "#     # Run performance benchmark\n",
    "#     perf_results = run_performance_benchmark()\n",
    "    \n",
    "#     print(f\"\\n✅ All testing and monitoring demonstrations completed successfully!\")\n",
    "#     print(f\"   - Unit tests passed\")\n",
    "#     print(f\"   - Integration tests passed\") \n",
    "#     print(f\"   - Production monitoring demonstrated\")\n",
    "#     print(f\"   - Performance benchmarking completed\")\n",
    "#     print(f\"   - System ready for production deployment!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911b438b-4f6a-4275-a292-0de12787ba81",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d47b342-2a77-4bb5-849e-b7ad635894c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44581b11-42a1-4c55-af41-71519afe9680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Step 10: Final JSON Output Generation ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.0.0 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"C:\\Users\\pavan\\anaconda3\\Lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"C:\\Users\\pavan\\anaconda3\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"C:\\Users\\pavan\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 701, in start\n",
      "    self.io_loop.start()\n",
      "  File \"C:\\Users\\pavan\\AppData\\Roaming\\Python\\Python312\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"C:\\Users\\pavan\\anaconda3\\Lib\\asyncio\\windows_events.py\", line 322, in run_forever\n",
      "    super().run_forever()\n",
      "  File \"C:\\Users\\pavan\\anaconda3\\Lib\\asyncio\\base_events.py\", line 641, in run_forever\n",
      "    self._run_once()\n",
      "  File \"C:\\Users\\pavan\\anaconda3\\Lib\\asyncio\\base_events.py\", line 1986, in _run_once\n",
      "    handle._run()\n",
      "  File \"C:\\Users\\pavan\\anaconda3\\Lib\\asyncio\\events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"C:\\Users\\pavan\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"C:\\Users\\pavan\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 523, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"C:\\Users\\pavan\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 429, in dispatch_shell\n",
      "    await result\n",
      "  File \"C:\\Users\\pavan\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 767, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"C:\\Users\\pavan\\anaconda3\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 429, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"C:\\Users\\pavan\\anaconda3\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"C:\\Users\\pavan\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"C:\\Users\\pavan\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"C:\\Users\\pavan\\anaconda3\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"C:\\Users\\pavan\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"C:\\Users\\pavan\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"C:\\Users\\pavan\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\pavan\\AppData\\Local\\Temp\\ipykernel_1044980\\4242960888.py\", line 9, in <module>\n",
      "    import keras\n",
      "  File \"C:\\Users\\pavan\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\__init__.py\", line 7, in <module>\n",
      "    from keras import _tf_keras as _tf_keras\n",
      "  File \"C:\\Users\\pavan\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\_tf_keras\\__init__.py\", line 1, in <module>\n",
      "    from keras._tf_keras import keras\n",
      "  File \"C:\\Users\\pavan\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\_tf_keras\\keras\\__init__.py\", line 7, in <module>\n",
      "    from keras import activations as activations\n",
      "  File \"C:\\Users\\pavan\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\activations\\__init__.py\", line 7, in <module>\n",
      "    from keras.src.activations import deserialize as deserialize\n",
      "  File \"C:\\Users\\pavan\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\__init__.py\", line 8, in <module>\n",
      "    from keras.src import models\n",
      "  File \"C:\\Users\\pavan\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\models\\__init__.py\", line 1, in <module>\n",
      "    from keras.src.models.functional import Functional\n",
      "  File \"C:\\Users\\pavan\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\models\\functional.py\", line 16, in <module>\n",
      "    from keras.src.models.model import Model\n",
      "  File \"C:\\Users\\pavan\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\models\\model.py\", line 12, in <module>\n",
      "    from keras.src.trainers import trainer as base_trainer\n",
      "  File \"C:\\Users\\pavan\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\trainers\\trainer.py\", line 14, in <module>\n",
      "    from keras.src.trainers.data_adapters import data_adapter_utils\n",
      "  File \"C:\\Users\\pavan\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\trainers\\data_adapters\\__init__.py\", line 4, in <module>\n",
      "    from keras.src.trainers.data_adapters import array_data_adapter\n",
      "  File \"C:\\Users\\pavan\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\trainers\\data_adapters\\array_data_adapter.py\", line 7, in <module>\n",
      "    from keras.src.trainers.data_adapters import array_slicing\n",
      "  File \"C:\\Users\\pavan\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\trainers\\data_adapters\\array_slicing.py\", line 12, in <module>\n",
      "    import pandas\n",
      "  File \"C:\\Users\\pavan\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\__init__.py\", line 62, in <module>\n",
      "    from pandas.core.api import (\n",
      "  File \"C:\\Users\\pavan\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\api.py\", line 28, in <module>\n",
      "    from pandas.core.arrays import Categorical\n",
      "  File \"C:\\Users\\pavan\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\arrays\\__init__.py\", line 1, in <module>\n",
      "    from pandas.core.arrays.arrow import ArrowExtensionArray\n",
      "  File \"C:\\Users\\pavan\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\arrays\\arrow\\__init__.py\", line 5, in <module>\n",
      "    from pandas.core.arrays.arrow.array import ArrowExtensionArray\n",
      "  File \"C:\\Users\\pavan\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\arrays\\arrow\\array.py\", line 50, in <module>\n",
      "    from pandas.core import (\n",
      "  File \"C:\\Users\\pavan\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\ops\\__init__.py\", line 8, in <module>\n",
      "    from pandas.core.ops.array_ops import (\n",
      "  File \"C:\\Users\\pavan\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\ops\\array_ops.py\", line 56, in <module>\n",
      "    from pandas.core.computation import expressions\n",
      "  File \"C:\\Users\\pavan\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\computation\\expressions.py\", line 21, in <module>\n",
      "    from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "  File \"C:\\Users\\pavan\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\computation\\check.py\", line 5, in <module>\n",
      "    ne = import_optional_dependency(\"numexpr\", errors=\"warn\")\n",
      "  File \"C:\\Users\\pavan\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\compat\\_optional.py\", line 135, in import_optional_dependency\n",
      "    module = importlib.import_module(name)\n",
      "  File \"C:\\Users\\pavan\\anaconda3\\Lib\\importlib\\__init__.py\", line 90, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"C:\\Users\\pavan\\anaconda3\\Lib\\site-packages\\numexpr\\__init__.py\", line 24, in <module>\n",
      "    from numexpr.interpreter import MAX_THREADS, use_vml, __BLOCK_SIZE1__\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "_ARRAY_API not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;31mAttributeError\u001b[0m: _ARRAY_API not found"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.0.0 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"C:\\Users\\pavan\\anaconda3\\Lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"C:\\Users\\pavan\\anaconda3\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"C:\\Users\\pavan\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 701, in start\n",
      "    self.io_loop.start()\n",
      "  File \"C:\\Users\\pavan\\AppData\\Roaming\\Python\\Python312\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"C:\\Users\\pavan\\anaconda3\\Lib\\asyncio\\windows_events.py\", line 322, in run_forever\n",
      "    super().run_forever()\n",
      "  File \"C:\\Users\\pavan\\anaconda3\\Lib\\asyncio\\base_events.py\", line 641, in run_forever\n",
      "    self._run_once()\n",
      "  File \"C:\\Users\\pavan\\anaconda3\\Lib\\asyncio\\base_events.py\", line 1986, in _run_once\n",
      "    handle._run()\n",
      "  File \"C:\\Users\\pavan\\anaconda3\\Lib\\asyncio\\events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"C:\\Users\\pavan\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"C:\\Users\\pavan\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 523, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"C:\\Users\\pavan\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 429, in dispatch_shell\n",
      "    await result\n",
      "  File \"C:\\Users\\pavan\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 767, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"C:\\Users\\pavan\\anaconda3\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 429, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"C:\\Users\\pavan\\anaconda3\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"C:\\Users\\pavan\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"C:\\Users\\pavan\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"C:\\Users\\pavan\\anaconda3\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"C:\\Users\\pavan\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"C:\\Users\\pavan\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"C:\\Users\\pavan\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\pavan\\AppData\\Local\\Temp\\ipykernel_1044980\\4242960888.py\", line 9, in <module>\n",
      "    import keras\n",
      "  File \"C:\\Users\\pavan\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\__init__.py\", line 7, in <module>\n",
      "    from keras import _tf_keras as _tf_keras\n",
      "  File \"C:\\Users\\pavan\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\_tf_keras\\__init__.py\", line 1, in <module>\n",
      "    from keras._tf_keras import keras\n",
      "  File \"C:\\Users\\pavan\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\_tf_keras\\keras\\__init__.py\", line 7, in <module>\n",
      "    from keras import activations as activations\n",
      "  File \"C:\\Users\\pavan\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\activations\\__init__.py\", line 7, in <module>\n",
      "    from keras.src.activations import deserialize as deserialize\n",
      "  File \"C:\\Users\\pavan\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\__init__.py\", line 8, in <module>\n",
      "    from keras.src import models\n",
      "  File \"C:\\Users\\pavan\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\models\\__init__.py\", line 1, in <module>\n",
      "    from keras.src.models.functional import Functional\n",
      "  File \"C:\\Users\\pavan\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\models\\functional.py\", line 16, in <module>\n",
      "    from keras.src.models.model import Model\n",
      "  File \"C:\\Users\\pavan\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\models\\model.py\", line 12, in <module>\n",
      "    from keras.src.trainers import trainer as base_trainer\n",
      "  File \"C:\\Users\\pavan\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\trainers\\trainer.py\", line 14, in <module>\n",
      "    from keras.src.trainers.data_adapters import data_adapter_utils\n",
      "  File \"C:\\Users\\pavan\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\trainers\\data_adapters\\__init__.py\", line 4, in <module>\n",
      "    from keras.src.trainers.data_adapters import array_data_adapter\n",
      "  File \"C:\\Users\\pavan\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\trainers\\data_adapters\\array_data_adapter.py\", line 7, in <module>\n",
      "    from keras.src.trainers.data_adapters import array_slicing\n",
      "  File \"C:\\Users\\pavan\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\trainers\\data_adapters\\array_slicing.py\", line 12, in <module>\n",
      "    import pandas\n",
      "  File \"C:\\Users\\pavan\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\__init__.py\", line 62, in <module>\n",
      "    from pandas.core.api import (\n",
      "  File \"C:\\Users\\pavan\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\api.py\", line 28, in <module>\n",
      "    from pandas.core.arrays import Categorical\n",
      "  File \"C:\\Users\\pavan\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\arrays\\__init__.py\", line 1, in <module>\n",
      "    from pandas.core.arrays.arrow import ArrowExtensionArray\n",
      "  File \"C:\\Users\\pavan\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\arrays\\arrow\\__init__.py\", line 5, in <module>\n",
      "    from pandas.core.arrays.arrow.array import ArrowExtensionArray\n",
      "  File \"C:\\Users\\pavan\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\arrays\\arrow\\array.py\", line 64, in <module>\n",
      "    from pandas.core.arrays.masked import BaseMaskedArray\n",
      "  File \"C:\\Users\\pavan\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\arrays\\masked.py\", line 60, in <module>\n",
      "    from pandas.core import (\n",
      "  File \"C:\\Users\\pavan\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\nanops.py\", line 52, in <module>\n",
      "    bn = import_optional_dependency(\"bottleneck\", errors=\"warn\")\n",
      "  File \"C:\\Users\\pavan\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\compat\\_optional.py\", line 135, in import_optional_dependency\n",
      "    module = importlib.import_module(name)\n",
      "  File \"C:\\Users\\pavan\\anaconda3\\Lib\\importlib\\__init__.py\", line 90, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"C:\\Users\\pavan\\anaconda3\\Lib\\site-packages\\bottleneck\\__init__.py\", line 7, in <module>\n",
      "    from .move import (move_argmax, move_argmin, move_max, move_mean, move_median,\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "\nA module that was compiled using NumPy 1.x cannot be run in\nNumPy 2.0.0 as it may crash. To support both 1.x and 2.x\nversions of NumPy, modules must be compiled with NumPy 2.0.\nSome module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n\nIf you are a user of the module, the easiest solution will be to\ndowngrade to 'numpy<2' or try to upgrade the affected module.\nWe expect that some modules will need time to support NumPy 2.\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\numpy\\core\\_multiarray_umath.py:44\u001b[0m, in \u001b[0;36m__getattr__\u001b[1;34m(attr_name)\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;66;03m# Also print the message (with traceback).  This is because old versions\u001b[39;00m\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;66;03m# of NumPy unfortunately set up the import to replace (and hide) the\u001b[39;00m\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;66;03m# error.  The traceback shouldn't be needed, but e.g. pytest plugins\u001b[39;00m\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;66;03m# seem to swallow it and we should be failing anyway...\u001b[39;00m\n\u001b[0;32m     43\u001b[0m     sys\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mwrite(msg \u001b[38;5;241m+\u001b[39m tb_msg)\n\u001b[1;32m---> 44\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[0;32m     46\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(_multiarray_umath, attr_name, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mImportError\u001b[0m: \nA module that was compiled using NumPy 1.x cannot be run in\nNumPy 2.0.0 as it may crash. To support both 1.x and 2.x\nversions of NumPy, modules must be compiled with NumPy 2.0.\nSome module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n\nIf you are a user of the module, the easiest solution will be to\ndowngrade to 'numpy<2' or try to upgrade the affected module.\nWe expect that some modules will need time to support NumPy 2.\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Loading Pipeline from Saved Artifacts (Production Mode) ===\n",
      "\n",
      "=== Loading Classification Pipeline from Saved Artifacts ===\n",
      "✓ Found model manifest: resume_classifier v1.0\n",
      "  Created: 2025-09-29T22:28:32.024462\n",
      "  Test Accuracy: 0.7800\n",
      "\n",
      "✓ Loading complete pipeline from: artifacts/classification_pipeline.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pavan\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\saving\\saving_lib.py:797: UserWarning: Skipping variable loading for optimizer 'rmsprop', because it has 20 variables whereas the saved optimizer has 38 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✗ ERROR: Failed to load pipeline: Can't get attribute 'FeatureVectorBuilder' on <module '__main__'>\n",
      "\n",
      "Please ensure you have run Step 8 to save all models and artifacts.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Cannot proceed without saved models. Run Step 8 first.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 393\u001b[0m\n\u001b[0;32m    391\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m✗ ERROR: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmanifest\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    392\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mPlease ensure you have run Step 8 to save all models and artifacts.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 393\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot proceed without saved models. Run Step 8 first.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    394\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    395\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m✓ Successfully loaded pipeline from artifacts!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Cannot proceed without saved models. Run Step 8 first."
     ]
    }
   ],
   "source": [
    "# Step 10 - Postprocessing: Building Final JSON Output with Model Loading\n",
    "print(\"\\n=== Step 10: Final JSON Output Generation ===\")\n",
    "\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import keras\n",
    "\n",
    "class ResumeClassificationPipeline:\n",
    "    \"\"\"\n",
    "    Complete pipeline for resume classification with JSON output\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, feature_builder, label_encoder, skill_vocab, \n",
    "                 domain_requirements, scaler):\n",
    "        self.model = model\n",
    "        self.feature_builder = feature_builder\n",
    "        self.label_encoder = label_encoder\n",
    "        self.skill_vocab = skill_vocab\n",
    "        self.domain_requirements = domain_requirements\n",
    "        self.scaler = scaler\n",
    "        \n",
    "    def classify_resume(self, resume_json, include_raw_scores=True, precision=3):\n",
    "        \"\"\"\n",
    "        Complete pipeline: raw resume JSON → final classification JSON\n",
    "        Includes alternative domain suggestions for Partial Fit/Not Fit\n",
    "        \n",
    "        Pipeline:\n",
    "        1. Run class_probs = model.predict(final_vector)\n",
    "        2. pred_idx = argmax(class_probs); label = classes[pred_idx]\n",
    "        3. confidence = float(class_probs[pred_idx])\n",
    "        4. matched_skills, missing_skills from Step 3\n",
    "        5. feature_summary = {...}\n",
    "        6. explanation = construct from template\n",
    "        7. alternative_domains for Partial Fit/Not Fit\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Extract all features using Step 3 pipeline\n",
    "            resume_features = extract_all_features(resume_json, self.skill_vocab, self.domain_requirements)\n",
    "            \n",
    "            # Apply scaling to numeric features\n",
    "            scaled_numeric = self.scaler.transform([resume_features['numeric_features']])\n",
    "            resume_features['scaled_numeric_features'] = scaled_numeric[0]\n",
    "            \n",
    "            # 1. Build final feature vector and get model prediction\n",
    "            final_vector = self.feature_builder.build_final_vector(resume_features)\n",
    "            model_inputs = model_classifier.prepare_inputs(final_vector.reshape(1, -1), self.feature_builder)\n",
    "            class_probs = self.model.predict(model_inputs, verbose=0)[0]\n",
    "            \n",
    "            # 2. Get prediction and label\n",
    "            pred_idx = np.argmax(class_probs)\n",
    "            label = self.label_encoder.classes_[pred_idx]\n",
    "            \n",
    "            # 3. Get confidence (formatted to specified precision)\n",
    "            confidence = float(class_probs[pred_idx])\n",
    "            confidence = round(confidence, precision)\n",
    "            \n",
    "            # 4. Matched & missing skills (from Step 3)\n",
    "            matched_skills = resume_features['matched_skills']\n",
    "            missing_skills = resume_features['missing_skills']\n",
    "            \n",
    "            # 5. Feature summary with proper numeric formatting\n",
    "            skill_match_ratio = resume_features['skill_match_ratio']\n",
    "            years_experience = resume_features['years_experience'] \n",
    "            test_score_raw = resume_features['test_score']\n",
    "            test_score_norm = resume_features['test_score_norm']\n",
    "            project_count = resume_features['project_count']\n",
    "            \n",
    "            # Format skill_match_ratio: 8÷20 = 0.4 → format as 0.40 or 0.400\n",
    "            formatted_skill_ratio = round(skill_match_ratio, precision)\n",
    "            \n",
    "            feature_summary = {\n",
    "                \"skill_match_ratio\": formatted_skill_ratio,\n",
    "                \"years_experience\": int(years_experience),\n",
    "                \"test_score_norm\": round(test_score_norm, precision),\n",
    "                \"project_count\": int(project_count)\n",
    "            }\n",
    "            \n",
    "            # Include raw test score if requested\n",
    "            if include_raw_scores:\n",
    "                feature_summary[\"test_score_raw\"] = int(test_score_raw)\n",
    "            \n",
    "            # 7. Get alternative domain suggestions for Partial Fit/Not Fit\n",
    "            alternative_domains = None\n",
    "            if label in [\"Partial Fit\", \"Not Fit\"]:\n",
    "                alternative_domains = self._format_alternative_domains(\n",
    "                    resume_features.get('alternative_domains', []),\n",
    "                    precision\n",
    "                )\n",
    "            \n",
    "            # 6. Generate explanation using template (with alternative domains)\n",
    "            explanation = self._generate_explanation(\n",
    "                test_score_raw, skill_match_ratio, matched_skills, missing_skills,\n",
    "                project_count, years_experience, label, confidence,\n",
    "                alternative_domains\n",
    "            )\n",
    "            \n",
    "            # Build final JSON output\n",
    "            result = {\n",
    "                \"label\": label,\n",
    "                \"confidence\": confidence,\n",
    "                \"matched_skills\": matched_skills,\n",
    "                \"missing_skills\": missing_skills,\n",
    "                \"feature_summary\": feature_summary,\n",
    "                \"explanation\": explanation,\n",
    "                \"metadata\": {\n",
    "                    \"domain\": resume_features['domain'],\n",
    "                    \"candidate_id\": resume_features['id'],\n",
    "                    \"classification_timestamp\": datetime.now().isoformat(),\n",
    "                    \"model_version\": \"1.0\"\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            # Add alternative domain suggestions if applicable\n",
    "            if alternative_domains:\n",
    "                result[\"alternative_domain_suggestions\"] = alternative_domains\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"error\": f\"Classification failed: {str(e)}\",\n",
    "                \"candidate_id\": resume_json.get('id', 'unknown'),\n",
    "                \"timestamp\": datetime.now().isoformat()\n",
    "            }\n",
    "    \n",
    "    def _format_alternative_domains(self, suggestions, precision=3):\n",
    "        \"\"\"\n",
    "        Format alternative domain suggestions for JSON output\n",
    "        \n",
    "        Args:\n",
    "            suggestions: List of alternative domain suggestions from Step 3\n",
    "            precision: Number of decimal places for ratios\n",
    "        \n",
    "        Returns:\n",
    "            List of formatted domain suggestions\n",
    "        \"\"\"\n",
    "        if not suggestions:\n",
    "            return None\n",
    "        \n",
    "        formatted = []\n",
    "        \n",
    "        for i, suggestion in enumerate(suggestions, 1):\n",
    "            formatted.append({\n",
    "                \"rank\": i,\n",
    "                \"domain\": suggestion['domain'],\n",
    "                \"skill_match_ratio\": round(suggestion['skill_match_ratio'], precision),\n",
    "                \"matched_skills_count\": suggestion['matched_count'],\n",
    "                \"required_skills_count\": suggestion['required_count'],\n",
    "                \"matched_skills\": suggestion['matched_skills'][:5],  # Top 5\n",
    "                \"key_missing_skills\": suggestion['missing_skills'][:3]  # Top 3\n",
    "            })\n",
    "        \n",
    "        return formatted\n",
    "    \n",
    "    def _generate_explanation(self, test_score, skill_match_ratio, matched_skills, \n",
    "                            missing_skills, project_count, years_experience, \n",
    "                            predicted_label, confidence, alternative_domains=None):\n",
    "        \"\"\"\n",
    "        Generate human-readable explanation using template from Step 8\n",
    "        Includes alternative domain suggestion in explanation\n",
    "        \"\"\"\n",
    "        # Score description\n",
    "        if test_score >= 85:\n",
    "            score_desc = \"Excellent\"\n",
    "        elif test_score >= 75:\n",
    "            score_desc = \"High\"\n",
    "        elif test_score >= 60:\n",
    "            score_desc = \"Good\" \n",
    "        elif test_score >= 50:\n",
    "            score_desc = \"Fair\"\n",
    "        else:\n",
    "            score_desc = \"Low\"\n",
    "        \n",
    "        # Skills description\n",
    "        total_required = len(matched_skills) + len(missing_skills)\n",
    "        skills_fraction = f\"({len(matched_skills)}/{total_required} matched)\"\n",
    "        \n",
    "        if skill_match_ratio >= 0.8:\n",
    "            skills_desc = f\"covers most required skills {skills_fraction}\"\n",
    "        elif skill_match_ratio >= 0.6:\n",
    "            skills_desc = f\"covers many required skills {skills_fraction}\"\n",
    "        elif skill_match_ratio >= 0.4:\n",
    "            skills_desc = f\"covers some required skills {skills_fraction}\"\n",
    "        else:\n",
    "            skills_desc = f\"covers few required skills {skills_fraction}\"\n",
    "        \n",
    "        # Missing skills (top 3)\n",
    "        top_missing = missing_skills[:3]\n",
    "        missing_desc = f\", but lacks {', '.join(top_missing)}\" if top_missing else \"\"\n",
    "        \n",
    "        # Experience description\n",
    "        if years_experience >= 3:\n",
    "            exp_desc = f\"{int(years_experience)} years of solid experience\"\n",
    "        elif years_experience >= 1:\n",
    "            exp_desc = f\"{int(years_experience)} year{'s' if years_experience != 1 else ''} of experience\"\n",
    "        else:\n",
    "            exp_desc = \"limited professional experience\"\n",
    "        \n",
    "        # Project description\n",
    "        if project_count >= 3:\n",
    "            proj_desc = f\"strong portfolio ({project_count} projects)\"\n",
    "        elif project_count >= 1:\n",
    "            proj_desc = f\"{project_count} project{'s' if project_count != 1 else ''}\"\n",
    "        else:\n",
    "            proj_desc = \"no projects listed\"\n",
    "        \n",
    "        # Recommendation based on missing skills\n",
    "        recommendation = \"\"\n",
    "        if predicted_label == \"Partial Fit\" and missing_skills:\n",
    "            key_missing = [skill for skill in missing_skills[:2]]  # Top 2 missing\n",
    "            if key_missing:\n",
    "                recommendation = f\" Recommend gaining experience in {', '.join(key_missing)}.\"\n",
    "        \n",
    "        # Add alternative domain suggestion for Partial Fit/Not Fit\n",
    "        domain_suggestion = \"\"\n",
    "        if predicted_label in [\"Partial Fit\", \"Not Fit\"] and alternative_domains:\n",
    "            top_domain = alternative_domains[0]\n",
    "            match_pct = int(top_domain['skill_match_ratio'] * 100)\n",
    "            domain_suggestion = (f\" Consider applying for {top_domain['domain']} roles \"\n",
    "                               f\"({match_pct}% skill match with {top_domain['matched_skills_count']}\"\n",
    "                               f\"/{top_domain['required_skills_count']} required skills).\")\n",
    "        \n",
    "        # Combine into explanation\n",
    "        explanation = (f\"{score_desc} test score ({int(test_score)}/100) and {skills_desc}\"\n",
    "                      f\"{missing_desc}. Has {proj_desc} and {exp_desc}. \"\n",
    "                      f\"Model confidence: {confidence:.2f} → {predicted_label}.\"\n",
    "                      f\"{recommendation}{domain_suggestion}\")\n",
    "        \n",
    "        return explanation\n",
    "    \n",
    "    def batch_classify(self, resume_list, output_file=None):\n",
    "        \"\"\"\n",
    "        Classify multiple resumes and optionally save to file\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for i, resume in enumerate(resume_list):\n",
    "            print(f\"Processing resume {i+1}/{len(resume_list)}: {resume.get('id', 'unknown')}\")\n",
    "            result = self.classify_resume(resume)\n",
    "            results.append(result)\n",
    "        \n",
    "        if output_file:\n",
    "            with open(output_file, 'w') as f:\n",
    "                json.dump(results, f, indent=2)\n",
    "            print(f\"Saved {len(results)} results to {output_file}\")\n",
    "        \n",
    "        return results\n",
    "\n",
    "\n",
    "#==============================================================================\n",
    "# NEW: Load Saved Models and Artifacts from Step 8\n",
    "#==============================================================================\n",
    "\n",
    "def load_classification_pipeline_from_artifacts():\n",
    "    \"\"\"\n",
    "    Load the complete classification pipeline from saved artifacts (Step 8)\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (loaded_pipeline, model_manifest) or (None, error_message)\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Loading Classification Pipeline from Saved Artifacts ===\")\n",
    "    \n",
    "    try:\n",
    "        # Check if artifacts exist\n",
    "        artifacts_path = Path('artifacts')\n",
    "        models_path = Path('models')\n",
    "        \n",
    "        if not artifacts_path.exists():\n",
    "            return None, \"Artifacts directory not found. Please run Step 8 first.\"\n",
    "        \n",
    "        if not models_path.exists():\n",
    "            return None, \"Models directory not found. Please run Step 8 first.\"\n",
    "        \n",
    "        # Load model manifest to verify artifacts\n",
    "        manifest_file = artifacts_path / 'model_manifest.json'\n",
    "        if not manifest_file.exists():\n",
    "            return None, \"Model manifest not found. Please run Step 8 first.\"\n",
    "        \n",
    "        with open(manifest_file, 'r') as f:\n",
    "            model_manifest = json.load(f)\n",
    "        \n",
    "        print(f\"✓ Found model manifest: {model_manifest['model_name']} v{model_manifest['version']}\")\n",
    "        print(f\"  Created: {model_manifest['created_date']}\")\n",
    "        print(f\"  Test Accuracy: {model_manifest['test_accuracy']:.4f}\")\n",
    "        \n",
    "        # Load the complete pipeline directly\n",
    "        pipeline_file = artifacts_path / 'classification_pipeline.pkl'\n",
    "        if pipeline_file.exists():\n",
    "            print(\"\\n✓ Loading complete pipeline from: artifacts/classification_pipeline.pkl\")\n",
    "            loaded_pipeline = joblib.load(pipeline_file)\n",
    "            print(\"  Pipeline loaded successfully!\")\n",
    "            return loaded_pipeline, model_manifest\n",
    "        else:\n",
    "            # Fallback: Load individual components and reconstruct pipeline\n",
    "            print(\"\\n⚠ Pipeline file not found. Loading individual components...\")\n",
    "            \n",
    "            # Load model\n",
    "            print(\"  Loading model...\")\n",
    "            model_file = models_path / 'resume_classifier_complete.h5'\n",
    "            if not model_file.exists():\n",
    "                return None, \"Model file not found: models/resume_classifier_complete.h5\"\n",
    "            loaded_model = keras.models.load_model(model_file)\n",
    "            print(\"  ✓ Model loaded\")\n",
    "            \n",
    "            # Load feature scaler\n",
    "            print(\"  Loading feature scaler...\")\n",
    "            scaler_file = artifacts_path / 'feature_scaler.pkl'\n",
    "            if not scaler_file.exists():\n",
    "                return None, \"Scaler file not found: artifacts/feature_scaler.pkl\"\n",
    "            loaded_scaler = joblib.load(scaler_file)\n",
    "            print(\"  ✓ Scaler loaded\")\n",
    "            \n",
    "            # Load skill vocabulary\n",
    "            print(\"  Loading skill vocabulary...\")\n",
    "            skill_vocab_file = artifacts_path / 'skill_vocabulary.json'\n",
    "            if not skill_vocab_file.exists():\n",
    "                return None, \"Skill vocabulary not found: artifacts/skill_vocabulary.json\"\n",
    "            with open(skill_vocab_file, 'r') as f:\n",
    "                loaded_skill_vocab = json.load(f)\n",
    "            print(f\"  ✓ Skill vocabulary loaded ({len(loaded_skill_vocab)} skills)\")\n",
    "            \n",
    "            # Load label encoder\n",
    "            print(\"  Loading label encoder...\")\n",
    "            label_encoder_file = artifacts_path / 'label_encoder.pkl'\n",
    "            if not label_encoder_file.exists():\n",
    "                return None, \"Label encoder not found: artifacts/label_encoder.pkl\"\n",
    "            loaded_label_encoder = joblib.load(label_encoder_file)\n",
    "            print(f\"  ✓ Label encoder loaded (classes: {loaded_label_encoder.classes_})\")\n",
    "            \n",
    "            # Load feature vector builder\n",
    "            print(\"  Loading feature vector builder...\")\n",
    "            feature_builder_file = artifacts_path / 'feature_vector_builder.pkl'\n",
    "            if not feature_builder_file.exists():\n",
    "                return None, \"Feature builder not found: artifacts/feature_vector_builder.pkl\"\n",
    "            loaded_feature_builder = joblib.load(feature_builder_file)\n",
    "            print(\"  ✓ Feature builder loaded\")\n",
    "            \n",
    "            # Load domain requirements\n",
    "            print(\"  Loading domain requirements...\")\n",
    "            domain_req_file = artifacts_path / 'domain_requirements.json'\n",
    "            if not domain_req_file.exists():\n",
    "                return None, \"Domain requirements not found: artifacts/domain_requirements.json\"\n",
    "            with open(domain_req_file, 'r') as f:\n",
    "                loaded_domain_requirements = json.load(f)\n",
    "            print(f\"  ✓ Domain requirements loaded ({len(loaded_domain_requirements)} domains)\")\n",
    "            \n",
    "            # Reconstruct pipeline\n",
    "            print(\"\\n  Reconstructing classification pipeline...\")\n",
    "            loaded_pipeline = ResumeClassificationPipeline(\n",
    "                model=loaded_model,\n",
    "                feature_builder=loaded_feature_builder,\n",
    "                label_encoder=loaded_label_encoder,\n",
    "                skill_vocab=loaded_skill_vocab,\n",
    "                domain_requirements=loaded_domain_requirements,\n",
    "                scaler=loaded_scaler\n",
    "            )\n",
    "            print(\"  ✓ Pipeline reconstructed successfully!\")\n",
    "            \n",
    "            return loaded_pipeline, model_manifest\n",
    "            \n",
    "    except Exception as e:\n",
    "        return None, f\"Failed to load pipeline: {str(e)}\"\n",
    "\n",
    "\n",
    "#==============================================================================\n",
    "# Main Execution: Load and Test Pipeline\n",
    "#==============================================================================\n",
    "\n",
    "# Option 1: Initialize from current session (if models already in memory)\n",
    "if 'model' in locals() and 'vector_builder' in locals():\n",
    "    print(\"\\n=== Using Current Session Pipeline ===\")\n",
    "    classification_pipeline = ResumeClassificationPipeline(\n",
    "        model=model,\n",
    "        feature_builder=vector_builder,\n",
    "        label_encoder=label_encoder,\n",
    "        skill_vocab=skill_vocab,\n",
    "        domain_requirements=domain_requirements,\n",
    "        scaler=scaler\n",
    "    )\n",
    "    print(\"✓ Pipeline initialized from current session\")\n",
    "    use_loaded_pipeline = False\n",
    "else:\n",
    "    # Option 2: Load from saved artifacts (production scenario)\n",
    "    print(\"\\n=== Loading Pipeline from Saved Artifacts (Production Mode) ===\")\n",
    "    classification_pipeline, manifest = load_classification_pipeline_from_artifacts()\n",
    "    \n",
    "    if classification_pipeline is None:\n",
    "        print(f\"\\n✗ ERROR: {manifest}\")\n",
    "        print(\"\\nPlease ensure you have run Step 8 to save all models and artifacts.\")\n",
    "        raise RuntimeError(\"Cannot proceed without saved models. Run Step 8 first.\")\n",
    "    else:\n",
    "        print(f\"\\n✓ Successfully loaded pipeline from artifacts!\")\n",
    "        print(f\"  Model version: {manifest['model_version']}\")\n",
    "        print(f\"  Input features: {manifest['input_features']['total_features']}\")\n",
    "        print(f\"  Output classes: {manifest['output_classes']}\")\n",
    "        use_loaded_pipeline = True\n",
    "\n",
    "#==============================================================================\n",
    "# Test JSON Output Generation\n",
    "#==============================================================================\n",
    "\n",
    "print(\"\\n=== Testing JSON Output Generation ===\")\n",
    "\n",
    "# Load sample resumes (if not in memory, load from file)\n",
    "if 'labeled_resumes' not in locals():\n",
    "    print(\"Loading sample resumes from data file...\")\n",
    "    # You would load your resume data here\n",
    "    # For now, we'll assume sample_resumes is available\n",
    "    sample_resumes = []  # Placeholder\n",
    "else:\n",
    "    sample_resumes = labeled_resumes[:5]  # Test first 5 resumes\n",
    "\n",
    "if len(sample_resumes) == 0:\n",
    "    print(\"⚠ No sample resumes available. Loading from saved results...\")\n",
    "    # Try to load previously saved results\n",
    "    sample_json_file = Path('data/sample_json_outputs.json')\n",
    "    if sample_json_file.exists():\n",
    "        with open(sample_json_file, 'r') as f:\n",
    "            sample_results = json.load(f)\n",
    "        print(f\"✓ Loaded {len(sample_results)} previously saved results\")\n",
    "    else:\n",
    "        print(\"✗ No sample data available. Please provide resume data.\")\n",
    "        sample_results = []\n",
    "else:\n",
    "    print(f\"Generating JSON outputs for {len(sample_resumes)} sample resumes...\")\n",
    "    sample_results = []\n",
    "\n",
    "    for i, resume in enumerate(sample_resumes):\n",
    "        print(f\"\\n--- Sample {i+1}: {resume['id']} ({resume.get('preferred_domain', 'N/A')}) ---\")\n",
    "        \n",
    "        # Classify resume and get JSON output using loaded pipeline\n",
    "        result = classification_pipeline.classify_resume(resume, include_raw_scores=True, precision=3)\n",
    "        \n",
    "        if 'error' not in result:\n",
    "            print(f\"Prediction: {result['label']} (confidence: {result['confidence']})\")\n",
    "            print(f\"Matched skills: {len(result['matched_skills'])}, Missing: {len(result['missing_skills'])}\")\n",
    "            print(f\"Feature summary: {result['feature_summary']}\")\n",
    "            print(f\"Explanation: {result['explanation']}\")\n",
    "            \n",
    "            # Display alternative domain suggestions if present\n",
    "            if 'alternative_domain_suggestions' in result:\n",
    "                print(f\"\\nAlternative Domain Suggestions:\")\n",
    "                for alt in result['alternative_domain_suggestions']:\n",
    "                    print(f\"  {alt['rank']}. {alt['domain']}: {alt['skill_match_ratio']:.1%} match \"\n",
    "                          f\"({alt['matched_skills_count']}/{alt['required_skills_count']} skills)\")\n",
    "        else:\n",
    "            print(f\"Error: {result['error']}\")\n",
    "        \n",
    "        sample_results.append(result)\n",
    "\n",
    "    # Save sample results\n",
    "    output_dir = Path('data')\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    with open(output_dir / 'sample_json_outputs.json', 'w') as f:\n",
    "        json.dump(sample_results, f, indent=2)\n",
    "\n",
    "    print(\"\\n=== Saved sample JSON outputs to: data/sample_json_outputs.json ===\")\n",
    "\n",
    "#==============================================================================\n",
    "# Display Results and Statistics\n",
    "#==============================================================================\n",
    "\n",
    "if sample_results:\n",
    "    # Demonstrate exact arithmetic formatting\n",
    "    print(f\"\\n=== Numeric Formatting Examples ===\")\n",
    "    for result in sample_results[:2]:\n",
    "        if 'error' not in result:\n",
    "            skill_ratio = result['feature_summary']['skill_match_ratio']\n",
    "            matched_count = len(result['matched_skills'])\n",
    "            total_count = matched_count + len(result['missing_skills'])\n",
    "            \n",
    "            print(f\"\\nResume {result['metadata']['candidate_id']}:\")\n",
    "            print(f\"  Matched skills: {matched_count}, Required: {total_count}\")\n",
    "            print(f\"  Arithmetic: {matched_count} ÷ {total_count} = {matched_count/total_count:.3f}\")\n",
    "            print(f\"  Formatted in JSON: {skill_ratio}\")\n",
    "\n",
    "    # Show example of complete JSON with alternative domains\n",
    "    print(f\"\\n=== Example Complete JSON Output ===\")\n",
    "    example_result = None\n",
    "    for result in sample_results:\n",
    "        if 'error' not in result and result['label'] in ['Partial Fit', 'Not Fit']:\n",
    "            example_result = result\n",
    "            break\n",
    "\n",
    "    if example_result:\n",
    "        print(json.dumps(example_result, indent=2))\n",
    "    else:\n",
    "        print(\"No Partial Fit/Not Fit examples in sample set. Showing first result:\")\n",
    "        if sample_results and 'error' not in sample_results[0]:\n",
    "            print(json.dumps(sample_results[0], indent=2))\n",
    "\n",
    "    # Statistics on alternative domain suggestions\n",
    "    print(f\"\\n=== Alternative Domain Suggestion Statistics ===\")\n",
    "    partial_not_fit_count = sum(1 for r in sample_results \n",
    "                                if 'error' not in r and r['label'] in ['Partial Fit', 'Not Fit'])\n",
    "    with_alternatives = sum(1 for r in sample_results \n",
    "                           if 'error' not in r and 'alternative_domain_suggestions' in r)\n",
    "\n",
    "    print(f\"Partial Fit/Not Fit candidates: {partial_not_fit_count}\")\n",
    "    print(f\"Candidates with alternative suggestions: {with_alternatives}\")\n",
    "\n",
    "    if with_alternatives > 0:\n",
    "        # Calculate average top alternative match\n",
    "        top_matches = [r['alternative_domain_suggestions'][0]['skill_match_ratio'] \n",
    "                       for r in sample_results \n",
    "                       if 'error' not in r and 'alternative_domain_suggestions' in r]\n",
    "        avg_top_match = np.mean(top_matches)\n",
    "        print(f\"Average top alternative domain match: {avg_top_match:.3f}\")\n",
    "\n",
    "#==============================================================================\n",
    "# Production Deployment Example\n",
    "#==============================================================================\n",
    "\n",
    "print(\"\\n=== Production Deployment Example ===\")\n",
    "print(\"\"\"\n",
    "To use this pipeline in production:\n",
    "\n",
    "1. Load the pipeline once at startup:\n",
    "   ```python\n",
    "   from pathlib import Path\n",
    "   import joblib\n",
    "   \n",
    "   pipeline = joblib.load('artifacts/classification_pipeline.pkl')\n",
    "   ```\n",
    "\n",
    "2. Classify new resumes:\n",
    "   ```python\n",
    "   result = pipeline.classify_resume(new_resume_json)\n",
    "   ```\n",
    "\n",
    "3. Batch process resumes:\n",
    "   ```python\n",
    "   results = pipeline.batch_classify(resume_list, output_file='results.json')\n",
    "   ```\n",
    "\n",
    "4. Access individual components if needed:\n",
    "   - Model: pipeline.model\n",
    "   - Scaler: pipeline.scaler\n",
    "   - Skill vocabulary: pipeline.skill_vocab\n",
    "   - Label encoder: pipeline.label_encoder\n",
    "\"\"\")\n",
    "\n",
    "if use_loaded_pipeline:\n",
    "    print(\"\\n✓ All artifacts loaded successfully from Step 8!\")\n",
    "    print(\"✓ Pipeline is ready for production use!\")\n",
    "\n",
    "print(\"\\n=== Step 10 Complete ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164f663c-2fb5-4d80-9c7a-3033781baa4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1472378c-0a17-4972-bc89-24dda346fcfa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3d64af-00d7-41cd-bf25-dd29e1b66295",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a0ef43-fbfd-41f4-a265-2bd6d321b3ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
