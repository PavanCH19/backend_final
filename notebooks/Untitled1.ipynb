{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6f6a604-c4fe-4637-bbf7-99329fa25259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Step 11: Production Inference (Load & Predict) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.0.0 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"C:\\Users\\pavan\\anaconda3\\Lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"C:\\Users\\pavan\\anaconda3\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"C:\\Users\\pavan\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 701, in start\n",
      "    self.io_loop.start()\n",
      "  File \"C:\\Users\\pavan\\AppData\\Roaming\\Python\\Python312\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"C:\\Users\\pavan\\anaconda3\\Lib\\asyncio\\windows_events.py\", line 322, in run_forever\n",
      "    super().run_forever()\n",
      "  File \"C:\\Users\\pavan\\anaconda3\\Lib\\asyncio\\base_events.py\", line 641, in run_forever\n",
      "    self._run_once()\n",
      "  File \"C:\\Users\\pavan\\anaconda3\\Lib\\asyncio\\base_events.py\", line 1986, in _run_once\n",
      "    handle._run()\n",
      "  File \"C:\\Users\\pavan\\anaconda3\\Lib\\asyncio\\events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"C:\\Users\\pavan\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"C:\\Users\\pavan\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 523, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"C:\\Users\\pavan\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 429, in dispatch_shell\n",
      "    await result\n",
      "  File \"C:\\Users\\pavan\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 767, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"C:\\Users\\pavan\\anaconda3\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 429, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"C:\\Users\\pavan\\anaconda3\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"C:\\Users\\pavan\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"C:\\Users\\pavan\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"C:\\Users\\pavan\\anaconda3\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"C:\\Users\\pavan\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"C:\\Users\\pavan\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"C:\\Users\\pavan\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\pavan\\AppData\\Local\\Temp\\ipykernel_32092\\1300823679.py\", line 7, in <module>\n",
      "    from tensorflow import keras\n",
      "  File \"C:\\Users\\pavan\\AppData\\Roaming\\Python\\Python312\\site-packages\\tensorflow\\__init__.py\", line 468, in <module>\n",
      "    importlib.import_module(\"keras.src.optimizers\")\n",
      "  File \"C:\\Users\\pavan\\anaconda3\\Lib\\importlib\\__init__.py\", line 90, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"C:\\Users\\pavan\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\__init__.py\", line 7, in <module>\n",
      "    from keras import _tf_keras as _tf_keras\n",
      "  File \"C:\\Users\\pavan\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\_tf_keras\\__init__.py\", line 1, in <module>\n",
      "    from keras._tf_keras import keras\n",
      "  File \"C:\\Users\\pavan\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\_tf_keras\\keras\\__init__.py\", line 7, in <module>\n",
      "    from keras import activations as activations\n",
      "  File \"C:\\Users\\pavan\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\activations\\__init__.py\", line 7, in <module>\n",
      "    from keras.src.activations import deserialize as deserialize\n",
      "  File \"C:\\Users\\pavan\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\__init__.py\", line 8, in <module>\n",
      "    from keras.src import models\n",
      "  File \"C:\\Users\\pavan\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\models\\__init__.py\", line 1, in <module>\n",
      "    from keras.src.models.functional import Functional\n",
      "  File \"C:\\Users\\pavan\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\models\\functional.py\", line 16, in <module>\n",
      "    from keras.src.models.model import Model\n",
      "  File \"C:\\Users\\pavan\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\models\\model.py\", line 12, in <module>\n",
      "    from keras.src.trainers import trainer as base_trainer\n",
      "  File \"C:\\Users\\pavan\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\trainers\\trainer.py\", line 14, in <module>\n",
      "    from keras.src.trainers.data_adapters import data_adapter_utils\n",
      "  File \"C:\\Users\\pavan\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\trainers\\data_adapters\\__init__.py\", line 4, in <module>\n",
      "    from keras.src.trainers.data_adapters import array_data_adapter\n",
      "  File \"C:\\Users\\pavan\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\trainers\\data_adapters\\array_data_adapter.py\", line 7, in <module>\n",
      "    from keras.src.trainers.data_adapters import array_slicing\n",
      "  File \"C:\\Users\\pavan\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\trainers\\data_adapters\\array_slicing.py\", line 12, in <module>\n",
      "    import pandas\n",
      "  File \"C:\\Users\\pavan\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\__init__.py\", line 62, in <module>\n",
      "    from pandas.core.api import (\n",
      "  File \"C:\\Users\\pavan\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\api.py\", line 28, in <module>\n",
      "    from pandas.core.arrays import Categorical\n",
      "  File \"C:\\Users\\pavan\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\arrays\\__init__.py\", line 1, in <module>\n",
      "    from pandas.core.arrays.arrow import ArrowExtensionArray\n",
      "  File \"C:\\Users\\pavan\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\arrays\\arrow\\__init__.py\", line 5, in <module>\n",
      "    from pandas.core.arrays.arrow.array import ArrowExtensionArray\n",
      "  File \"C:\\Users\\pavan\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\arrays\\arrow\\array.py\", line 50, in <module>\n",
      "    from pandas.core import (\n",
      "  File \"C:\\Users\\pavan\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\ops\\__init__.py\", line 8, in <module>\n",
      "    from pandas.core.ops.array_ops import (\n",
      "  File \"C:\\Users\\pavan\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\ops\\array_ops.py\", line 56, in <module>\n",
      "    from pandas.core.computation import expressions\n",
      "  File \"C:\\Users\\pavan\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\computation\\expressions.py\", line 21, in <module>\n",
      "    from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "  File \"C:\\Users\\pavan\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\computation\\check.py\", line 5, in <module>\n",
      "    ne = import_optional_dependency(\"numexpr\", errors=\"warn\")\n",
      "  File \"C:\\Users\\pavan\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\compat\\_optional.py\", line 135, in import_optional_dependency\n",
      "    module = importlib.import_module(name)\n",
      "  File \"C:\\Users\\pavan\\anaconda3\\Lib\\importlib\\__init__.py\", line 90, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"C:\\Users\\pavan\\anaconda3\\Lib\\site-packages\\numexpr\\__init__.py\", line 24, in <module>\n",
      "    from numexpr.interpreter import MAX_THREADS, use_vml, __BLOCK_SIZE1__\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "_ARRAY_API not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;31mAttributeError\u001b[0m: _ARRAY_API not found"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.0.0 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"C:\\Users\\pavan\\anaconda3\\Lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"C:\\Users\\pavan\\anaconda3\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"C:\\Users\\pavan\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 701, in start\n",
      "    self.io_loop.start()\n",
      "  File \"C:\\Users\\pavan\\AppData\\Roaming\\Python\\Python312\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"C:\\Users\\pavan\\anaconda3\\Lib\\asyncio\\windows_events.py\", line 322, in run_forever\n",
      "    super().run_forever()\n",
      "  File \"C:\\Users\\pavan\\anaconda3\\Lib\\asyncio\\base_events.py\", line 641, in run_forever\n",
      "    self._run_once()\n",
      "  File \"C:\\Users\\pavan\\anaconda3\\Lib\\asyncio\\base_events.py\", line 1986, in _run_once\n",
      "    handle._run()\n",
      "  File \"C:\\Users\\pavan\\anaconda3\\Lib\\asyncio\\events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"C:\\Users\\pavan\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"C:\\Users\\pavan\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 523, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"C:\\Users\\pavan\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 429, in dispatch_shell\n",
      "    await result\n",
      "  File \"C:\\Users\\pavan\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 767, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"C:\\Users\\pavan\\anaconda3\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 429, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"C:\\Users\\pavan\\anaconda3\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"C:\\Users\\pavan\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"C:\\Users\\pavan\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"C:\\Users\\pavan\\anaconda3\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"C:\\Users\\pavan\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"C:\\Users\\pavan\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"C:\\Users\\pavan\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\pavan\\AppData\\Local\\Temp\\ipykernel_32092\\1300823679.py\", line 7, in <module>\n",
      "    from tensorflow import keras\n",
      "  File \"C:\\Users\\pavan\\AppData\\Roaming\\Python\\Python312\\site-packages\\tensorflow\\__init__.py\", line 468, in <module>\n",
      "    importlib.import_module(\"keras.src.optimizers\")\n",
      "  File \"C:\\Users\\pavan\\anaconda3\\Lib\\importlib\\__init__.py\", line 90, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"C:\\Users\\pavan\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\__init__.py\", line 7, in <module>\n",
      "    from keras import _tf_keras as _tf_keras\n",
      "  File \"C:\\Users\\pavan\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\_tf_keras\\__init__.py\", line 1, in <module>\n",
      "    from keras._tf_keras import keras\n",
      "  File \"C:\\Users\\pavan\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\_tf_keras\\keras\\__init__.py\", line 7, in <module>\n",
      "    from keras import activations as activations\n",
      "  File \"C:\\Users\\pavan\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\activations\\__init__.py\", line 7, in <module>\n",
      "    from keras.src.activations import deserialize as deserialize\n",
      "  File \"C:\\Users\\pavan\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\__init__.py\", line 8, in <module>\n",
      "    from keras.src import models\n",
      "  File \"C:\\Users\\pavan\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\models\\__init__.py\", line 1, in <module>\n",
      "    from keras.src.models.functional import Functional\n",
      "  File \"C:\\Users\\pavan\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\models\\functional.py\", line 16, in <module>\n",
      "    from keras.src.models.model import Model\n",
      "  File \"C:\\Users\\pavan\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\models\\model.py\", line 12, in <module>\n",
      "    from keras.src.trainers import trainer as base_trainer\n",
      "  File \"C:\\Users\\pavan\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\trainers\\trainer.py\", line 14, in <module>\n",
      "    from keras.src.trainers.data_adapters import data_adapter_utils\n",
      "  File \"C:\\Users\\pavan\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\trainers\\data_adapters\\__init__.py\", line 4, in <module>\n",
      "    from keras.src.trainers.data_adapters import array_data_adapter\n",
      "  File \"C:\\Users\\pavan\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\trainers\\data_adapters\\array_data_adapter.py\", line 7, in <module>\n",
      "    from keras.src.trainers.data_adapters import array_slicing\n",
      "  File \"C:\\Users\\pavan\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\trainers\\data_adapters\\array_slicing.py\", line 12, in <module>\n",
      "    import pandas\n",
      "  File \"C:\\Users\\pavan\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\__init__.py\", line 62, in <module>\n",
      "    from pandas.core.api import (\n",
      "  File \"C:\\Users\\pavan\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\api.py\", line 28, in <module>\n",
      "    from pandas.core.arrays import Categorical\n",
      "  File \"C:\\Users\\pavan\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\arrays\\__init__.py\", line 1, in <module>\n",
      "    from pandas.core.arrays.arrow import ArrowExtensionArray\n",
      "  File \"C:\\Users\\pavan\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\arrays\\arrow\\__init__.py\", line 5, in <module>\n",
      "    from pandas.core.arrays.arrow.array import ArrowExtensionArray\n",
      "  File \"C:\\Users\\pavan\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\arrays\\arrow\\array.py\", line 64, in <module>\n",
      "    from pandas.core.arrays.masked import BaseMaskedArray\n",
      "  File \"C:\\Users\\pavan\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\arrays\\masked.py\", line 60, in <module>\n",
      "    from pandas.core import (\n",
      "  File \"C:\\Users\\pavan\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\nanops.py\", line 52, in <module>\n",
      "    bn = import_optional_dependency(\"bottleneck\", errors=\"warn\")\n",
      "  File \"C:\\Users\\pavan\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\compat\\_optional.py\", line 135, in import_optional_dependency\n",
      "    module = importlib.import_module(name)\n",
      "  File \"C:\\Users\\pavan\\anaconda3\\Lib\\importlib\\__init__.py\", line 90, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"C:\\Users\\pavan\\anaconda3\\Lib\\site-packages\\bottleneck\\__init__.py\", line 7, in <module>\n",
      "    from .move import (move_argmax, move_argmin, move_max, move_mean, move_median,\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "\nA module that was compiled using NumPy 1.x cannot be run in\nNumPy 2.0.0 as it may crash. To support both 1.x and 2.x\nversions of NumPy, modules must be compiled with NumPy 2.0.\nSome module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n\nIf you are a user of the module, the easiest solution will be to\ndowngrade to 'numpy<2' or try to upgrade the affected module.\nWe expect that some modules will need time to support NumPy 2.\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\numpy\\core\\_multiarray_umath.py:44\u001b[0m, in \u001b[0;36m__getattr__\u001b[1;34m(attr_name)\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;66;03m# Also print the message (with traceback).  This is because old versions\u001b[39;00m\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;66;03m# of NumPy unfortunately set up the import to replace (and hide) the\u001b[39;00m\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;66;03m# error.  The traceback shouldn't be needed, but e.g. pytest plugins\u001b[39;00m\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;66;03m# seem to swallow it and we should be failing anyway...\u001b[39;00m\n\u001b[0;32m     43\u001b[0m     sys\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mwrite(msg \u001b[38;5;241m+\u001b[39m tb_msg)\n\u001b[1;32m---> 44\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[0;32m     46\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(_multiarray_umath, attr_name, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mImportError\u001b[0m: \nA module that was compiled using NumPy 1.x cannot be run in\nNumPy 2.0.0 as it may crash. To support both 1.x and 2.x\nversions of NumPy, modules must be compiled with NumPy 2.0.\nSome module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n\nIf you are a user of the module, the easiest solution will be to\ndowngrade to 'numpy<2' or try to upgrade the affected module.\nWe expect that some modules will need time to support NumPy 2.\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STEP 11: PRODUCTION INFERENCE DEMO\n",
      "============================================================\n",
      "Loading pre-trained model and artifacts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded model\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "Can't get attribute 'ResumeFeatureScaler' on <module '__main__'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 240\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m60\u001b[39m)\n\u001b[0;32m    239\u001b[0m \u001b[38;5;66;03m# Initialize production classifier (loads everything once)\u001b[39;00m\n\u001b[1;32m--> 240\u001b[0m classifier \u001b[38;5;241m=\u001b[39m ProductionResumeClassifier()\n\u001b[0;32m    242\u001b[0m \u001b[38;5;66;03m# Example 1: Predict single resume\u001b[39;00m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- Example 1: Single Resume Prediction ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[1], line 33\u001b[0m, in \u001b[0;36mProductionResumeClassifier.__init__\u001b[1;34m(self, artifacts_dir, models_dir)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✓ Loaded model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Load all preprocessing artifacts\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler \u001b[38;5;241m=\u001b[39m joblib\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39martifacts_dir \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeature_scaler.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✓ Loaded feature scaler\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39martifacts_dir \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mskill_vocabulary.json\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\joblib\\numpy_pickle.py:749\u001b[0m, in \u001b[0;36mload\u001b[1;34m(filename, mmap_mode, ensure_native_byte_order)\u001b[0m\n\u001b[0;32m    744\u001b[0m                 \u001b[38;5;28;01mreturn\u001b[39;00m load_compatibility(fobj)\n\u001b[0;32m    746\u001b[0m             \u001b[38;5;66;03m# A memory-mapped array has to be mapped with the endianness\u001b[39;00m\n\u001b[0;32m    747\u001b[0m             \u001b[38;5;66;03m# it has been written with. Other arrays are coerced to the\u001b[39;00m\n\u001b[0;32m    748\u001b[0m             \u001b[38;5;66;03m# native endianness of the host system.\u001b[39;00m\n\u001b[1;32m--> 749\u001b[0m             obj \u001b[38;5;241m=\u001b[39m _unpickle(\n\u001b[0;32m    750\u001b[0m                 fobj,\n\u001b[0;32m    751\u001b[0m                 ensure_native_byte_order\u001b[38;5;241m=\u001b[39mensure_native_byte_order,\n\u001b[0;32m    752\u001b[0m                 filename\u001b[38;5;241m=\u001b[39mfilename,\n\u001b[0;32m    753\u001b[0m                 mmap_mode\u001b[38;5;241m=\u001b[39mvalidated_mmap_mode,\n\u001b[0;32m    754\u001b[0m             )\n\u001b[0;32m    756\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\joblib\\numpy_pickle.py:626\u001b[0m, in \u001b[0;36m_unpickle\u001b[1;34m(fobj, ensure_native_byte_order, filename, mmap_mode)\u001b[0m\n\u001b[0;32m    624\u001b[0m obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 626\u001b[0m     obj \u001b[38;5;241m=\u001b[39m unpickler\u001b[38;5;241m.\u001b[39mload()\n\u001b[0;32m    627\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m unpickler\u001b[38;5;241m.\u001b[39mcompat_mode:\n\u001b[0;32m    628\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    629\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe file \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m has been generated with a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    630\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjoblib version less than 0.10. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    633\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[0;32m    634\u001b[0m         )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\pickle.py:1255\u001b[0m, in \u001b[0;36m_Unpickler.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1253\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEOFError\u001b[39;00m\n\u001b[0;32m   1254\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, bytes_types)\n\u001b[1;32m-> 1255\u001b[0m         dispatch[key[\u001b[38;5;241m0\u001b[39m]](\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m   1256\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _Stop \u001b[38;5;28;01mas\u001b[39;00m stopinst:\n\u001b[0;32m   1257\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m stopinst\u001b[38;5;241m.\u001b[39mvalue\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\pickle.py:1580\u001b[0m, in \u001b[0;36m_Unpickler.load_stack_global\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1578\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(name) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mstr\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(module) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m   1579\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m UnpicklingError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSTACK_GLOBAL requires str\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1580\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfind_class(module, name))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\pickle.py:1623\u001b[0m, in \u001b[0;36m_Unpickler.find_class\u001b[1;34m(self, module, name)\u001b[0m\n\u001b[0;32m   1621\u001b[0m \u001b[38;5;28m__import__\u001b[39m(module, level\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m   1622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproto \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m:\n\u001b[1;32m-> 1623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _getattribute(sys\u001b[38;5;241m.\u001b[39mmodules[module], name)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1624\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1625\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(sys\u001b[38;5;241m.\u001b[39mmodules[module], name)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\pickle.py:326\u001b[0m, in \u001b[0;36m_getattribute\u001b[1;34m(obj, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(obj, subpath)\n\u001b[0;32m    325\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[1;32m--> 326\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt get attribute \u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m on \u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    327\u001b[0m                              \u001b[38;5;241m.\u001b[39mformat(name, top)) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    328\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj, parent\n",
      "\u001b[1;31mAttributeError\u001b[0m: Can't get attribute 'ResumeFeatureScaler' on <module '__main__'>"
     ]
    }
   ],
   "source": [
    "# Step 11 - Production Inference with Pre-trained Model\n",
    "print(\"\\n=== Step 11: Production Inference (Load & Predict) ===\")\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import joblib\n",
    "from tensorflow import keras\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "class ProductionResumeClassifier:\n",
    "    \"\"\"\n",
    "    Lightweight inference class that loads pre-trained artifacts\n",
    "    No training - just prediction on new resumes\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, artifacts_dir='artifacts', models_dir='models'):\n",
    "        \"\"\"\n",
    "        Load all pre-trained artifacts once during initialization\n",
    "        \"\"\"\n",
    "        self.artifacts_dir = Path(artifacts_dir)\n",
    "        self.models_dir = Path(models_dir)\n",
    "        \n",
    "        print(\"Loading pre-trained model and artifacts...\")\n",
    "        \n",
    "        # Load trained model\n",
    "        self.model = keras.models.load_model(\n",
    "            self.models_dir / 'resume_classifier_complete.h5'\n",
    "        )\n",
    "        print(\"✓ Loaded model\")\n",
    "        \n",
    "        # Load all preprocessing artifacts\n",
    "        self.scaler = joblib.load(self.artifacts_dir / 'feature_scaler.pkl')\n",
    "        print(\"✓ Loaded feature scaler\")\n",
    "        \n",
    "        with open(self.artifacts_dir / 'skill_vocabulary.json', 'r') as f:\n",
    "            self.skill_vocab = json.load(f)\n",
    "        print(f\"✓ Loaded skill vocabulary ({len(self.skill_vocab)} skills)\")\n",
    "        \n",
    "        self.label_encoder = joblib.load(self.artifacts_dir / 'label_encoder.pkl')\n",
    "        print(\"✓ Loaded label encoder\")\n",
    "        \n",
    "        self.feature_builder = joblib.load(\n",
    "            self.artifacts_dir / 'feature_vector_builder.pkl'\n",
    "        )\n",
    "        print(\"✓ Loaded feature vector builder\")\n",
    "        \n",
    "        with open(self.artifacts_dir / 'domain_requirements.json', 'r') as f:\n",
    "            self.domain_requirements = json.load(f)\n",
    "        print(\"✓ Loaded domain requirements\")\n",
    "        \n",
    "        # IMPORTANT: Load the complete classification pipeline from Step 9\n",
    "        # This includes the explanation generation logic\n",
    "        self.pipeline = joblib.load(self.artifacts_dir / 'classification_pipeline.pkl')\n",
    "        print(\"✓ Loaded classification pipeline (with explanation generator)\")\n",
    "        \n",
    "        # Load model manifest for metadata\n",
    "        with open(self.artifacts_dir / 'model_manifest.json', 'r') as f:\n",
    "            self.manifest = json.load(f)\n",
    "        \n",
    "        print(f\"\\nModel version: {self.manifest['version']}\")\n",
    "        print(f\"Model created: {self.manifest['created_date']}\")\n",
    "        print(f\"Test accuracy: {self.manifest['test_accuracy']:.3f}\")\n",
    "        print(\"\\n✓ All artifacts loaded successfully!\")\n",
    "    \n",
    "    def predict_single(self, resume_dict, return_probabilities=False, include_raw_scores=True, precision=3):\n",
    "        \"\"\"\n",
    "        Predict classification for a single resume\n",
    "        Uses the exact same pipeline as Step 9\n",
    "        \n",
    "        Args:\n",
    "            resume_dict: Dictionary with resume data\n",
    "            return_probabilities: If True, return class probabilities\n",
    "            include_raw_scores: If True, include raw test score in feature summary\n",
    "            precision: Decimal precision for numeric values\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with prediction results (same format as Step 9)\n",
    "        \"\"\"\n",
    "        # Use the loaded pipeline's classify_resume method directly\n",
    "        # This ensures we use the EXACT same explanation generation logic\n",
    "        result = self.pipeline.classify_resume(\n",
    "            resume_dict, \n",
    "            include_raw_scores=include_raw_scores,\n",
    "            precision=precision\n",
    "        )\n",
    "        \n",
    "        # Add class probabilities if requested\n",
    "        if return_probabilities and 'error' not in result:\n",
    "            try:\n",
    "                # Extract features and get predictions to retrieve probabilities\n",
    "                resume_features = self._extract_features_minimal(resume_dict)\n",
    "                scaled_numeric = self.scaler.transform([resume_features['numeric_features']])\n",
    "                resume_features['scaled_numeric_features'] = scaled_numeric[0]\n",
    "                \n",
    "                feature_vector = self.feature_builder.build_final_vector(resume_features)\n",
    "                model_inputs = self._prepare_model_inputs(feature_vector.reshape(1, -1))\n",
    "                class_probs = self.model.predict(model_inputs, verbose=0)[0]\n",
    "                \n",
    "                result[\"class_probabilities\"] = {\n",
    "                    label: round(float(prob), precision)\n",
    "                    for label, prob in zip(self.label_encoder.classes_, class_probs)\n",
    "                }\n",
    "            except Exception as e:\n",
    "                result[\"probabilities_error\"] = str(e)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def predict_batch(self, resume_list, output_file=None, include_raw_scores=True, precision=3):\n",
    "        \"\"\"\n",
    "        Predict classifications for multiple resumes\n",
    "        Uses the pipeline's batch classification\n",
    "        \n",
    "        Args:\n",
    "            resume_list: List of resume dictionaries\n",
    "            output_file: Optional path to save results as JSON\n",
    "            include_raw_scores: Include raw test scores\n",
    "            precision: Decimal precision\n",
    "        \n",
    "        Returns:\n",
    "            List of prediction results\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        \n",
    "        print(f\"\\nProcessing {len(resume_list)} resumes...\")\n",
    "        for i, resume in enumerate(resume_list, 1):\n",
    "            if i % 10 == 0:\n",
    "                print(f\"  Processed {i}/{len(resume_list)}...\")\n",
    "            \n",
    "            result = self.predict_single(\n",
    "                resume, \n",
    "                return_probabilities=False,\n",
    "                include_raw_scores=include_raw_scores,\n",
    "                precision=precision\n",
    "            )\n",
    "            results.append(result)\n",
    "        \n",
    "        print(f\"✓ Completed {len(results)} predictions\")\n",
    "        \n",
    "        # Save if output file specified\n",
    "        if output_file:\n",
    "            output_path = Path(output_file)\n",
    "            output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "            with open(output_path, 'w') as f:\n",
    "                json.dump(results, f, indent=2)\n",
    "            print(f\"✓ Saved results to {output_file}\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _extract_features_minimal(self, resume):\n",
    "        \"\"\"Minimal feature extraction for probability calculation\"\"\"\n",
    "        domain_key = None\n",
    "        for key, req in self.domain_requirements.items():\n",
    "            if req[\"domain\"] == resume[\"preferred_domain\"]:\n",
    "                domain_key = key\n",
    "                break\n",
    "        \n",
    "        if domain_key is None:\n",
    "            raise ValueError(f\"Unknown domain: {resume['preferred_domain']}\")\n",
    "        \n",
    "        required_skills = self.domain_requirements[domain_key][\"required_skills\"]\n",
    "        candidate_skills = resume.get('skills', [])\n",
    "        \n",
    "        skill_vector = self._encode_skills(candidate_skills)\n",
    "        matched, missing, ratio = self._compute_skill_matches(candidate_skills, required_skills)\n",
    "        \n",
    "        projects = resume.get('projects', [])\n",
    "        work_exp = resume.get('work_experience', [])\n",
    "        test_score = resume.get('test_score', 0)\n",
    "        \n",
    "        project_count = len(projects)\n",
    "        project_text = \" \".join(projects) if projects else \"\"\n",
    "        \n",
    "        years_exp = sum(item.get('years', 0) for item in work_exp)\n",
    "        max_years = max((item.get('years', 0) for item in work_exp), default=0)\n",
    "        exp_text = \" \".join(item.get('title', '') for item in work_exp)\n",
    "        \n",
    "        return {\n",
    "            'skill_vector': skill_vector,\n",
    "            'skill_match_ratio': ratio,\n",
    "            'matched_skills': matched,\n",
    "            'missing_skills': missing,\n",
    "            'project_count': project_count,\n",
    "            'project_text': project_text,\n",
    "            'years_experience': years_exp,\n",
    "            'max_years': max_years,\n",
    "            'experience_text': exp_text,\n",
    "            'test_score': test_score,\n",
    "            'test_score_norm': test_score / 100.0,\n",
    "            'numeric_features': [years_exp, max_years, project_count]\n",
    "        }\n",
    "    \n",
    "    def _encode_skills(self, candidate_skills):\n",
    "        \"\"\"Binary encoding of skills\"\"\"\n",
    "        skill_vector = np.zeros(len(self.skill_vocab), dtype=int)\n",
    "        normalized_skills = {s.strip().lower() for s in candidate_skills}\n",
    "        \n",
    "        for i, vocab_skill in enumerate(self.skill_vocab):\n",
    "            if vocab_skill in normalized_skills:\n",
    "                skill_vector[i] = 1\n",
    "        \n",
    "        return skill_vector\n",
    "    \n",
    "    def _compute_skill_matches(self, candidate_skills, required_skills):\n",
    "        \"\"\"Compute matched/missing skills\"\"\"\n",
    "        candidate_set = {s.strip().lower() for s in candidate_skills}\n",
    "        required_set = {s.strip().lower() for s in required_skills}\n",
    "        \n",
    "        matched = list(candidate_set.intersection(required_set))\n",
    "        missing = list(required_set - candidate_set)\n",
    "        ratio = len(matched) / len(required_set) if required_set else 0.0\n",
    "        \n",
    "        return matched, missing, ratio\n",
    "    \n",
    "    def _prepare_model_inputs(self, X):\n",
    "        \"\"\"Split feature vector into model inputs\"\"\"\n",
    "        skill_dim = len(self.skill_vocab) + 1\n",
    "        numeric_dim = 4\n",
    "        \n",
    "        skill_features = X[:, :skill_dim]\n",
    "        numeric_features = X[:, skill_dim:skill_dim + numeric_dim]\n",
    "        \n",
    "        inputs = [skill_features, numeric_features]\n",
    "        \n",
    "        if self.feature_builder.use_text_embeddings:\n",
    "            text_features = X[:, skill_dim + numeric_dim:]\n",
    "            inputs.append(text_features)\n",
    "        \n",
    "        return inputs\n",
    "\n",
    "\n",
    "# ============= USAGE EXAMPLE =============\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 11: PRODUCTION INFERENCE DEMO\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Initialize production classifier (loads everything once)\n",
    "classifier = ProductionResumeClassifier()\n",
    "\n",
    "# Example 1: Predict single resume\n",
    "print(\"\\n--- Example 1: Single Resume Prediction ---\")\n",
    "\n",
    "new_resume = {\n",
    "    \"id\": \"new_candidate_001\",\n",
    "    \"preferred_domain\": \"Data Science\",\n",
    "    \"skills\": [\"Python\", \"Pandas\", \"NumPy\", \"Scikit-learn\", \"SQL\", \"Docker\"],\n",
    "    \"projects\": [\"Customer Churn Model\", \"Sales Forecasting\"],\n",
    "    \"work_experience\": [\n",
    "        {\"title\": \"Data Analyst\", \"years\": 2},\n",
    "        {\"title\": \"Junior Data Scientist\", \"years\": 1}\n",
    "    ],\n",
    "    \"test_score\": 82\n",
    "}\n",
    "\n",
    "result = classifier.predict_single(new_resume, return_probabilities=True)\n",
    "\n",
    "print(f\"\\nCandidate ID: {result['metadata']['candidate_id']}\")\n",
    "print(f\"Prediction: {result['label']}\")\n",
    "print(f\"Confidence: {result['confidence']}\")\n",
    "print(f\"Matched skills ({len(result['matched_skills'])}): {', '.join(result['matched_skills'])}\")\n",
    "print(f\"Missing skills ({len(result['missing_skills'])}): {', '.join(result['missing_skills'][:5])}\")\n",
    "print(f\"\\nExplanation: {result['explanation']}\")\n",
    "\n",
    "if 'class_probabilities' in result:\n",
    "    print(f\"\\nClass probabilities:\")\n",
    "    for label, prob in result['class_probabilities'].items():\n",
    "        print(f\"  {label}: {prob:.3f}\")\n",
    "\n",
    "if 'alternative_domain_suggestions' in result:\n",
    "    print(f\"\\nAlternative domain suggestions:\")\n",
    "    for alt in result['alternative_domain_suggestions']:\n",
    "        print(f\"  {alt['rank']}. {alt['domain']}: {alt['skill_match_ratio']:.1%} match\")\n",
    "\n",
    "# Example 2: Batch prediction\n",
    "print(\"\\n--- Example 2: Batch Prediction ---\")\n",
    "\n",
    "# Load some test resumes\n",
    "with open('data/balanced_synthetic_resumes.json', 'r') as f:\n",
    "    all_resumes = json.load(f)\n",
    "\n",
    "# Take unseen resumes (simulate new data)\n",
    "new_resumes = all_resumes[1800:1810]  # Last 10 resumes as \"new\" data\n",
    "\n",
    "# Batch predict\n",
    "batch_results = classifier.predict_batch(\n",
    "    new_resumes, \n",
    "    output_file='predictions/new_predictions.json'\n",
    ")\n",
    "\n",
    "# Show summary\n",
    "print(\"\\n--- Batch Prediction Summary ---\")\n",
    "label_counts = {}\n",
    "for result in batch_results:\n",
    "    if 'label' in result:\n",
    "        label = result['label']\n",
    "        label_counts[label] = label_counts.get(label, 0) + 1\n",
    "\n",
    "for label, count in sorted(label_counts.items()):\n",
    "    print(f\"{label}: {count} ({count/len(batch_results)*100:.1f}%)\")\n",
    "\n",
    "# Show sample predictions\n",
    "print(\"\\n--- Sample Predictions ---\")\n",
    "for result in batch_results[:3]:\n",
    "    if 'error' not in result:\n",
    "        print(f\"\\n{result['metadata']['candidate_id']}: {result['label']} ({result['confidence']})\")\n",
    "        print(f\"  {result['explanation']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"✓ Step 11 Complete - Ready for Production!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d5f969-1109-4e38-b935-91370cb039cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
